{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-e-Bo4GMmJn"
   },
   "source": [
    "## Directories Required\n",
    "Ensure you have the following directory structure no matter where you are hosting the data: GCP Storage, GCP Compute Engine local disk, Google Drive or your local desktop/laptop.\n",
    "\n",
    "The root folder on your local drive should be 'image_caption'. The rest follow from it.<br/>\n",
    "\n",
    "For Colab, there is one additional requirement. Ensure 'image_caption' is placed under 'Colab Notebooks' folder (which is created by Google automatically the first time you create a Colab notebook). Why? Since files within Google Colab are referenced with their full path starting all the way back to \"drive\".<br/>\n",
    "For example:<br/>\n",
    "filename = 'drive/My Drive/Colab Notebooks/Image Caption/Data/Flickr_8K/Labels/Flickr_8K.token.txt'<br/>\n",
    "filename = 'drive/My Drive/Colab Notebooks/Image Caption/Data/Flickr_30K/Labels/results.txt'<br/>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://drive.google.com/uc?id=1DrVeLA0fz0XYrPq0CZs6eAvRZbsB4GE1\" alt=\"Broken URL. Contact Anand Menon.\" width=\"60%\"/>\n",
    "</div>\n",
    "\n",
    "## Files Required\n",
    "The following files are required to run the code. All the files are shared, so you should ideally be able to copy the entire directory structure. Warning: It is over 2 GB, soon to be 10 GB.<br/>\n",
    "You will need to recreate the folder structure locally (with files and all) since the code writes files back to it.\n",
    "\n",
    "[Link to Data folder](https://drive.google.com/open?id=1bQtuMVTyaXCMGMC0XifOC2aKb1UGTSo6)\n",
    "\n",
    "### Dataset Files: Train, test, validate\n",
    "* Flickr_8K\n",
    "  * Data/Flickr_8K/Images/&lt;Images&gt;<br/>\n",
    "  * Data/Flickr_8K/Labels/&lt;Labels&gt;<br/>\n",
    "* Flickr_30K\n",
    "   * Data/Flickr_30K/Images/&lt;Images&gt;<br/>\n",
    "   * Data/Flickr_30K/Labels/&lt;Labels&gt;<br/>\n",
    "\n",
    "### Word Vectors File\n",
    "\n",
    "* Data/Vectors/&lt;Word vector file&gt;<br/>\n",
    "You only need one of these depending upon how well trained the vectors need to be for your need. The 300D is better (and much bigger) than the 100D file. Alternatively, feel free to use your own.\n",
    "    * glove.6B.100d.txt\n",
    "    * glove.6B.300d.txt\n",
    "\n",
    "### Transfer Learning Net: Features File\n",
    "In order to speed up training & testing, all features from our underlying transfer learning net are generated in-advance of the train/test code blocks. The first time you run this code, generate the features once by setting GEN_FEATURES = True above. Then set it back to False. The generated file is large, and it will take ~30 minutes to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drRBfB9mX-Ki"
   },
   "source": [
    "## Change Log\n",
    "1. 13 Apr: Switched to using GloVe encoding from a custom Embedding layer. The latter has the downside that if the number of training inputs are insufficient, the word embeddings it creates will be bad...and there is no easy way to tell if the number of training inputs are insufficient for this particular part of the net (the learning curves will give a global answer). Using GloVe word vectors eliminates this problem. In addition, it means the net is not having to learn the 'meaning' of words at the same time as it is learning to generate captions; hence less distractions for its nano-brain.\n",
    "2. 20 Apr: Dropped words that occur less frequently than 0.005% of the time in the corpus consisting of all captions. This tiny filter dramatically dropped the vocabulary from ~7500 words down to ~1100 words. Naturally, performance went up a bit with very little impact to the English of it all.\n",
    "3. 24 Apr: Switched the underlying pre-trained net over from VGG16 over to Inception v3. VGG16 is a big, plodding net and its architecture older than Inception. Inception also outperformed VGG16 in ImageNet, so overall it helped the performance a bit.\n",
    "4. 30 Apr: Cleaned up the code a bit and switched over to ResNet. The experiment here was to try and mimic human behaviour a bit more. Humans are great at fill in the blanks: The car is turning &lt;blank&gt;. So what is needed here is an identity function that helps recall prior learned patterns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49176,
     "status": "ok",
     "timestamp": 1588081865462,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "qTcM6SLmMe7Z",
    "outputId": "32eb9908-2a65-410d-9f76-34462a955449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN_FEATURES \t False\n",
      "TRAIN_MODEL \t True\n",
      "TL_MODEL \t Inceptionv3\n",
      "MAX_SAMPLES \t 5000\n",
      "DATASET \t Flickr_8K\n",
      "IN_COLAB \t False\n",
      "DIR_PREFIX \t ./Data/\n",
      "DIR_ALL_IMAGES \t ./Data/Flickr_8K/Images/\n",
      "FN_ALL_LABELS \t ./Data/Flickr_8K/Labels/Flickr_8K.captions.txt\n",
      "FN_GEN_MODEL \t ./Data/Model/model_transfer_learning_Inceptionv3.png\n",
      "FN_GEN_RESULT \t ./Data/Model/features_Inceptionv3_Flickr_8K.pkl\n",
      "FN_TRAIN_MODEL \t ./Data/Model/model_custom_Inceptionv3.png\n",
      "FN_WEIGHTS \t ./Data/Weights/weights.best_Inceptionv3_Flickr_8K.hdf5\n",
      "FN_TRAIN_REMODEL \t ./Data/Model/model_custom_from_weights_Inceptionv3.png\n",
      "FN_TEST_RESULTS \t ./Data/Model/results_Inceptionv3_Flickr_8K_1000.0\n",
      "FN_LEARN_CURVES \t ./Data/Model/results_lc_Inceptionv3_Flickr_8K_1000.0\n",
      "FN_BLEU \t ./Data/Model/results_lc_Inceptionv3_Flickr_8K_1000.0_BLEU.txt\n",
      "EMBEDDING_DIM \t 100\n",
      "FN_EMBEDDING \t ./Data/Vectors/glove.6B.100d.txt\n",
      "REMOVE_COLOURS \t True\n",
      "DEBUG \t\t False\n"
     ]
    }
   ],
   "source": [
    "#Clear all vars\n",
    "%reset -f\n",
    "\n",
    "#Common imports\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#Init code\n",
    "def initialize():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    import sys\n",
    "\n",
    "    # Init\n",
    "    dictInit = {}\n",
    "    dictInit['GEN_FEATURES'] = False  #Set to True if features need to be generated through the underlying pre-trained net\n",
    "    dictInit['TRAIN_MODEL'] = True  #Set to True if our custom net needs to learn the dataset\n",
    "    dictInit['TL_MODEL'] = 'Inceptionv3'  #Pre-trained model used for recognizing obects within images. One of Inceptionv3 or VGG16 trained on ImageNet.\n",
    "    dictInit['MAX_SAMPLES'] = 5000  #Maximum number of samples to train & test with. Set to -1 for training on full train set.\n",
    "    dictInit['DATASET'] = 'Flickr_8K'  #One of 'Flickr_8K', 'Flickr_30K'\n",
    "\n",
    "    #Change directory access paths depending upon where you are running\n",
    "    dictInit['IN_COLAB'] = 'google.colab' in sys.modules\n",
    "    if dictInit['IN_COLAB']:\n",
    "        from google.colab import drive  #Access google drive to load data\n",
    "        drive.mount('/content/drive')\n",
    "        dictInit['DIR_PREFIX'] = 'drive/My Drive/Colab Notebooks/image_caption/Data/'\n",
    "    else:\n",
    "        dictInit['DIR_PREFIX'] = './Data/'  #Local\n",
    "\n",
    "    #All directory & file names; Train, Validate, Test etc.\n",
    "    dictInit['DIR_ALL_IMAGES'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Images/'\n",
    "    dictInit['FN_ALL_LABELS'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Labels/' + dictInit['DATASET'] + '.captions.txt'\n",
    "    dictInit['FN_GEN_MODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_transfer_learning_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_GEN_RESULT'] = dictInit['DIR_PREFIX'] + 'Model/features_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '.pkl'\n",
    "    dictInit['FN_TRAIN_MODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_custom_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_WEIGHTS'] = dictInit['DIR_PREFIX'] + 'Weights/weights.best_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '.hdf5'\n",
    "    dictInit['FN_TRAIN_REMODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_custom_from_weights_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_TEST_RESULTS'] = dictInit['DIR_PREFIX'] + 'Model/results_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '_' + str(dictInit['MAX_SAMPLES']/5)\n",
    "    dictInit['FN_LEARN_CURVES'] = dictInit['DIR_PREFIX'] + 'Model/results_lc_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '_' + str(dictInit['MAX_SAMPLES']/5)\n",
    "    dictInit['FN_BLEU'] = dictInit['DIR_PREFIX'] + 'Model/results_lc_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '_' + str(dictInit['MAX_SAMPLES']/5) + '_BLEU.txt'\n",
    "\n",
    "    #GloVe embeddings\n",
    "    dictInit['EMBEDDING_DIM'] = 100\n",
    "    if dictInit['EMBEDDING_DIM'] == 100:\n",
    "        dictInit['FN_EMBEDDING'] = dictInit['DIR_PREFIX'] + 'Vectors/' + 'glove.6B.100d.txt'\n",
    "    elif dictInit['EMBEDDING_DIM'] == 300:\n",
    "        dictInit['FN_EMBEDDING'] = dictInit['DIR_PREFIX'] + 'Vectors/' + 'glove.6B.300d.txt'\n",
    "    \n",
    "    #Retain or drop colours in dataset\n",
    "    dictInit['REMOVE_COLOURS'] = True\n",
    "    \n",
    "    #Return\n",
    "    return dictInit\n",
    "\n",
    "#Initialize key variables\n",
    "dictInit = initialize()\n",
    "for key, val in dictInit.items():\n",
    "    print(key, '\\t', val)\n",
    "if (dictInit['MAX_SAMPLES'] == -1) or (dictInit['MAX_SAMPLES'] > 2000):\n",
    "    DEBUG = False\n",
    "else:\n",
    "    DEBUG = True\n",
    "print('DEBUG', '\\t\\t', DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate features via underlying pre-trained net\n",
    "if dictInit['GEN_FEATURES']:\n",
    "    from pickle import dump\n",
    "    from keras.preprocessing.image import load_img\n",
    "    from keras.preprocessing.image import img_to_array\n",
    "    from keras.models import Model\n",
    "    from keras.utils import plot_model\n",
    "    from os import listdir\n",
    "    \n",
    "    #Config\n",
    "    if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "        from keras.applications import InceptionV3\n",
    "        from keras.applications.inception_v3 import preprocess_input\n",
    "    else:        \n",
    "        from keras.applications.vgg16 import VGG16\n",
    "        from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "    #Extract features from each photo in the directory\n",
    "    def extract_features(directory):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #Load Model\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            model = InceptionV3(include_top=False, weights='imagenet')\n",
    "        else:\n",
    "            model = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "        #Remove the top layer, retaining the features generated up-to the layer below\n",
    "        #model.layers.pop()\n",
    "        model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "        #print(model.summary())\n",
    "        plot_model(model, to_file = dictInit['FN_GEN_MODEL'], show_shapes=True)\n",
    "\n",
    "        #Extract features from each photo\n",
    "        TARGET_SIZE = (0,0)\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            TARGET_SIZE = (299, 299)\n",
    "        else:\n",
    "            TARGET_SIZE=(224, 224)\n",
    "        counter = 0\n",
    "        features = dict()\n",
    "        start = time.time()\n",
    "        current = start\n",
    "        for fn in listdir(directory):\n",
    "            #Load image\n",
    "            image = load_img(directory+'/'+fn, target_size=TARGET_SIZE, interpolation='bicubic')\n",
    "\n",
    "            #Expand dims to include batch size\n",
    "            image = img_to_array(image)\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "            #Prepare & predict\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            features[fn] = feature\n",
    "            if counter % 1000 == 0:\n",
    "                if counter == 0:\n",
    "                    print(counter, 0, fn)\n",
    "                else:\n",
    "                    print(counter, 1000.0/(time.time()-current), (time.time()-current), time.time(), fn)\n",
    "                current = time.time()\n",
    "            counter += 1\n",
    "        return features\n",
    "\n",
    "    # Extract features from all images\n",
    "    features = extract_features(dictInit['DIR_ALL_IMAGES'])\n",
    "    print('# of Extracted Features: %d' % len(features), (current-start)/60.0)\n",
    "\n",
    "    # Save to file\n",
    "    dump(features, open(dictInit['FN_GEN_RESULT'], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Stores an instance of a sample\n",
    "class Xy:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, photo_id):\n",
    "        #Init\n",
    "        self.id = photo_id\n",
    "        self.features = None\n",
    "        self.captions = []\n",
    "        self.captions_bookends = []\n",
    "        self.prediction = None\n",
    "    \n",
    "    def add_caption(self, cap):\n",
    "        self.captions.append(cap)\n",
    "    \n",
    "    def add_bookends(self):\n",
    "        self.captions_bookends = ['startseq ' + cap + ' endseq' for cap in self.captions]\n",
    "        return self.captions_bookends\n",
    "    \n",
    "    def add_prediction(self, prediction):\n",
    "        self.prediction = prediction.replace('startseq ','').replace(' endseq','')\n",
    "    \n",
    "    #Print\n",
    "    def toString(self):\n",
    "        res = self.id + '\\n'\n",
    "        if self.features is not None:\n",
    "            res += '\\t' + str(self.features.shape) + '\\n'\n",
    "        for cap in self.captions:\n",
    "            res += \"\\t'\" + cap + \"'\\n\"\n",
    "        if self.prediction is not None:\n",
    "            res += '\\t' + self.prediction + '\\n'\n",
    "        return res\n",
    "\n",
    "#Stores all samples belonging to a particular data; train, dev test.\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Attribute name: One of Train, Dev, Test.\n",
    "    Attribute samples: List of samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, samples):\n",
    "        #Init\n",
    "        self.name = name\n",
    "        self.samples = samples\n",
    "        \n",
    "        #Create dict for easy access\n",
    "        self.dictSamples = {}\n",
    "        for xy in self.samples:\n",
    "            self.dictSamples[xy.id] = xy\n",
    "    \n",
    "    #Add features samples\n",
    "    def add_features(self, dict_features_all):\n",
    "        dict_features = {id: dict_features_all[id] for id in self.dictSamples}\n",
    "        for id, features in dict_features.items():\n",
    "            self.dictSamples[id].features = features[0]  #Else shape = (1,N)\n",
    "\n",
    "    #Create inputs images, input sequences and output words for an image\n",
    "    def create_inputs(self, ca):\n",
    "        \"\"\"\n",
    "        Create inputs images, input sequences and output words for an image.\n",
    "        \"\"\"\n",
    "        #Foreach sample...\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        for xy in self.samples:\n",
    "            #Foreach bookended caption, encode text to integers\n",
    "            sequences = ca.tokenizer.texts_to_sequences(xy.add_bookends())\n",
    "\n",
    "            #Foreach sequence, form inputs incrementing 1 word at a time\n",
    "            for seq in sequences:\n",
    "                #Split sequence into multiple X,y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    #Split & pad input, encode output as one-hot\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen = ca.max_seq_len)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes = ca.vocab_size)[0]\n",
    "\n",
    "                    #Store\n",
    "                    X1.append(xy.features)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "        #Convert to numpy arrays\n",
    "        self.X1 = np.array(X1)\n",
    "        self.X2 = np.array(X2)\n",
    "        self.y = np.array(y)\n",
    "        \n",
    "    #Print\n",
    "    def toString(self):\n",
    "        res = self.name + '\\n'\n",
    "        for xy in self.samples:\n",
    "            res += '\\t' + xy.toString() + '\\n'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 5000\n",
      "# of samples: 1000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Helper method for loading provided filename\n",
    "def read_file(filename, max_lines=-1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        if max_lines > 0:\n",
    "            doc = [next(f).replace('\\n','') for x in range(max_lines)]  #Read only the # of lines specified\n",
    "        else:\n",
    "            doc = f.read().splitlines()  #Read all lines\n",
    "        return doc\n",
    "\n",
    "#Load captions from input file\n",
    "def load_captions():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Read file\n",
    "    doc = read_file(dictInit['FN_ALL_LABELS'], dictInit['MAX_SAMPLES'])\n",
    "    print(type(doc), len(doc))\n",
    "    if DEBUG:\n",
    "        print(doc[0:10])\n",
    "\n",
    "    #--Extract name and captions associated with each image and save in Xy--\n",
    "    #If dataset Flickr_8K...\n",
    "    lines = []\n",
    "    if dictInit['DATASET'] == 'Flickr_8K':\n",
    "        #Split on tabs\n",
    "        for line in doc:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            tokens = line.split('\\t')\n",
    "            #Skip '_577c3a7d70....#1' part of name  #1001773457_577c3a7d70.jpg#1\tTwo dogs are playing with each other .\n",
    "            photo_id = tokens[0].split('_')[0]+'.'+tokens[0].split('.')[1][0:-2]\n",
    "            photo_caption = tokens[1]  #Rest of words\n",
    "            lines.append((photo_id, photo_caption))\n",
    "    elif dictInit['DATASET'] == 'Flickr_30K':\n",
    "        #Split on pipes\n",
    "        first = True  #Skip header \"image_name| comment_number| comment\"\n",
    "        for line in doc:\n",
    "            if (first) or (line.strip() == \"\"):\n",
    "                first = False\n",
    "                continue\n",
    "            tokens = line.split('|')\n",
    "            photo_id = tokens[0]\n",
    "            photo_caption = tokens[2]\n",
    "            lines.append((photo_id, photo_caption))\n",
    "\n",
    "    #Concatenate descriptions by image\n",
    "    dictCaptions = {}  #Key = Photo identifier, Value = Xy containing photo id & list of captions\n",
    "    for tokens in lines:\n",
    "        id = tokens[0]\n",
    "        caption = tokens[1]\n",
    "        if id not in dictCaptions:\n",
    "            dictCaptions[id] = Xy(id)\n",
    "        dictCaptions[id].add_caption(caption)\n",
    "    return dictCaptions\n",
    "\n",
    "#Load captions\n",
    "dictCaptions = load_captions()\n",
    "print('# of samples:', len(dictCaptions), '\\n')\n",
    "if DEBUG:\n",
    "    counter = 0\n",
    "    for key,val in dictCaptions.items():\n",
    "        print(key, val.toString())\n",
    "        if counter>1:\n",
    "            break;\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53043,
     "status": "ok",
     "timestamp": 1588081869368,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "nWkwIQrS9mKR",
    "outputId": "1f4a8819-0f8f-4281-bc26-7ec9d70ffb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3.01\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import time\n",
    "\n",
    "\n",
    "def priorTag(i, pos):\n",
    "    if i > 0:\n",
    "        return pos[i-1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def nextTag(i, length, pos):\n",
    "    if i <= length-2:\n",
    "        return pos[i+1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def isLastTag(i, length):\n",
    "    if i == length-1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def isAdjective(tag):\n",
    "    if tag is None:\n",
    "        return False\n",
    "    if (tag[1]=='JJ') or (tag[1]=='ADJ'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def isConjunction(tag):\n",
    "    if tag is None:\n",
    "        return False\n",
    "    if (tag[1]=='CC') or (tag[1]=='CONJ'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def isGerund(tag):\n",
    "    if tag is None:\n",
    "        return False\n",
    "    if (tag[1]=='VBG'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def isPreposition(tag):\n",
    "    if tag is None:\n",
    "        return False\n",
    "    if (tag[1]=='IN'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def isNoun(tag):\n",
    "    if tag is None:\n",
    "        return False\n",
    "    if (tag[1]=='NOUN'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def isVerb(tag):\n",
    "    if tag is None:\n",
    "        return False\n",
    "    if tag[1].startswith('VB'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def getPosPattern(pos):\n",
    "    return [tag[1] for tag in pos]\n",
    "\n",
    "\n",
    "#Cleanup text\n",
    "def cleanup_text(dictCaptions):\n",
    "    \"\"\"\n",
    "    Remove punctuation & special characters, lowercase captions, remove hanging 's, remove extra spaces.\n",
    "    Returns the sum of # of words across all captions. Includes duplicate counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Attributes to be replaced\n",
    "    dictReplacements = {'little girls':'children', 'little girl':'child', 'little boys':'children', 'little boy':'child', \n",
    "                        'young girls':'children', 'young girl':'child', 'young boys':'children', 'young boy':'child', \n",
    "                        'gentleman':'person', 'lady':'person', 'youths':'people', 'guys ':'people ', 'guy ':'person ',\n",
    "                        ' boys':' children', 'girls':'children', ' boy ':' child ', ' girl ':' child ', 'kids':'children',\n",
    "                        'females':'people', 'males':'people', 'female':'person', ' male ':' person ', ' man ':' person ', 'woman':'person', ' men ':' people ', 'women':'people',\n",
    "                        ' 1 ':' one ', ' 2 ':' two ', ' 3 ':' three ', ' 4 ':' four ', ' 5 ':' five ', ' 6 ':' six ', ' 7 ':' seven ',\n",
    "                        'baseball cap':'cap', 'ball cap':'cap', 'backwards ':'', 'top hat':'hat', ' tshirt':' shirt', \n",
    "                        'officer':'person', 'policeman':'person', 'police man':'person', '':'', '':'', '':'', \n",
    "                        'babies':'children', 'infants':'children', 'infant':'child', 'person children':'children', 'toddlers':'children', \n",
    "                        'toddler':'child', 'child child':'child', 'baby child':'child', 'baby':'child', '':'', '':'', '':'',\n",
    "                       }\n",
    "\n",
    "    dictRegexReplacements = {'^(man)':'person', '^(boy) ':' child ', '^(girl) ':' child ',\n",
    "                             '^1 ':'one ', '^2 ':'two ', '^3 ':'three', '^4 ':'four ', '^5 ':'five ', '^6 ':'six ', '^7 ':'seven ',\n",
    "    #                          '':'', '':'', '':'', '':'', '':'', '':'', '':'', '':'', '':''\n",
    "                            }\n",
    "\n",
    "    posTemplate = [['CD', 'VBZ', 'IN', 'JJ', 'DT', 'JJ', 'VBZ', 'IN', 'JJ'], ['CD', 'VBZ', 'IN', 'JJ', 'DT', 'JJ', 'IN', 'JJ'], \n",
    "                   ['CD', 'IN', 'DT', 'JJ', 'NN', 'CD', 'IN', 'DT', 'JJ', 'NN'], ['CD', 'IN', 'DT', 'JJ', 'NN'], \n",
    "                   ['JJ', 'TO', 'DT', 'JJ'], ['NN', 'CC', 'JJ']]\n",
    "\n",
    "    #Patterns\n",
    "    p1 = re.compile(r'( a [aeiou])+')  #Match a apple\n",
    "    p2 = re.compile(r'( an [b-df-hj-np-tv-z])+')  #Match an ruler\n",
    "\n",
    "    #Pre-process\n",
    "    for id, xy in dictCaptions.items():\n",
    "        clean = []\n",
    "        for caption in xy.captions:\n",
    "            #Punctuation\n",
    "            cap = caption.lower().replace(\" 's\", \"s\").strip()\n",
    "            cap = re.sub('[^A-Za-z0-9 ]+', '', cap)  #Remove special chars\n",
    "            cap = re.sub('( )+', ' ', cap)\n",
    "\n",
    "            #Replace equivalence classes\n",
    "            for key,value in dictReplacements.items():\n",
    "                cap = cap.replace(key, value)\n",
    "            for key, value in dictRegexReplacements.items():\n",
    "                cap = re.sub(key, value, cap)\n",
    "            cap = cap.strip()\n",
    "\n",
    "            #Init pos\n",
    "            pos = nltk.pos_tag(cap.split(' '))\n",
    "            length = len(pos)\n",
    "            tokens = []\n",
    "            mask = [1]*(length+2)\n",
    "\n",
    "            #Replace entire strings that match posTemplate\n",
    "            posPattern = getPosPattern(pos)\n",
    "            pps = ' '.join(posPattern)\n",
    "            for pt in posTemplate:\n",
    "                pts = ' '.join(pt)\n",
    "                if pts in pps:\n",
    "                    start = -1\n",
    "                    i, j = 0, 0\n",
    "                    while i<len(posPattern) and j<len(pt):\n",
    "                        if posPattern[i] == pt[j]:\n",
    "                            if start == -1:\n",
    "                                start = i\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            j = 0\n",
    "                            start = -1\n",
    "                            #Check from the start again of template pattern\n",
    "                            if posPattern[i] == pt[j]:\n",
    "                                if start == -1:\n",
    "                                    start = i\n",
    "                                j += 1\n",
    "                        i += 1\n",
    "                    for j in range(start, start+len(pt)):\n",
    "                        mask[j] = 0\n",
    "                    break;  #No need to search for further patterns within same string\n",
    "\n",
    "            for i in range(len(pos)):\n",
    "                tag = pos[i]\n",
    "                if mask[i] == 1:\n",
    "                    #If next tag is a determinant, replace all 3 tags 'black & brown'\n",
    "                    if isAdjective(tag) and tag[0]!='other' and not isLastTag(i, length):\n",
    "                        pTag = priorTag(i, pos)\n",
    "                        nTag = nextTag(i, length, pos)\n",
    "                        if isConjunction(nTag):\n",
    "                            mask[i] = 0\n",
    "                            mask[i+1] = 0\n",
    "                            mask[i+2] = 0\n",
    "                        elif isGerund(nTag):\n",
    "                            mask[i] = 0\n",
    "                            mask[i+1] = 0\n",
    "                        elif isVerb(pTag):\n",
    "                            mask[i] = 1\n",
    "                        else:\n",
    "                            mask[i] = 0\n",
    "                        if isPreposition(pTag) and isVerb(nTag):  #If the preceeding tag is a preposition & the next a verb, delete preposition\n",
    "                            mask[i-1] = 0\n",
    "\n",
    "            #Only keep words with mask = 1\n",
    "            cap = []\n",
    "            for i in range(len(pos)):\n",
    "                if mask[i] == 1:\n",
    "                    cap.append(pos[i][0])\n",
    "            cap = ' '.join(cap)\n",
    "\n",
    "            #Fix grammar for a vs. an followed by vowels / consonants\n",
    "            cap = ' ' + cap\n",
    "            match = p1.match(cap)\n",
    "            matches = re.finditer(p1, cap)\n",
    "            for match in matches:\n",
    "                cap = cap[0:match.start(1)] + ' |' + cap[match.start(1)+2:match.end(1)] + cap[match.end(1):len(cap)]\n",
    "            cap = cap.replace('|', 'an')\n",
    "            matches = re.finditer(p2, cap)\n",
    "            for match in matches:\n",
    "                cap = cap[0:match.start(1)+1] + ' |' + cap[match.start(1)+3:match.end(1)] + cap[match.end(1):len(cap)]\n",
    "            cap = cap.replace(' |', 'a')\n",
    "            \n",
    "            #Save\n",
    "            clean.append(cap)\n",
    "        xy.captions = clean\n",
    "\n",
    "#Cleanup text\n",
    "start = time.time()\n",
    "cleanup_text(dictCaptions)\n",
    "print('Time taken:', round(time.time()-start,2))\n",
    "if DEBUG:\n",
    "    counter = 0\n",
    "    for key,val in dictCaptions.items():\n",
    "        print(key, val.toString())\n",
    "        if counter>1:\n",
    "            break;\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53033,
     "status": "ok",
     "timestamp": 1588081869369,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0f9XTjDPAo8J",
    "outputId": "0283e774-2e57-4a69-90bf-694433331648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total words across all captions: 54656\n",
      "# of unique words: 2949 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Count frequency of all words\n",
    "def count_word_freq(dictCaptions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Aggregate words across all captions\n",
    "    allWordsInCaptions = []\n",
    "    for id, xy in dictCaptions.items():\n",
    "        for caption in xy.captions:\n",
    "            allWordsInCaptions.extend(caption.split(' '))\n",
    "    numWords = len(allWordsInCaptions)\n",
    "\n",
    "    #Count\n",
    "    dictWords = {}\n",
    "    for word in allWordsInCaptions:\n",
    "        if word not in dictWords:\n",
    "            dictWords[word] = 0\n",
    "        dictWords[word] += 1\n",
    "    return numWords, dictWords\n",
    "\n",
    "#Count frequency of all words\n",
    "numWords, dictWords = count_word_freq(dictCaptions)\n",
    "print('# of total words across all captions:', numWords)\n",
    "print('# of unique words:', len(dictWords), '\\n')\n",
    "if DEBUG:\n",
    "    for key in random.sample(list(dictWords.keys()), 5):\n",
    "        print(key+':', dictWords[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53328,
     "status": "ok",
     "timestamp": 1588081869678,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0p16yVjgA44w",
    "outputId": "e47bc0e4-e266-4628-fd8a-507b42e75332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1000\n",
      "# of total words across all captions: 52472\n",
      "# of unique words: 1206 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop infrequent words from captions, reducing vocabulary size\n",
    "def drop_infrequent(dictCaptions, dictWords):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Form dict of frequent words\n",
    "    truncated = []\n",
    "    for word in dictWords.keys():\n",
    "        if dictWords[word]/numWords > 0.00004:  #0.005%\n",
    "            truncated.append(word)\n",
    "    dictWordsTrunc = dict.fromkeys(truncated, 0)\n",
    "\n",
    "    #Drop infrequent\n",
    "    for id, xy in dictCaptions.items():\n",
    "        newCaptions = []\n",
    "        for caption in xy.captions:\n",
    "            newCaptions.append(' '.join([word for word in caption.split(' ') if word in dictWordsTrunc]))\n",
    "        xy.captions = newCaptions\n",
    "\n",
    "#Drop infrequent words\n",
    "drop_infrequent(dictCaptions, dictWords)\n",
    "print('Total samples:', len(dictCaptions))\n",
    "print('# of total words across all captions:', sum([len(cap.split(' ')) for xy in dictCaptions.values() for cap in xy.captions]))\n",
    "print('# of unique words:', len(set([words for sublist in [cap.split(' ') for xy in dictCaptions.values() for cap in xy.captions] for words in sublist])), '\\n')\n",
    "if DEBUG:\n",
    "    for key in random.sample(list(dictCaptions.keys()), 2):\n",
    "        print(key, dictCaptions[key].toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 sequence lengths: [30, 27, 26]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skew: 0.815\n",
      "99th Percentile: 24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "\n",
    "#4 May: Limit caption length to N'th percentile\n",
    "seq_lengths = [len(cap.split(' ')) for xy in dictCaptions.values() for cap in xy.captions]\n",
    "seq_lengths.sort(reverse=True)\n",
    "print('Top 3 sequence lengths:', seq_lengths[0:3])\n",
    "\n",
    "#Histogram\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "fig=plt.figure(figsize=(7, 5))\n",
    "numBins = 40\n",
    "plt.hist(seq_lengths, bins=numBins)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Sequence Length Bins', color='black')\n",
    "ax.set_ylabel('Frequency', color='black')\n",
    "ax.spines['bottom'].set_color('lightgrey')\n",
    "ax.spines['left'].set_color('lightgrey')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "print('Skew:', round(skew(seq_lengths),3))\n",
    "\n",
    "#Get the N'th percentile\n",
    "p99 = int(np.percentile(seq_lengths, 99.8))  #N'th percentile\n",
    "print('99th Percentile:', p99)\n",
    "\n",
    "#Trunc captions to p99\n",
    "for id, xy in dictCaptions.items():\n",
    "    newCaptions = []\n",
    "    for caption in xy.captions:\n",
    "        newCaptions.append(' '.join(caption.split(' ')[0:p99]))\n",
    "    xy.captions = newCaptions\n",
    "\n",
    "#Print\n",
    "counter = 0\n",
    "if DEBUG:\n",
    "    for id, xy in dictCaptions.items():\n",
    "        print(id)\n",
    "        for caption in xy.captions:\n",
    "            print('\\t', caption)\n",
    "        if counter>1:\n",
    "            break;\n",
    "        counter += 1\n",
    "    \n",
    "    #Print longest description\n",
    "    print('\\nLongest description belongs to...')\n",
    "    temp = -1\n",
    "    lid = -1\n",
    "    for id, xy in dictCaptions.items():\n",
    "            for caption in xy.captions:\n",
    "                if len(caption.split(' ')) > temp:\n",
    "                    temp = len(caption.split(' '))\n",
    "                    lid = id\n",
    "    print(lid, temp)\n",
    "    print(dictCaptions[lid].toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54232,
     "status": "ok",
     "timestamp": 1588081870593,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "M3_M8jPqCRWX",
    "outputId": "80b91ba9-9850-479e-e53d-c2045bb846dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 : 640 160 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Create train, dev & test datasets\n",
    "def split_dataset(dictCaptions):\n",
    "    #Split\n",
    "    X = list(dictCaptions.keys())\n",
    "    y = list(dictCaptions.values())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)  #Train+Dev vs. Test\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.20, random_state=42)  #Train vs. Dev\n",
    "    \n",
    "    #Save\n",
    "    dsTrain = Dataset('Train', y_train)\n",
    "    dsDev = Dataset('Dev', y_dev)\n",
    "    dsTest = Dataset('Test', y_test)\n",
    "    return dsTrain, dsDev, dsTest\n",
    "\n",
    "#Split\n",
    "dsTrain, dsDev, dsTest = split_dataset(dictCaptions)\n",
    "print(len(dsTrain.samples)+len(dsDev.samples)+len(dsTest.samples), ':', len(dsTrain.samples), len(dsDev.samples), len(dsTest.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence length: 26\n",
      "Vocabulary size: 1128 \n",
      "\n",
      "640 \t 160 \t 200\n",
      "(33319, 2048) \t (33319, 26) \t (33319, 1128)\n",
      "(8534, 2048) \t (8534, 26) \t (8534, 1128)\n",
      "(10294, 2048) \t (10294, 26) \t (10294, 1128)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "class CorpusAttributes:\n",
    "    \"\"\"\n",
    "    Attribute tokenizer: Keras tokenizer for converting words to integers.\n",
    "    Attribute max_seq_len: Maximum sequence length amongst all captions; includes bookends.\n",
    "    Attribute vocab_size: Number of words in vocabulary + 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, max_seq_len, vocab_size):\n",
    "        #Init\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "#Convert the dataset into the right format for our custom net + LSTM\n",
    "def process_dataset(dsTrain, dsDev, dsTest):\n",
    "    #Create tokenizer and fit on train set\n",
    "    only_captions = [captions for xy in dsTrain.samples for captions in xy.add_bookends()]\n",
    "    max_seq_len = max([len(cap.split(' ')) for cap in only_captions])\n",
    "    \n",
    "    #Tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(only_captions)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    ca = CorpusAttributes(tokenizer, max_seq_len, vocab_size)\n",
    "    \n",
    "    #Load & save transfer learning features from pickle file\n",
    "    dict_features_all = pickle.load(open(dictInit['FN_GEN_RESULT'], 'rb'))\n",
    "    dsTrain.add_features(dict_features_all)\n",
    "    dsDev.add_features(dict_features_all)\n",
    "    dsTest.add_features(dict_features_all)\n",
    "    \n",
    "    #Create inputs for upcoming NN.fit() method\n",
    "    dsTrain.create_inputs(ca)\n",
    "    dsDev.create_inputs(ca)\n",
    "    dsTest.create_inputs(ca)\n",
    "    \n",
    "    #Return\n",
    "    return ca\n",
    "\n",
    "#Process dataset\n",
    "ca = process_dataset(dsTrain, dsDev, dsTest)\n",
    "print('Longest sequence length:', ca.max_seq_len)\n",
    "print('Vocabulary size:', ca.vocab_size, '\\n')\n",
    "print(len(dsTrain.samples), '\\t', len(dsDev.samples), '\\t', len(dsTest.samples))\n",
    "print(dsTrain.X1.shape, '\\t', dsTrain.X2.shape, '\\t', dsTrain.y.shape)\n",
    "print(dsDev.X1.shape, '\\t', dsDev.X2.shape, '\\t', dsDev.y.shape)\n",
    "print(dsTest.X1.shape, '\\t', dsTest.X2.shape, '\\t', dsTest.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    counter = 0\n",
    "    for val in dsTrain.X1:\n",
    "        print(val.shape, val)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            break;\n",
    "    print(dsTrain.samples[0].toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ug3D8Im8HQqZ"
   },
   "source": [
    "## Create & Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30820,
     "status": "ok",
     "timestamp": 1588082067473,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hRp9_ly-8urm",
    "outputId": "7ee7d535-d2f6-46cf-d321-022145a64fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors...\n",
      "# of vectors loaded: 400000\n",
      "Time taken: 17.9\n",
      "(1128, 100)\n"
     ]
    }
   ],
   "source": [
    "#Create a matrix of GloVe word vectors of all words in our vocabulary\n",
    "def createEmbeddingMatrix(ca):\n",
    "    #Load GloVe word vectors\n",
    "    start = time.time()\n",
    "    dict_glove = {}\n",
    "    lines = read_file(dictInit['FN_EMBEDDING'])\n",
    "    with open(dictInit['FN_EMBEDDING']) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            dict_glove[word] = coefs\n",
    "    print('# of vectors loaded:', len(dict_glove))\n",
    "    print('Time taken:', round(time.time()-start,1))\n",
    "\n",
    "    #Create embedding matrix\n",
    "    embedding_matrix = np.zeros((ca.vocab_size, dictInit['EMBEDDING_DIM']))\n",
    "    for word, i in ca.tokenizer.word_index.items():\n",
    "        embedding_vector = dict_glove.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  #Words not found in GloVe will be all-zeros\n",
    "    return embedding_matrix\n",
    "\n",
    "#Create a matrix of GloVe word vectors of all words in our vocabulary\n",
    "if dictInit['TRAIN_MODEL']:\n",
    "    print('Loading vectors...')\n",
    "    embedding_matrix = createEmbeddingMatrix(ca)\n",
    "    print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63520,
     "status": "ok",
     "timestamp": 1588082100188,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "T_EtOFVe2egn",
    "outputId": "296df2c5-928d-48b3-f17b-681bf377556a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 1589544026.3525214\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 26)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 26, 100)      112800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 65)           133185      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 65)           43160       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 65)           0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 65)           4290        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1128)         74448       dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 367,883\n",
      "Trainable params: 255,083\n",
      "Non-trainable params: 112,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 33319 samples, validate on 8534 samples\n",
      "Epoch 1/3\n",
      " - 100s - loss: 4.1260 - accuracy: 0.2666 - val_loss: 3.6115 - val_accuracy: 0.3235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.61145, saving model to ./Data/Weights/weights.best_Inceptionv3_Flickr_8K.hdf5\n",
      "Epoch 2/3\n"
     ]
    }
   ],
   "source": [
    "if dictInit['TRAIN_MODEL']:\n",
    "    from keras.applications import densenet\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Embedding\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers.merge import add\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.utils import plot_model\n",
    "    \n",
    "    #Define the captioning model\n",
    "    def define_model(ca):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #Transfer learning model\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            branch1_in = Input(shape=(2048,))\n",
    "        else:\n",
    "            branch1_in = Input(shape=(4096,))\n",
    "        branch1_drop = Dropout(0.5)(branch1_in)\n",
    "        branch1_dense = Dense(65, activation='relu')(branch1_drop)\n",
    "        \n",
    "        #LSTM model\n",
    "        branch2_in = Input(shape=(ca.max_seq_len,))\n",
    "        # branch2_embed = Embedding(ca.vocab_size, 300, mask_zero=True)(branch2_in)\n",
    "        branch2_embed = Embedding(ca.vocab_size, dictInit['EMBEDDING_DIM'], weights=[embedding_matrix], input_length=ca.max_seq_len, trainable=False)(branch2_in)\n",
    "        branch2_lstm = LSTM(65)(branch2_embed)\n",
    "        \n",
    "        #Hidden layer\n",
    "        hidden_add = add([branch1_dense, branch2_lstm])\n",
    "        hidden_dense = Dense(65, activation='relu')(hidden_add)\n",
    "        hidden_out = Dense(ca.vocab_size, activation='softmax')(hidden_dense)\n",
    "        \n",
    "        #Model: [image, seq] --> [word]\n",
    "        model = Model(inputs=[branch1_in, branch2_in], outputs=hidden_out)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "        print(model.summary())\n",
    "        plot_model(model, to_file=dictInit['FN_TRAIN_MODEL'], show_shapes=True)\n",
    "        return model\n",
    "\n",
    "    #Define model\n",
    "    start = time.time()\n",
    "    print('Start time:', start)\n",
    "    model = define_model(ca)\n",
    "    checkpoint = ModelCheckpoint(dictInit['FN_WEIGHTS'], monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    #Fit\n",
    "    history = model.fit([dsTrain.X1, dsTrain.X2], dsTrain.y, epochs=3, \n",
    "              verbose=2, callbacks=[checkpoint], validation_data=([dsDev.X1, dsDev.X2], dsDev.y))\n",
    "    print('\\nTotal time taken:', round(time.time()-start,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KJvdPpDD19E"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot learning curves\n",
    "def plotCurve(history, ax, metric1, metric2, title, title_x, title_y):\n",
    "    ax.plot(history.history[metric1], '-*', label=metric1)\n",
    "    ax.plot(history.history[metric2], '--*', label=metric2)\n",
    "    ax.set_title('Training & Validation Loss', color='black')\n",
    "    ax.legend(loc='best', shadow=True)\n",
    "    ax.set_xlabel(title_x, color='black')\n",
    "    ax.set_ylabel(title_y, color='black')\n",
    "    ax.spines['bottom'].set_color('lightgrey')\n",
    "    ax.spines['left'].set_color('lightgrey')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', colors='black')\n",
    "    ax.tick_params(axis='y', colors='black')\n",
    "\n",
    "#Plot learning curves\n",
    "if dictInit['TRAIN_MODEL']:\n",
    "    #Init plot\n",
    "    plt.rcParams['figure.facecolor'] = 'white'\n",
    "    fig=plt.figure(figsize=(22, 24))\n",
    "    \n",
    "    #Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    plotCurve(history, ax1, 'loss', 'val_loss', 'Training & Valiation Loss', '# of Epochs', 'Loss')\n",
    "    plotCurve(history, ax2, 'accuracy', 'val_accuracy', 'Training & Valiation Accuracy', '# of Epochs', 'Accuracy')\n",
    "    plt.savefig(dictInit['FN_LEARN_CURVES'] + '_' + datetime.today().strftime('%Y-%m-%d-%H:%M:%S') + '.png',\n",
    "                                                                            bbox_inches='tight', facecolor='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67278,
     "status": "ok",
     "timestamp": 1588082103956,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hP4meSU1vNQc",
    "outputId": "a0950deb-daaa-4187-c52c-d5354af6bcba"
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(dictLookup, model, ca, photo_features):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    #Iteratively predict one word at a time\n",
    "    in_text = 'startseq'\n",
    "    for i in range(ca.max_seq_len):\n",
    "        #Encode & Pad\n",
    "        sequence = ca.tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen = ca.max_seq_len)\n",
    "        \n",
    "        #Predict next word\n",
    "        yhat = model.predict([photo_features,sequence], verbose=0)\n",
    "        yhat = argmax(yhat)  #Index of highest probability\n",
    "        word = dictLookup.get(yhat)\n",
    "        \n",
    "        #Stop if we cannot find a word within our vocabulary or predict the end\n",
    "        if (word is None) or (word == 'endseq'):\n",
    "            break\n",
    "        \n",
    "        #Append to input\n",
    "        in_text += ' ' + word\n",
    "        \n",
    "    #Return\n",
    "    return in_text\n",
    "\n",
    "#Evaluate model\n",
    "def evaluate_model(model, ca, dsTest):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    #Create a reverse lookup to go from integer encodings back to english words\n",
    "    dictLookup = dict([(value,key) for (key,value) in ca.tokenizer.word_index.items()])\n",
    "    \n",
    "    #Predict caption foreach test case...\n",
    "    for xy in dsTest.samples:\n",
    "        yhat = generate_desc(dictLookup, model, ca, np.expand_dims(xy.features, axis=0))  #Add batch size to features\n",
    "        xy.add_prediction(yhat)\n",
    "    \n",
    "    #Format input for BLEU, Compute sentence level BLEU score\n",
    "    dictBleu = {}\n",
    "    candidate, references = list(), list()\n",
    "    for xy in dsTest.samples:\n",
    "        references.append([cap.split(' ') for cap in xy.captions])\n",
    "        candidate.append(xy.prediction.split(' '))\n",
    "        sbleu1 = str(round(sentence_bleu([cap.split(' ') for cap in xy.captions], xy.prediction.split(' '), weights=(1.0, 0, 0, 0)),4))\n",
    "        sbleu2 = str(round(sentence_bleu([cap.split(' ') for cap in xy.captions], xy.prediction.split(' '), weights=(0.5, 0.5, 0, 0)),4))\n",
    "        sbleu3 = str(round(sentence_bleu([cap.split(' ') for cap in xy.captions], xy.prediction.split(' '), weights=(0.3, 0.3, 0.3, 0)),4))\n",
    "        sbleu4 = str(round(sentence_bleu([cap.split(' ') for cap in xy.captions], xy.prediction.split(' '), weights=(0.25, 0.25, 0.25, 0.25)),4))\n",
    "        dictBleu[xy.id] = (xy, [sbleu1, sbleu2, sbleu3, sbleu4])\n",
    "    \n",
    "    #Compute BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(references, candidate, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(references, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(references, candidate, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "    #Return\n",
    "    return dictBleu\n",
    "\n",
    "#Load custom from weights if training was not carried out\n",
    "print('Evaluating model...')\n",
    "if not dictInit['TRAIN_MODEL']:\n",
    "    model = load_model(dictInit['FN_WEIGHTS'])\n",
    "#     plot_model(model, to_file = dictInit['FN_TRAIN_REMODEL'], show_shapes=True)\n",
    "\n",
    "#Evaluate BLEU score\n",
    "start = time.time()\n",
    "dictBleu = evaluate_model(model, ca, dsTest)\n",
    "print('\\nTime taken:', round(time.time()-start,1))\n",
    "\n",
    "# for key, (xy, sbleu) in dictBleu.items():\n",
    "#     print(key)\n",
    "#     print(sbleu, '\\t', xy.prediction)\n",
    "#     for cap in xy.captions:\n",
    "#         print('\\t', cap)\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save sentence BLEU scores\n",
    "with open(dictInit['FN_BLEU'], 'w') as fn_bleu:\n",
    "    for key, (xy, sbleus) in dictBleu.items():\n",
    "        fn_bleu.write(xy.id + '\\t' + xy.prediction + '\\n')\n",
    "        for sb in sbleus:\n",
    "            fn_bleu.write(sb + '\\t')\n",
    "        fn_bleu.write('\\n')\n",
    "        for cap in xy.captions:\n",
    "            fn_bleu.write('\\t' + cap + '\\n')\n",
    "        fn_bleu.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1LmTvQ8D4ONvEnj3npjPvpRvgQTOLdUGi"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21650,
     "status": "ok",
     "timestamp": 1588084779903,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "DJprBlgr--wM",
    "outputId": "e6f11dd4-9cd6-4a34-9dae-b78e47b51de3"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from textwrap import wrap\n",
    "\n",
    "#Plot sample test images\n",
    "def plotSamples(dsTest):\n",
    "    #Get random images\n",
    "    testIDs = random.sample(dsTest.dictSamples.keys(), 16)\n",
    "    print(testIDs)\n",
    "    \n",
    "    #Init plot\n",
    "    plt.rcParams['figure.facecolor'] = 'black'\n",
    "    fig=plt.figure(figsize=(22, 24))\n",
    "    fig.patch.set_facecolor('black')\n",
    "    columns = 4\n",
    "    rows = 4\n",
    "    \n",
    "    #Plot\n",
    "    for i in range(1, columns*rows+1):\n",
    "        #Prediction\n",
    "        yhat = dsTest.dictSamples[testIDs[i-1]].prediction\n",
    "        \n",
    "        #Plot image and description together\n",
    "        img = mpimg.imread(dictInit['DIR_ALL_IMAGES']+testIDs[i-1])\n",
    "        ax = fig.add_subplot(rows, columns, i)\n",
    "        ax.set_title(\"\\n\".join(wrap(yhat, 50)), color='white')\n",
    "        ax.set_facecolor('xkcd:black')\n",
    "        plt.imshow(img)\n",
    "    plt.savefig(dictInit['FN_TEST_RESULTS']+'_'+datetime.today().strftime('%Y-%m-%d-%H:%M:%S')+'.png',\n",
    "                bbox_inches='tight', facecolor='black', dpi = 50)\n",
    "    plt.show()\n",
    "\n",
    "#Plot sample test images\n",
    "plotSamples(dsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Papineni K, Roukos S, Ward T, Zhu W, (Jul 2002), [BLEU: a method for automatic evaluation of machine translation, Proceedings of the 40th Annual Meeting on Association for Computational Linguistics](https://dl.acm.org/doi/10.3115/1073083.1073135)\n",
    "\n",
    "Szegedy C et al., (Sep 2014), [Going Deeper with Convolutions,  arXiv:1409.4842 cs.CV](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "Simonyan K, Zisserman A, (Sep 2014), [Very Deep Convolutional Networks for Large-Scale Image Recognition, arXiv:1409.1556 cs.CV](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Tanti M, Gatt A, Camilleri K.P., (Aug 2017), [What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?, arXiv:1708.02043 cs.CL](https://arxiv.org/abs/1708.02043)\n",
    "\n",
    "Brownlee J., (Jun 2019), [How to Develop a Deep Learning Photo Caption Generator from Scratch, Machine Learning Mastery](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/)\n",
    "\n",
    "Plummer B. et al., (May 2015), [Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models, arXiv:1505.04870 cs.CV](https://arxiv.org/abs/1505.04870)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Image_Caption_v1.0.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
