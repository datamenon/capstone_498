{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-e-Bo4GMmJn"
   },
   "source": [
    "## Directories Required\n",
    "Ensure you have the following directory structure no matter where you are hosting the data: GCP Storage, GCP Compute Engine local disk, Google Drive or your local desktop/laptop.\n",
    "\n",
    "The root folder on your local drive should be 'image_caption'. The rest follow from it.<br/>\n",
    "\n",
    "For Colab, there is one additional requirement. Ensure 'image_caption' is placed under 'Colab Notebooks' folder (which is created by Google automatically the first time you create a Colab notebook). Why? Since files within Google Colab are referenced with their full path starting all the way back to \"drive\".<br/>\n",
    "For example:<br/>\n",
    "filename = 'drive/My Drive/Colab Notebooks/Image Caption/Data/Flickr_8K/Labels/Flickr_8K.token.txt'<br/>\n",
    "filename = 'drive/My Drive/Colab Notebooks/Image Caption/Data/Flickr_30K/Labels/results.txt'<br/>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://drive.google.com/uc?id=1DrVeLA0fz0XYrPq0CZs6eAvRZbsB4GE1\" alt=\"Broken URL. Contact Anand Menon.\" width=\"60%\"/>\n",
    "</div>\n",
    "\n",
    "## Files Required\n",
    "The following files are required to run the code. All the files are shared, so you should ideally be able to copy the entire directory structure. Warning: It is over 2 GB, soon to be 10 GB.<br/>\n",
    "You will need to recreate the folder structure locally (with files and all) since the code writes files back to it.\n",
    "\n",
    "[Link to Data folder](https://drive.google.com/open?id=1bQtuMVTyaXCMGMC0XifOC2aKb1UGTSo6)\n",
    "\n",
    "### Dataset Files: Train, test, validate\n",
    "* Flickr_8K\n",
    "  * Data/Flickr_8K/Images/&lt;Images&gt;<br/>\n",
    "  * Data/Flickr_8K/Labels/&lt;Labels&gt;<br/>\n",
    "* Flickr_30K\n",
    "   * Data/Flickr_30K/Images/&lt;Images&gt;<br/>\n",
    "   * Data/Flickr_30K/Labels/&lt;Labels&gt;<br/>\n",
    "\n",
    "### Word Vectors File\n",
    "\n",
    "* Data/Vectors/&lt;Word vector file&gt;<br/>\n",
    "You only need one of these depending upon how well trained the vectors need to be for your need. The 300D is better (and much bigger) than the 100D file. Alternatively, feel free to use your own.\n",
    "    * glove.6B.100d.txt\n",
    "    * glove.6B.300d.txt\n",
    "\n",
    "### Transfer Learning Net: Features File\n",
    "In order to speed up training & testing, all features from our underlying transfer learning net are generated in-advance of the train/test code blocks. The first time you run this code, generate the features once by setting GEN_FEATURES = True above. Then set it back to False. The generated file is large, and it will take ~30 minutes to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drRBfB9mX-Ki"
   },
   "source": [
    "## Change Log\n",
    "1. 13 Apr: Switched to using GloVe encoding from a custom Embedding layer. The latter has the downside that if the number of training inputs are insufficient, the word embeddings it creates will be bad...and there is no easy way to tell if the number of training inputs are insufficient for this particular part of the net (the learning curves will give a global answer). Using GloVe word vectors eliminates this problem. In addition, it means the net is not having to learn the 'meaning' of words at the same time as it is learning to generate captions; hence less distractions for its nano-brain.\n",
    "2. 20 Apr: Dropped words that occur less frequently than 0.005% of the time in the corpus consisting of all captions. This tiny filter dramatically dropped the vocabulary from ~7500 words down to ~1100 words. Naturally, performance went up a bit with very little impact to the English of it all.\n",
    "3. 24 Apr: Switched the underlying pre-trained net over from VGG16 over to Inception v3. VGG16 is a big, plodding net and its architecture older than Inception. Inception also outperformed VGG16 in ImageNet, so overall it helped the performance a bit.\n",
    "4. 30 Apr: Cleaned up the code a bit and switched over to ResNet. The experiment here was to try and mimic human behaviour a bit more. Humans are great at fill in the blanks: The car is turning &lt;blank&gt;. So what is needed here is an identity function that helps recall prior learned patterns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49176,
     "status": "ok",
     "timestamp": 1588081865462,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "qTcM6SLmMe7Z",
    "outputId": "32eb9908-2a65-410d-9f76-34462a955449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN_FEATURES \t False\n",
      "TRAIN_MODEL \t True\n",
      "TL_MODEL \t Inceptionv3\n",
      "MAX_SAMPLES \t 75000\n",
      "DATASET \t Flickr_30K\n",
      "IN_COLAB \t False\n",
      "DIR_PREFIX \t ./Data/\n",
      "DIR_ALL_IMAGES \t ./Data/Flickr_30K/Images/\n",
      "FN_ALL_LABELS \t ./Data/Flickr_30K/Labels/Flickr_30K.captions.txt\n",
      "FN_GEN_MODEL \t ./Data/Model/model_transfer_learning_Inceptionv3.png\n",
      "FN_GEN_RESULT \t ./Data/Model/features_Inceptionv3_Flickr_30K.pkl\n",
      "FN_TRAIN_MODEL \t ./Data/Model/model_custom_Inceptionv3.png\n",
      "FN_WEIGHTS \t ./Data/Weights/weights.best_Inceptionv3_Flickr_30K.hdf5\n",
      "FN_TRAIN_REMODEL \t ./Data/Model/model_custom_from_weights_Inceptionv3.png\n",
      "FN_TEST_RESULTS \t ./Data/Model/results_Inceptionv3_Flickr_30K_15000.0\n",
      "FN_LEARN_CURVES \t ./Data/Model/results_lc_Inceptionv3_Flickr_30K_15000.0\n",
      "EMBEDDING_DIM \t 300\n",
      "FN_EMBEDDING \t ./Data/Vectors/glove.6B.300d.txt\n",
      "DEBUG \t\t False\n"
     ]
    }
   ],
   "source": [
    "#Clear all vars\n",
    "%reset -f\n",
    "\n",
    "#Init code\n",
    "def initialize():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    import sys\n",
    "\n",
    "    # Init\n",
    "    dictInit = {}\n",
    "    dictInit['GEN_FEATURES'] = False  #Set to True if features need to be generated through the underlying pre-trained net\n",
    "    dictInit['TRAIN_MODEL'] = True  #Set to True if our custom net needs to learn the dataset\n",
    "    dictInit['TL_MODEL'] = 'Inceptionv3'  #Pre-trained model used for recognizing obects within images. One of Inceptionv3 or VGG16 trained on ImageNet.\n",
    "    dictInit['MAX_SAMPLES'] = 75000  #Maximum number of samples to train & test with. Set to -1 for training on full train set.\n",
    "    dictInit['DATASET'] = 'Flickr_30K'  #One of 'Flickr_8K', 'Flickr_30K'\n",
    "\n",
    "    #Change directory access paths depending upon where you are running\n",
    "    dictInit['IN_COLAB'] = 'google.colab' in sys.modules\n",
    "    if dictInit['IN_COLAB']:\n",
    "        from google.colab import drive  #Access google drive to load data\n",
    "        drive.mount('/content/drive')\n",
    "        dictInit['DIR_PREFIX'] = 'drive/My Drive/Colab Notebooks/image_caption/Data/'\n",
    "    else:\n",
    "        dictInit['DIR_PREFIX'] = './Data/'  #Local\n",
    "\n",
    "    #All directory & file names; Train, Validate, Test etc.\n",
    "    dictInit['DIR_ALL_IMAGES'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Images/'\n",
    "    dictInit['FN_ALL_LABELS'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Labels/' + dictInit['DATASET'] + '.captions.txt'\n",
    "    dictInit['FN_GEN_MODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_transfer_learning_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_GEN_RESULT'] = dictInit['DIR_PREFIX'] + 'Model/features_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '.pkl'\n",
    "    dictInit['FN_TRAIN_MODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_custom_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_WEIGHTS'] = dictInit['DIR_PREFIX'] + 'Weights/weights.best_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '.hdf5'\n",
    "    dictInit['FN_TRAIN_REMODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_custom_from_weights_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_TEST_RESULTS'] = dictInit['DIR_PREFIX'] + 'Model/results_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '_' + str(dictInit['MAX_SAMPLES']/5)\n",
    "    dictInit['FN_LEARN_CURVES'] = dictInit['DIR_PREFIX'] + 'Model/results_lc_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '_' + str(dictInit['MAX_SAMPLES']/5)\n",
    "\n",
    "    #GloVe embeddings\n",
    "    dictInit['EMBEDDING_DIM'] = 300\n",
    "    if dictInit['EMBEDDING_DIM'] == 100:\n",
    "        dictInit['FN_EMBEDDING'] = dictInit['DIR_PREFIX'] + 'Vectors/' + 'glove.6B.100d.txt'\n",
    "    elif dictInit['EMBEDDING_DIM'] == 300:\n",
    "        dictInit['FN_EMBEDDING'] = dictInit['DIR_PREFIX'] + 'Vectors/' + 'glove.6B.300d.txt'\n",
    "    \n",
    "    #Return\n",
    "    return dictInit\n",
    "\n",
    "#Initialize key variables\n",
    "dictInit = initialize()\n",
    "for key, val in dictInit.items():\n",
    "    print(key, '\\t', val)\n",
    "if (dictInit['MAX_SAMPLES'] == -1) or (dictInit['MAX_SAMPLES'] > 100):\n",
    "    DEBUG = False\n",
    "else:\n",
    "    DEBUG = True\n",
    "print('DEBUG', '\\t\\t', DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate features via underlying pre-trained net\n",
    "if dictInit['GEN_FEATURES']:\n",
    "    from pickle import dump\n",
    "    from keras.preprocessing.image import load_img\n",
    "    from keras.preprocessing.image import img_to_array\n",
    "    from keras.models import Model\n",
    "    from keras.utils import plot_model\n",
    "    from os import listdir\n",
    "    import time\n",
    "    \n",
    "    #Config\n",
    "    if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "        from keras.applications import InceptionV3\n",
    "        from keras.applications.inception_v3 import preprocess_input\n",
    "    else:        \n",
    "        from keras.applications.vgg16 import VGG16\n",
    "        from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "    #Extract features from each photo in the directory\n",
    "    def extract_features(directory):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #Load Model\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            model = InceptionV3(include_top=False, weights='imagenet')\n",
    "        else:\n",
    "            model = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "        #Remove the top layer, retaining the features generated up-to the layer below\n",
    "        #model.layers.pop()\n",
    "        model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "        #print(model.summary())\n",
    "        plot_model(model, to_file = dictInit['FN_GEN_MODEL'], show_shapes=True)\n",
    "\n",
    "        #Extract features from each photo\n",
    "        TARGET_SIZE = (0,0)\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            TARGET_SIZE = (299, 299)\n",
    "        else:\n",
    "            TARGET_SIZE=(224, 224)\n",
    "        counter = 0\n",
    "        features = dict()\n",
    "        start = time.time()\n",
    "        current = start\n",
    "        for fn in listdir(directory):\n",
    "            #Load image\n",
    "            image = load_img(directory+'/'+fn, target_size=TARGET_SIZE, interpolation='bicubic')\n",
    "\n",
    "            #Expand dims to include batch size\n",
    "            image = img_to_array(image)\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "            #Prepare & predict\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            features[fn] = feature\n",
    "            if counter % 1000 == 0:\n",
    "                if counter == 0:\n",
    "                    print(counter, 0, fn)\n",
    "                else:\n",
    "                    print(counter, 1000.0/(time.time()-current), (time.time()-current), time.time(), fn)\n",
    "                current = time.time()\n",
    "            counter += 1\n",
    "        return features\n",
    "\n",
    "    # Extract features from all images\n",
    "    features = extract_features(dictInit['DIR_ALL_IMAGES'])\n",
    "    print('# of Extracted Features: %d' % len(features), (current-start)/60.0)\n",
    "\n",
    "    # Save to file\n",
    "    dump(features, open(dictInit['FN_GEN_RESULT'], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52242,
     "status": "ok",
     "timestamp": 1588081868543,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "SvDs-f-E7_Qx",
    "outputId": "8830b1fc-62fc-44f0-cbe3-eb0ef68b093b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 75000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load provided filename\n",
    "def read_file(filename, max_lines=-1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        if max_lines > 0:\n",
    "            doc = [next(f) for x in range(max_lines)]  #Read only the # of lines specified\n",
    "        else:\n",
    "            doc = f.read().splitlines()  #Read all lines\n",
    "        return doc\n",
    "\n",
    "#Load captions\n",
    "doc = read_file(dictInit['FN_ALL_LABELS'], dictInit['MAX_SAMPLES'])\n",
    "print(type(doc), len(doc), '\\n')\n",
    "if DEBUG:\n",
    "    print(doc[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52230,
     "status": "ok",
     "timestamp": 1588081868544,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "WUe0dQdu8qYd",
    "outputId": "d44d27c4-89b3-4c36-aeb5-6884b7974574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of samples: 15000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracts name and captions associated with each image\n",
    "def extract_captions(doc):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #If dataset Flickr_8K...\n",
    "    lines = []\n",
    "    if dictInit['DATASET'] == 'Flickr_8K':\n",
    "        #Split on tabs\n",
    "        for line in doc:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            tokens = line.split('\\t')\n",
    "            #Skip '_577c3a7d70....#1' part of name  #1001773457_577c3a7d70.jpg#1\tTwo dogs are playing with each other .\n",
    "            photo_id = tokens[0].split('_')[0]+'.'+tokens[0].split('.')[1][0:-2]\n",
    "            photo_caption = tokens[1]  #Rest of words\n",
    "            lines.append((photo_id, photo_caption))\n",
    "    elif dictInit['DATASET'] == 'Flickr_30K':\n",
    "        #Split on pipes\n",
    "        first = True  #Skip header \"image_name| comment_number| comment\"\n",
    "        for line in doc:\n",
    "            if (first) or (line.strip() == \"\"):\n",
    "                first = False\n",
    "                continue\n",
    "            tokens = line.split('|')\n",
    "            photo_id = tokens[0]\n",
    "            photo_caption = tokens[2]\n",
    "            lines.append((photo_id, photo_caption))\n",
    "\n",
    "    #Concatenate descriptions by image\n",
    "    dictCaptions = {}  #Key = Photo identifier, Value = List of captions\n",
    "    for tokens in lines:\n",
    "        photo_id = tokens[0]\n",
    "        photo_caption = tokens[1]\n",
    "        if photo_id not in dictCaptions:\n",
    "            dictCaptions[photo_id] = []\n",
    "        dictCaptions[photo_id].extend([photo_caption])\n",
    "    return dictCaptions\n",
    "\n",
    "#Extract all words and captions\n",
    "dictCaptions = extract_captions(doc)\n",
    "print('# of samples:', len(dictCaptions), '\\n')\n",
    "if DEBUG:\n",
    "    counter = 0\n",
    "    for key,val in dictCaptions.items():\n",
    "        print(key,val)\n",
    "        if counter>2:\n",
    "            break;\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53043,
     "status": "ok",
     "timestamp": 1588081869368,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "nWkwIQrS9mKR",
    "outputId": "1f4a8819-0f8f-4281-bc26-7ec9d70ffb5f"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Cleanup text\n",
    "def cleanup_text(dictCaptions):\n",
    "    \"\"\"\n",
    "    Remove punctuation & special characters, lowercase captions, remove hanging 's, remove extra spaces.\n",
    "    Returns the sum of # of words across all captions. Includes duplicate counts.\n",
    "    \"\"\"\n",
    "    #Pre-process\n",
    "    for id, captions in dictCaptions.items():\n",
    "        clean = []\n",
    "        for caption in captions:\n",
    "            desc = caption.lower().replace(\" 's\", \"s\")\n",
    "            desc = re.sub('[^A-Za-z0-9 ]+', '', desc)  #Remove punc & special chars\n",
    "            desc = desc.replace('   ', ' ').replace('  ', ' ').strip()\n",
    "            clean.append(desc)\n",
    "        dictCaptions[id] = clean\n",
    "\n",
    "#Cleanup text\n",
    "cleanup_text(dictCaptions)\n",
    "if DEBUG:\n",
    "    counter = 0\n",
    "    for key,val in dictCaptions.items():\n",
    "        print(key,val)\n",
    "        if counter>2:\n",
    "            break;\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53033,
     "status": "ok",
     "timestamp": 1588081869369,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0f9XTjDPAo8J",
    "outputId": "0283e774-2e57-4a69-90bf-694433331648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total words across all captions: 886815\n",
      "# of unique words: 14037 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Aggregate words across all captions\n",
    "def agg_words(dictCaptions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    allWordsInCaptions = []\n",
    "    for id, captions in dictCaptions.items():\n",
    "        for caption in captions:\n",
    "            allWordsInCaptions.extend(caption.split(' '))\n",
    "    return allWordsInCaptions\n",
    "\n",
    "#Count frequency of all words\n",
    "def count_word_freq(dictCaptions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Aggregate words across all captions\n",
    "    allWordsInCaptions = agg_words(dictCaptions)\n",
    "    numWords = len(allWordsInCaptions)\n",
    "    \n",
    "    #Count\n",
    "    dictWords = {}\n",
    "    for word in allWordsInCaptions:\n",
    "        if word not in dictWords:\n",
    "            dictWords[word] = 0\n",
    "        dictWords[word] += 1\n",
    "    return numWords, dictWords\n",
    "\n",
    "#Count frequency of all words\n",
    "numWords, dictWords = count_word_freq(dictCaptions)\n",
    "print('# of total words across all captions:', numWords)\n",
    "print('# of unique words:', len(dictWords), '\\n')\n",
    "if DEBUG:\n",
    "    for key in random.sample(list(dictWords.keys()), 5):\n",
    "        print(key+':', dictWords[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53328,
     "status": "ok",
     "timestamp": 1588081869678,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0p16yVjgA44w",
    "outputId": "e47bc0e4-e266-4628-fd8a-507b42e75332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 15000\n",
      "# of total words across all captions: 815742\n",
      "# of unique words: 1343 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop infrequent words from captions, reducing vocabulary size\n",
    "def drop_infrequent(dictCaptions, dictWords):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Form dict of frequent words\n",
    "    truncated = []\n",
    "    for word in dictWords.keys():\n",
    "        if dictWords[word]/numWords > 0.00005:  #0.005%\n",
    "            truncated.append(word)\n",
    "    dictWordsTrunc = dict.fromkeys(truncated , 0)\n",
    "\n",
    "    #Drop infrequent\n",
    "    for id, captions in dictCaptions.items():\n",
    "        newCaptions = []\n",
    "        for caption in captions:\n",
    "            newCaption = []\n",
    "            for word in caption.split(' '):\n",
    "                if word in dictWordsTrunc:\n",
    "                    newCaption.append(word)\n",
    "            newCaptions.append(' '.join(newCaption))\n",
    "        dictCaptions[id] = newCaptions\n",
    "\n",
    "#Drop infrequent words\n",
    "drop_infrequent(dictCaptions, dictWords)\n",
    "print('Total samples:', len(dictCaptions))\n",
    "print('# of total words across all captions:', len(agg_words(dictCaptions)))\n",
    "print('# of unique words:', len(set(agg_words(dictCaptions))), '\\n')\n",
    "if DEBUG:\n",
    "    for key in random.sample(list(dictCaptions.keys()), 5):\n",
    "        print(key, dictCaptions[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limit caption length to 95% percentile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54232,
     "status": "ok",
     "timestamp": 1588081870593,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "M3_M8jPqCRWX",
    "outputId": "80b91ba9-9850-479e-e53d-c2045bb846dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 : 10800 3000 1200\n",
      "15000 : 10800 3000 1200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Create train, dev & test datasets\n",
    "def split_dataset(dictCaptions):\n",
    "    #Split\n",
    "    X = list(dictCaptions.keys())\n",
    "    y = list(dictCaptions.values())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)  #Train+Dev vs. Test\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.10, random_state=42)  #Train vs. Dev\n",
    "    return X_train, X_dev, X_test, y_train, y_dev, y_test\n",
    "\n",
    "#Split\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = split_dataset(dictCaptions)\n",
    "print(len(X_train)+len(X_test)+len(X_dev), ':', len(X_train), len(X_test), len(X_dev))\n",
    "print(len(y_train)+len(y_test)+len(y_dev), ':', len(y_train), len(y_test), len(y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1370773415.jpg\n",
      "135992929.jpg\n",
      "2408570692.jpg\n",
      "2102422015.jpg\n",
      "2979914158.jpg\n",
      "2298661279.jpg\n",
      "2467856402.jpg\n",
      "199809190.jpg\n",
      "3327563443.jpg\n",
      "2831394884.jpg\n"
     ]
    }
   ],
   "source": [
    "for val in X_train[0:10]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence length: 73\n",
      "Vocabulary size: 1345 \n",
      "\n",
      "53999 \t 6000 \t 15000\n",
      "3 \t 3 \t 3\n",
      "(640981, 2048) \t (640981, 73) \t (640981, 1345)\n",
      "(71521, 2048) \t (71521, 73) \t (71521, 1345)\n",
      "(178236, 2048) \t (178236, 73) \t (178236, 1345)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Flatten sets and add startseq endseq bookends\n",
    "def flatten_add_bookends(X, y):\n",
    "    \"\"\"\n",
    "        X is list of photo ID's.\n",
    "        y is a list of captions (usually 5) for each photo.\n",
    "    \"\"\"\n",
    "    Xy = []\n",
    "    idx = 0\n",
    "    for captions in y:\n",
    "        Xy.extend([[X[idx], 'startseq ' + cap + ' endseq'] for cap in captions])\n",
    "        idx += 1\n",
    "    return Xy\n",
    "\n",
    "#Create inputs images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_seq_len, Xy, features, vocab_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Foreach sample...\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for key, value in Xy:\n",
    "        #Encode: text to integers\n",
    "        seq = tokenizer.texts_to_sequences([value])[0]\n",
    "        \n",
    "        #Split sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            #Split\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            \n",
    "            #Pad input sequences\n",
    "            in_seq = pad_sequences([in_seq], maxlen = max_seq_len)[0]\n",
    "            \n",
    "            #Encode output sequence: \n",
    "            out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n",
    "            \n",
    "            #Store\n",
    "            X1.append(features[key][0])\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return (array(X1), array(X2), array(y))\n",
    "\n",
    "#Convert the dataset into the right format for our custom net + LSTM\n",
    "def process_dataset(X_train, X_dev, X_test, y_train, y_dev, y_test):\n",
    "    #Flatten sets ++ startseq endseq bookends\n",
    "    Xy_train = flatten_add_bookends(X_train, y_train)\n",
    "    Xy_dev = flatten_add_bookends(X_dev, y_dev)\n",
    "    Xy_test = flatten_add_bookends(X_test, y_test)\n",
    "    \n",
    "    #Compute max length of tokens in train set\n",
    "    max_seq_len = max([len(token.split(' ')) for val in Xy_train for token in val])\n",
    "    \n",
    "    #Create tokenizer and fit on train set\n",
    "    only_captions = [val[1] for val in Xy_train]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(only_captions)\n",
    "    \n",
    "    #Load transfer learning features from pickle file\n",
    "    features_all = load(open(dictInit['FN_GEN_RESULT'], 'rb'))\n",
    "    features_train = {k: features_all[k] for k in X_train}\n",
    "    features_dev = {k: features_all[k] for k in X_dev}\n",
    "    features_test = {k: features_all[k] for k in X_test}\n",
    "    \n",
    "    #Create training sequences\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    Xy_train_seq = create_sequences(tokenizer, max_seq_len, Xy_train, features_train, vocab_size)  #X1train, X2train, ytrain\n",
    "    \n",
    "    #Create dev sequences\n",
    "    Xy_dev_seq = create_sequences(tokenizer, max_seq_len, Xy_dev, features_dev, vocab_size)  #X1dev, X2dev, ydev\n",
    "    \n",
    "    #Create test sequences\n",
    "    Xy_test_seq = create_sequences(tokenizer, max_seq_len, Xy_test, features_test, vocab_size)  #X1test, X2test, ytest\n",
    "    \n",
    "    #Add features into Xy_train, Xy_dev & Xy_test for use in testing later (only need test but doing the rest for debugging consistency)\n",
    "    Xy_train = [(features_train[Xy_train[idx][0]], Xy_train[idx][0], Xy_train[idx][1]) for idx,val in enumerate(Xy_train)]\n",
    "    Xy_dev = [(features_dev[Xy_dev[idx][0]], Xy_dev[idx][0], Xy_dev[idx][1]) for idx,val in enumerate(Xy_dev)]\n",
    "    Xy_test = [(features_test[Xy_test[idx][0]], Xy_test[idx][0], Xy_test[idx][1]) for idx,val in enumerate(Xy_test)]\n",
    "    \n",
    "    #Return\n",
    "    return tokenizer, max_seq_len, vocab_size, \\\n",
    "            Xy_train, Xy_dev, Xy_test, \\\n",
    "            Xy_train_seq, Xy_dev_seq, Xy_test_seq\n",
    "\n",
    "#Process dataset\n",
    "tokenizer, max_seq_len, vocab_size, Xy_train, Xy_dev, Xy_test, Xy_train_seq, Xy_dev_seq, Xy_test_seq = \\\n",
    "                                                    process_dataset(X_train, X_dev, X_test, y_train, y_dev, y_test)\n",
    "print('Longest sequence length:', max_seq_len)\n",
    "print('Vocabulary size:', vocab_size, '\\n')\n",
    "print(len(Xy_train), '\\t', len(Xy_dev), '\\t', len(Xy_test))\n",
    "print(len(Xy_train_seq), '\\t', len(Xy_dev_seq), '\\t', len(Xy_test_seq))\n",
    "print(Xy_train_seq[0].shape, '\\t', Xy_train_seq[1].shape, '\\t', Xy_train_seq[2].shape)\n",
    "print(Xy_dev_seq[0].shape, '\\t', Xy_dev_seq[1].shape, '\\t', Xy_dev_seq[2].shape)\n",
    "print(Xy_test_seq[0].shape, '\\t', Xy_test_seq[1].shape, '\\t', Xy_test_seq[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ug3D8Im8HQqZ"
   },
   "source": [
    "## Create & Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30820,
     "status": "ok",
     "timestamp": 1588082067473,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hRp9_ly-8urm",
    "outputId": "7ee7d535-d2f6-46cf-d321-022145a64fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors...\n",
      "# of vectors loaded: 400000\n",
      "Time taken: 35.34368371963501\n",
      "(1345, 300)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#Create a matrix of GloVe word vectors of all words in our vocabulary\n",
    "def createEmbeddingMatrix(dict_word_index, vocab_size):\n",
    "    #Load GloVe word vectors\n",
    "    start = time.time()\n",
    "    dict_glove = {}\n",
    "    lines = read_file(dictInit['FN_EMBEDDING'])\n",
    "    with open(dictInit['FN_EMBEDDING']) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            dict_glove[word] = coefs\n",
    "    print('# of vectors loaded:', len(dict_glove))\n",
    "    print('Time taken:', time.time()-start)\n",
    "\n",
    "    #Create embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, dictInit['EMBEDDING_DIM']))\n",
    "    for word, i in dict_word_index.items():\n",
    "        embedding_vector = dict_glove.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  #Words not found in GloVe will be all-zeros\n",
    "    return embedding_matrix\n",
    "\n",
    "#Create a matrix of GloVe word vectors of all words in our vocabulary\n",
    "print('Loading vectors...')\n",
    "if dictInit['TRAIN_MODEL']:\n",
    "    embedding_matrix = createEmbeddingMatrix(tokenizer.word_index, vocab_size)\n",
    "    print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63520,
     "status": "ok",
     "timestamp": 1588082100188,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "T_EtOFVe2egn",
    "outputId": "296df2c5-928d-48b3-f17b-681bf377556a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 1588630026.2279468\n",
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 73)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 73, 300)      403500      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 400)          819600      input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 400)          1121600     embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 400)          0           dense_17[0][0]                   \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 500)          200500      add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1345)         673845      dense_18[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,219,045\n",
      "Trainable params: 2,815,545\n",
      "Non-trainable params: 403,500\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 640981 samples, validate on 71521 samples\n",
      "Epoch 1/3\n",
      " - 1016s - loss: 3.2857 - accuracy: 0.3456 - val_loss: 3.0940 - val_accuracy: 0.3612\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.09403, saving model to ./Data/Weights/weights.best_Inceptionv3_Flickr_30K.hdf5\n",
      "Epoch 2/3\n",
      " - 1006s - loss: 2.9726 - accuracy: 0.3748 - val_loss: 3.0549 - val_accuracy: 0.3682\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.09403 to 3.05494, saving model to ./Data/Weights/weights.best_Inceptionv3_Flickr_30K.hdf5\n",
      "Epoch 3/3\n"
     ]
    }
   ],
   "source": [
    "if dictInit['TRAIN_MODEL']:\n",
    "    from keras.utils import plot_model\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Embedding\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers.merge import add\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    \n",
    "    #Define the captioning model\n",
    "    def define_model(vocab_size, max_seq_len):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # feature extractor model\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            inputs1 = Input(shape=(2048,))\n",
    "        else:\n",
    "            inputs1 = Input(shape=(4096,))\n",
    "        fe1 = inputs1  #Dropout(0.2)(inputs1)\n",
    "        fe2 = Dense(400, activation='relu')(fe1)\n",
    "        \n",
    "        # sequence model\n",
    "        inputs2 = Input(shape=(max_seq_len,))\n",
    "        # se1 = Embedding(vocab_size, 300, mask_zero=True)(inputs2)\n",
    "        se1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_len, trainable=False)(inputs2)\n",
    "        se2 = se1  #Dropout(0.2)(se1)\n",
    "        se3 = LSTM(400)(se2)\n",
    "        \n",
    "        # decoder model\n",
    "        decoder1 = add([fe2, se3])\n",
    "        decoder2 = Dense(500, activation='relu')(decoder1)\n",
    "        outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "        \n",
    "        # tie it together [image, seq] [word]\n",
    "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "        \n",
    "        # summarize model\n",
    "        print(model.summary())\n",
    "        plot_model(model, to_file=dictInit['FN_TRAIN_MODEL'], show_shapes=True)\n",
    "        return model\n",
    "\n",
    "    # fit model\n",
    "\n",
    "    # define the model & checkpoint callback\n",
    "    start = time.time()\n",
    "    print('Start time:', start)\n",
    "    model = define_model(vocab_size, max_seq_len)\n",
    "    checkpoint = ModelCheckpoint(dictInit['FN_WEIGHTS'], monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit([Xy_train_seq[0], Xy_train_seq[1]], Xy_train_seq[2], epochs=3, \n",
    "              verbose=2, callbacks=[checkpoint], validation_data=([Xy_dev_seq[0], Xy_dev_seq[1]], Xy_dev_seq[2]))\n",
    "    print('Total time taken:', time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KJvdPpDD19E"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "#Init plot\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "fig=plt.figure(figsize=(22, 24))\n",
    "fig.patch.set_facecolor('black')\n",
    "\n",
    "#Plot learning curves\n",
    "def plotCurve(ax, metric1, metric2, title, title_x, title_y):\n",
    "    ax.plot(history.history[metric1], '-*', label=metric1)\n",
    "    ax.plot(history.history[metric2], '--*', label=metric2)\n",
    "    ax.set_title('Training & Validation Loss', color='black')\n",
    "    ax.legend(loc='best', shadow=True)\n",
    "    ax.set_xlabel(title_x, color='black')\n",
    "    ax.set_ylabel(title_y, color='black')\n",
    "    ax.spines['bottom'].set_color('lightgrey')\n",
    "    ax.spines['left'].set_color('lightgrey')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', colors='black')\n",
    "    ax.tick_params(axis='y', colors='black')\n",
    "\n",
    "#Plot learning curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "plotCurve(ax1, 'loss', 'val_loss', 'Training & Valiation Loss', '# of Epochs', 'Loss')\n",
    "plotCurve(ax2, 'accuracy', 'val_accuracy', 'Training & Valiation Accuracy', '# of Epochs', 'Accuracy')\n",
    "plt.savefig(dictInit['FN_LEARN_CURVES'] + '_' + datetime.today().strftime('%Y-%m-%d-%H:%M:%S') + '.png',\n",
    "                                                                        bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67278,
     "status": "ok",
     "timestamp": 1588082103956,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hP4meSU1vNQc",
    "outputId": "a0950deb-daaa-4187-c52c-d5354af6bcba"
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo_features, max_seq_len):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_seq_len):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen = max_seq_len)\n",
    "        \n",
    "        # predict next word\n",
    "        yhat = model.predict([photo_features,sequence], verbose = 0)\n",
    "        \n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        \n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        \n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        \n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "#Stores an instance of a test case\n",
    "class XyTest:\n",
    "    def __init__(self, photo_id, features):\n",
    "        self.photo_id = photo_id\n",
    "        self.features = features\n",
    "        self.descriptions = list()\n",
    "        self.prediction = []\n",
    "    \n",
    "    def addPrediction(self, prediction):\n",
    "        self.prediction = prediction.replace('startseq ','').replace(' endseq','').split(' ')\n",
    "    \n",
    "    def addDescription(self, description):\n",
    "        self.descriptions.append(description.replace('startseq ','').replace(' endseq','').split(' '))\n",
    "        \n",
    "    def toString(self):\n",
    "        res = self.photo_id + '\\n'\n",
    "        for desc in self.descriptions:\n",
    "            res += '\\t' + desc + '\\n'\n",
    "        if len(self.prediction):\n",
    "            res += '\\t' + self.prediction + '\\n'\n",
    "        return res\n",
    "\n",
    "#Evaluate model\n",
    "def evaluate_model(model, Xy_test, tokenizer, max_seq_len):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Convert Xy_test to dict: Key = photo_id, Value = List of descriptions\n",
    "    dictXy_test = {}\n",
    "    for photo_features, photo_id, photo_description in Xy_test:\n",
    "        if photo_id not in dictXy_test:\n",
    "            dictXy_test[photo_id] = XyTest(photo_id, photo_features)\n",
    "        dictXy_test[photo_id].addDescription(photo_description)\n",
    "    \n",
    "    #Predict caption foreach test case...\n",
    "    for photo_id, xyTest in dictXy_test.items():\n",
    "        photo_features = xyTest.features\n",
    "        photo_descriptions = xyTest.descriptions  #Actual\n",
    "        yhat = generate_desc(model, tokenizer, photo_features, max_seq_len)  #Predicted description\n",
    "        xyTest.addPrediction(yhat)\n",
    "    \n",
    "    #Format input for BLEU\n",
    "    candidate, references = list(), list()\n",
    "    for photo_id, xyTest in dictXy_test.items():\n",
    "        references.append(xyTest.descriptions)\n",
    "        candidate.append(xyTest.prediction)\n",
    "    \n",
    "    #Compute BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(references, candidate, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(references, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(references, candidate, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "    #Return\n",
    "    return dictXy_test\n",
    "\n",
    "# load the model\n",
    "model_custom_from_weights = load_model(dictInit['FN_WEIGHTS'])\n",
    "# print(model_custom_from_weights.summary())\n",
    "plot_model(model_custom_from_weights, to_file = dictInit['FN_TRAIN_REMODEL'], show_shapes=True)\n",
    "\n",
    "# evaluate model\n",
    "start = time.time()\n",
    "dictXy_test = evaluate_model(model_custom_from_weights, Xy_test, tokenizer, max_seq_len)\n",
    "print('Time taken:', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 105216,
     "status": "ok",
     "timestamp": 1588082141905,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "xW19WkiyvOOs",
    "outputId": "b30bd86f-5050-4005-aba4-2a8b2abb2406"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "\n",
    "#Config model imports\n",
    "if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "    from keras.applications.inception_v3 import InceptionV3\n",
    "    from keras.applications.inception_v3 import preprocess_input\n",
    "else:\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "#Load Model\n",
    "if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "    model_TL = InceptionV3(include_top=False, weights='imagenet')\n",
    "else:\n",
    "    model_TL = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "#Restructure the model\n",
    "# model_TL.layers.pop()\n",
    "model_TL = Model(inputs = model_TL.inputs, outputs = model_TL.layers[-1].output)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1LmTvQ8D4ONvEnj3npjPvpRvgQTOLdUGi"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21650,
     "status": "ok",
     "timestamp": 1588084779903,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "DJprBlgr--wM",
    "outputId": "e6f11dd4-9cd6-4a34-9dae-b78e47b51de3"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from textwrap import wrap\n",
    "\n",
    "#Plot sample test images\n",
    "def plotSamples(dictXy_test):\n",
    "    #Get random images\n",
    "    testIDs = random.sample(dictXy_test.keys(), 16)\n",
    "    print(testIDs)\n",
    "    \n",
    "    #Init plot\n",
    "    plt.rcParams['figure.facecolor'] = 'black'\n",
    "    fig=plt.figure(figsize=(22, 24))\n",
    "    fig.patch.set_facecolor('black')\n",
    "    columns = 4\n",
    "    rows = 4\n",
    "    \n",
    "    #Plot\n",
    "    for i in range(1, columns*rows+1):\n",
    "        #Prediction\n",
    "        yhat = ' '.join(dictXy_test[testIDs[i-1]].prediction)\n",
    "        \n",
    "        #Plot image and description together\n",
    "        img = mpimg.imread(dictInit['DIR_ALL_IMAGES']+testIDs[i-1])\n",
    "        ax = fig.add_subplot(rows, columns, i)\n",
    "        ax.set_title(\"\\n\".join(wrap(yhat, 50)), color='white')\n",
    "        ax.set_facecolor('xkcd:black')\n",
    "        plt.imshow(img)\n",
    "    plt.savefig(dictInit['FN_TEST_RESULTS']+'_'+datetime.today().strftime('%Y-%m-%d-%H:%M:%S')+'.png',\n",
    "                bbox_inches='tight', facecolor='black', dpi = 50)\n",
    "    plt.show()\n",
    "    \n",
    "#Plot sample test images\n",
    "plotSamples(dictXy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Papineni K, Roukos S, Ward T, Zhu W, (Jul 2002), [BLEU: a method for automatic evaluation of machine translation, Proceedings of the 40th Annual Meeting on Association for Computational Linguistics](https://dl.acm.org/doi/10.3115/1073083.1073135)\n",
    "\n",
    "Szegedy C et al., (Sep 2014), [Going Deeper with Convolutions,  arXiv:1409.4842 cs.CV](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "Simonyan K, Zisserman A, (Sep 2014), [Very Deep Convolutional Networks for Large-Scale Image Recognition, arXiv:1409.1556 cs.CV](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Tanti M, Gatt A, Camilleri K.P., (Aug 2017), [What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?, arXiv:1708.02043 cs.CL](https://arxiv.org/abs/1708.02043)\n",
    "\n",
    "Brownlee J., (Jun 2019), [How to Develop a Deep Learning Photo Caption Generator from Scratch, Machine Learning Mastery](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/)\n",
    "\n",
    "Plummer B. et al., (May 2015), [Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models, arXiv:1505.04870 cs.CV](https://arxiv.org/abs/1505.04870)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Image_Caption_v1.0.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
