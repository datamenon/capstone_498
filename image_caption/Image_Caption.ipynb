{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-e-Bo4GMmJn"
   },
   "source": [
    "## Directories Required\n",
    "Ensure you have the following directory structure no matter where you are hosting the data: GCP Storage, GCP Compute Engine local disk, Google Drive or your local desktop/laptop.\n",
    "\n",
    "The root folder on your local drive should be 'image_caption'. The rest follow from it.<br/>\n",
    "\n",
    "For Colab, there is one additional requirement. Ensure 'image_caption' is placed under 'Colab Notebooks' folder (which is created by Google automatically the first time you create a Colab notebook). Why? Since files within Google Colab are referenced with their full path starting all the way back to \"drive\".<br/>\n",
    "For example:<br/>\n",
    "filename = 'drive/My Drive/Colab Notebooks/Image Caption/Data/Flickr_8K/Labels/Flickr_8K.token.txt'<br/>\n",
    "filename = 'drive/My Drive/Colab Notebooks/Image Caption/Data/Flickr_30K/Labels/results.txt'<br/>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://drive.google.com/uc?id=1DrVeLA0fz0XYrPq0CZs6eAvRZbsB4GE1\" alt=\"Broken URL. Contact Anand Menon.\" width=\"60%\"/>\n",
    "</div>\n",
    "\n",
    "## Files Required\n",
    "The following files are required to run the code. All the files are shared, so you should ideally be able to copy the entire directory structure. Warning: It is over 2 GB, soon to be 10 GB.<br/>\n",
    "You will need to recreate the folder structure locally (with files and all) since the code writes files back to it.\n",
    "\n",
    "[Link to Data folder](https://drive.google.com/open?id=1bQtuMVTyaXCMGMC0XifOC2aKb1UGTSo6)\n",
    "\n",
    "### Dataset Files: Train, test, validate\n",
    "* Flickr_8K\n",
    "  * Data/Flickr_8K/Images/&lt;Images&gt;<br/>\n",
    "  * Data/Flickr_8K/Labels/&lt;Labels&gt;<br/>\n",
    "* Flickr_30K\n",
    "   * Data/Flickr_30K/Images/&lt;Images&gt;<br/>\n",
    "   * Data/Flickr_30K/Labels/&lt;Labels&gt;<br/>\n",
    "\n",
    "### Word Vectors File\n",
    "\n",
    "* Data/Vectors/&lt;Word vector file&gt;<br/>\n",
    "You only need one of these depending upon how well trained the vectors need to be for your need. The 300D is better (and much bigger) than the 100D file. Alternatively, feel free to use your own.\n",
    "    * glove.6B.100d.txt\n",
    "    * glove.6B.300d.txt\n",
    "\n",
    "### Transfer Learning Net: Features File\n",
    "In order to speed up training & testing, all features from our underlying transfer learning net are generated in-advance of the train/test code blocks. The first time you run this code, generate the features once by setting GEN_FEATURES = True above. Then set it back to False. The generated file is large, and it will take ~30 minutes to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drRBfB9mX-Ki"
   },
   "source": [
    "## Change Log\n",
    "1. 13 Apr: Switched to using GloVe encoding from a custom Embedding layer. The latter has the downside that if the number of training inputs are insufficient, the word embeddings it creates will be bad...and there is no easy way to tell if the number of training inputs are insufficient for this particular part of the net (the learning curves will give a global answer). Using GloVe word vectors eliminates this problem. In addition, it means the net is not having to learn the 'meaning' of words at the same time as it is learning to generate captions; hence less distractions for its nano-brain.\n",
    "2. 20 Apr: Dropped words that occur less frequently than 0.005% of the time in the corpus consisting of all captions. This tiny filter dramatically dropped the vocabulary from ~7500 words down to ~1100 words. Naturally, performance went up a bit with very little impact to the English of it all.\n",
    "3. 24 Apr: Switched the underlying pre-trained net over from VGG16 over to Inception v3. VGG16 is a big, plodding net and its architecture older than Inception. Inception also outperformed VGG16 in ImageNet, so overall it helped the performance a bit.\n",
    "4. 30 Apr: Cleaned up the code a bit and switched over to ResNet. The experiment here was to try and mimic human behaviour a bit more. Humans are great at fill in the blanks: The car is turning &lt;blank&gt;. So what is needed here is an identity function that helps recall prior learned patterns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49176,
     "status": "ok",
     "timestamp": 1588081865462,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "qTcM6SLmMe7Z",
    "outputId": "32eb9908-2a65-410d-9f76-34462a955449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN_FEATURES \t False\n",
      "TRAIN_MODEL \t True\n",
      "TL_MODEL \t Inceptionv3\n",
      "MAX_SAMPLES \t 3000\n",
      "DATASET \t Flickr_8K\n",
      "IN_COLAB \t False\n",
      "DIR_PREFIX \t ./Data/\n",
      "DIR_ALL_IMAGES \t ./Data/Flickr_8K/Images/\n",
      "FN_ALL_LABELS \t ./Data/Flickr_8K/Labels/Flickr_8K.captions.txt\n",
      "FN_GEN_MODEL \t ./Data/Model/model_transfer_learning_Inceptionv3.png\n",
      "FN_GEN_RESULT \t ./Data/Model/features_Inceptionv3_Flickr_8K.pkl\n",
      "FN_TRAIN_MODEL \t ./Data/Model/model_custom_Inceptionv3.png\n",
      "FN_WEIGHTS \t ./Data/Weights/weights.best_Inceptionv3_Flickr_8K.hdf5\n",
      "FN_TRAIN_REMODEL \t ./Data/Model/model_custom_from_weights_Inceptionv3.png\n",
      "FN_TEST_RESULTS \t ./Data/Model/results_Inceptionv3_Flickr_8K\n",
      "EMBEDDING_DIM \t 300\n",
      "FN_EMBEDDING \t ./Data/Vectors/glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "#Clear all vars\n",
    "%reset -f\n",
    "\n",
    "#Init code\n",
    "def initialize():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    import sys\n",
    "\n",
    "    # Init\n",
    "    dictInit = {}\n",
    "    dictInit['GEN_FEATURES'] = False  #Set to True if features need to be generated through the underlying pre-trained net\n",
    "    dictInit['TRAIN_MODEL'] = True  #Set to True if our custom net needs to learn the dataset\n",
    "    dictInit['TL_MODEL'] = 'Inceptionv3'  #Pre-trained model used for recognizing obects within images. One of Inceptionv3 or VGG16 trained on ImageNet.\n",
    "    dictInit['MAX_SAMPLES'] = 3000  #Maximum number of samples to train & test with. Set to -1 for training on full train set.\n",
    "    dictInit['DATASET'] = 'Flickr_8K'  #One of 'Flickr_8K', 'Flickr_30K'\n",
    "\n",
    "    #Change directory access paths depending upon where you are running\n",
    "    dictInit['IN_COLAB'] = 'google.colab' in sys.modules\n",
    "    if dictInit['IN_COLAB']:\n",
    "        from google.colab import drive  #Access google drive to load data\n",
    "        drive.mount('/content/drive')\n",
    "        dictInit['DIR_PREFIX'] = 'drive/My Drive/Colab Notebooks/image_caption/Data/'\n",
    "    else:\n",
    "        dictInit['DIR_PREFIX'] = './Data/'  #Local\n",
    "\n",
    "    #All directory & file names; Train, Validate, Test etc.\n",
    "    dictInit['DIR_ALL_IMAGES'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Images/'\n",
    "    dictInit['FN_ALL_LABELS'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Labels/' + dictInit['DATASET'] + '.captions.txt'\n",
    "    dictInit['FN_GEN_MODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_transfer_learning_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_GEN_RESULT'] = dictInit['DIR_PREFIX'] + 'Model/features_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '.pkl'\n",
    "#     dictInit['FN_TRAIN'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Labels/' + dictInit['DATASET'] + '.trainImages.txt'\n",
    "#     dictInit['FN_VALIDATE'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Labels/' + dictInit['DATASET'] + '.devImages.txt'\n",
    "#     dictInit['FN_TEST'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Labels/' + dictInit['DATASET'] + '.testImages.txt'\n",
    "    dictInit['FN_TRAIN_MODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_custom_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_WEIGHTS'] = dictInit['DIR_PREFIX'] + 'Weights/weights.best_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '.hdf5'\n",
    "    dictInit['FN_TRAIN_REMODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_custom_from_weights_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_TEST_RESULTS'] = dictInit['DIR_PREFIX'] + 'Model/results_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET']\n",
    "\n",
    "    #GloVe embeddings\n",
    "    dictInit['EMBEDDING_DIM'] = 300\n",
    "    if dictInit['EMBEDDING_DIM'] == 100:\n",
    "        dictInit['FN_EMBEDDING'] = dictInit['DIR_PREFIX'] + 'Vectors/' + 'glove.6B.100d.txt'\n",
    "    elif dictInit['EMBEDDING_DIM'] == 300:\n",
    "        dictInit['FN_EMBEDDING'] = dictInit['DIR_PREFIX'] + 'Vectors/' + 'glove.6B.300d.txt'\n",
    "    \n",
    "    #Return\n",
    "    return dictInit\n",
    "\n",
    "#Initialize key variables\n",
    "DEBUG = True\n",
    "dictInit = initialize()\n",
    "for key, val in dictInit.items():\n",
    "    print(key, '\\t', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate features via underlying pre-trained net\n",
    "if dictInit['GEN_FEATURES']:\n",
    "    from pickle import dump\n",
    "    from keras.preprocessing.image import load_img\n",
    "    from keras.preprocessing.image import img_to_array\n",
    "    from keras.models import Model\n",
    "    from keras.utils import plot_model\n",
    "    from os import listdir\n",
    "    import time\n",
    "    \n",
    "    #Config\n",
    "    if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "        from keras.applications import InceptionV3\n",
    "        from keras.applications.inception_v3 import preprocess_input\n",
    "    else:        \n",
    "        from keras.applications.vgg16 import VGG16\n",
    "        from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "    #Extract features from each photo in the directory\n",
    "    def extract_features(directory):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #Load Model\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            model = InceptionV3(include_top=False, weights='imagenet')\n",
    "        else:\n",
    "            model = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "        #Remove the top layer, retaining the features generated up-to the layer below\n",
    "        #model.layers.pop()\n",
    "        model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "        #print(model.summary())\n",
    "        plot_model(model, to_file = dictInit['FN_GEN_MODEL'], show_shapes=True)\n",
    "\n",
    "        #Extract features from each photo\n",
    "        TARGET_SIZE = (0,0)\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            TARGET_SIZE = (299, 299)\n",
    "        else:\n",
    "            TARGET_SIZE=(224, 224)\n",
    "        counter = 0\n",
    "        features = dict()\n",
    "        start = time.time()\n",
    "        current = start\n",
    "        for fn in listdir(directory):\n",
    "            #Load image\n",
    "            image = load_img(directory+'/'+fn, target_size=TARGET_SIZE, interpolation='bicubic')\n",
    "\n",
    "            #Expand dims to include batch size\n",
    "            image = img_to_array(image)\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "            #Prepare & predict\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            features[fn] = feature\n",
    "            if counter % 1000 == 0:\n",
    "                if counter == 0:\n",
    "                    print(counter, 0, fn)\n",
    "                else:\n",
    "                    print(counter, 1000.0/(time.time()-current), (time.time()-current), time.time(), fn)\n",
    "                current = time.time()\n",
    "            counter += 1\n",
    "        return features\n",
    "\n",
    "    # Extract features from all images\n",
    "    features = extract_features(dictInit['DIR_ALL_IMAGES'])\n",
    "    print('# of Extracted Features: %d' % len(features), (current-start)/60.0)\n",
    "\n",
    "    # Save to file\n",
    "    dump(features, open(dictInit['FN_GEN_RESULT'], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52242,
     "status": "ok",
     "timestamp": 1588081868543,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "SvDs-f-E7_Qx",
    "outputId": "8830b1fc-62fc-44f0-cbe3-eb0ef68b093b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3000 \n",
      "\n",
      "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .\\n', '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .\\n', '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .\\n', '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her playhouse .\\n', '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a wooden cabin .\\n', '1001773457_577c3a7d70.jpg#0\\tA black dog and a spotted dog are fighting\\n', '1001773457_577c3a7d70.jpg#1\\tA black dog and a tri-colored dog playing with each other on the road .\\n', '1001773457_577c3a7d70.jpg#2\\tA black dog and a white dog with brown spots are staring at each other in the street .\\n', '1001773457_577c3a7d70.jpg#3\\tTwo dogs of different breeds looking at each other on the road .\\n', '1001773457_577c3a7d70.jpg#4\\tTwo dogs on pavement moving toward each other .\\n']\n"
     ]
    }
   ],
   "source": [
    "#Load provided filename\n",
    "def read_file(filename, max_lines=-1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        if max_lines > 0:\n",
    "            doc = [next(f) for x in range(max_lines)]  #Read only the # of lines specified\n",
    "        else:\n",
    "            doc = f.read().splitlines()  #Read all lines\n",
    "        return doc\n",
    "\n",
    "#Load captions\n",
    "doc = read_file(dictInit['FN_ALL_LABELS'], dictInit['MAX_SAMPLES'])\n",
    "print(type(doc), len(doc), '\\n')\n",
    "if DEBUG:\n",
    "    print(doc[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52230,
     "status": "ok",
     "timestamp": 1588081868544,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "WUe0dQdu8qYd",
    "outputId": "d44d27c4-89b3-4c36-aeb5-6884b7974574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of samples: 600 \n",
      "\n",
      "1000268201.jpg ['A child in a pink dress is climbing up a set of stairs in an entry way .\\n', 'A girl going into a wooden building .\\n', 'A little girl climbing into a wooden playhouse .\\n', 'A little girl climbing the stairs to her playhouse .\\n', 'A little girl in a pink dress going into a wooden cabin .\\n']\n",
      "1001773457.jpg ['A black dog and a spotted dog are fighting\\n', 'A black dog and a tri-colored dog playing with each other on the road .\\n', 'A black dog and a white dog with brown spots are staring at each other in the street .\\n', 'Two dogs of different breeds looking at each other on the road .\\n', 'Two dogs on pavement moving toward each other .\\n']\n",
      "1002674143.jpg ['A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\\n', 'A little girl is sitting in front of a large painted rainbow .\\n', 'A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .\\n', 'There is a girl with pigtails sitting in front of a rainbow painting .\\n', 'Young girl with pigtails painting outside in the grass .\\n']\n",
      "1003163366.jpg ['A man lays on a bench while his dog sits by him .\\n', 'A man lays on the bench to which a white dog is also tied .\\n', 'a man sleeping on a bench outside with a white and black dog sitting next to him .\\n', 'A shirtless man lies on a park bench with his dog .\\n', 'man laying on bench holding leash of dog sitting on ground\\n']\n"
     ]
    }
   ],
   "source": [
    "#Extracts name and captions associated with each image\n",
    "def extract_captions(doc):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #If dataset Flickr_8K...\n",
    "    lines = []\n",
    "    if dictInit['DATASET'] == 'Flickr_8K':\n",
    "        #Split on tabs\n",
    "        for line in doc:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            tokens = line.split('\\t')\n",
    "            #Skip '_577c3a7d70....#1' part of name  #1001773457_577c3a7d70.jpg#1\tTwo dogs are playing with each other .\n",
    "            photo_id = tokens[0].split('_')[0]+'.'+tokens[0].split('.')[1][0:-2]\n",
    "            photo_caption = tokens[1]  #Rest of words\n",
    "            lines.append((photo_id, photo_caption))\n",
    "    elif dictInit['DATASET'] == 'Flickr_30K':\n",
    "        #Split on pipes\n",
    "        first = True  #Skip header \"image_name| comment_number| comment\"\n",
    "        for line in doc:\n",
    "            if (first) or (line.strip() == \"\"):\n",
    "                first = False\n",
    "                continue\n",
    "            tokens = line.split('|')\n",
    "            photo_id = tokens[0]\n",
    "            photo_caption = tokens[2]\n",
    "            lines.append((photo_id, photo_caption))\n",
    "\n",
    "    #Concatenate descriptions by image\n",
    "    dictCaptions = {}  #Key = Photo identifier, Value = List of captions\n",
    "    for tokens in lines:\n",
    "        photo_id = tokens[0]\n",
    "        photo_caption = tokens[1]\n",
    "        if photo_id not in dictCaptions:\n",
    "            dictCaptions[photo_id] = []\n",
    "        dictCaptions[photo_id].extend([photo_caption])\n",
    "    return dictCaptions\n",
    "\n",
    "#Extract all words and captions\n",
    "dictCaptions = extract_captions(doc)\n",
    "print('# of samples:', len(dictCaptions), '\\n')\n",
    "if DEBUG:\n",
    "    counter = 0\n",
    "    for key,val in dictCaptions.items():\n",
    "        print(key,val)\n",
    "        if counter>2:\n",
    "            break;\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53043,
     "status": "ok",
     "timestamp": 1588081869368,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "nWkwIQrS9mKR",
    "outputId": "1f4a8819-0f8f-4281-bc26-7ec9d70ffb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201.jpg ['a child in a pink dress is climbing up a set of stairs in an entry way', 'a girl going into a wooden building', 'a little girl climbing into a wooden playhouse', 'a little girl climbing the stairs to her playhouse', 'a little girl in a pink dress going into a wooden cabin']\n",
      "1001773457.jpg ['a black dog and a spotted dog are fighting', 'a black dog and a tricolored dog playing with each other on the road', 'a black dog and a white dog with brown spots are staring at each other in the street', 'two dogs of different breeds looking at each other on the road', 'two dogs on pavement moving toward each other']\n",
      "1002674143.jpg ['a little girl covered in paint sits in front of a painted rainbow with her hands in a bowl', 'a little girl is sitting in front of a large painted rainbow', 'a small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it', 'there is a girl with pigtails sitting in front of a rainbow painting', 'young girl with pigtails painting outside in the grass']\n",
      "1003163366.jpg ['a man lays on a bench while his dog sits by him', 'a man lays on the bench to which a white dog is also tied', 'a man sleeping on a bench outside with a white and black dog sitting next to him', 'a shirtless man lies on a park bench with his dog', 'man laying on bench holding leash of dog sitting on ground']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#Aggregate words across all captions\n",
    "def agg_words(dictCaptions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    allWordsInCaptions = []\n",
    "    for id,captions in dictCaptions.items():\n",
    "        for caption in captions:\n",
    "            allWordsInCaptions.extend(caption.split(' '))\n",
    "    return allWordsInCaptions\n",
    "\n",
    "#Cleanup text\n",
    "def cleanup_text(dictCaptions):\n",
    "    \"\"\"\n",
    "    Remove punctuation & special characters, lowercase captions, remove hanging 's, remove extra spaces.\n",
    "    Returns the sum of # of words across all captions. Includes duplicate counts.\n",
    "    \"\"\"\n",
    "    #Pre-process\n",
    "    for id,captions in dictCaptions.items():\n",
    "        clean = []\n",
    "        for caption in captions:\n",
    "            desc = caption.lower().replace(\" 's\", \"s\")\n",
    "            desc = re.sub('[^A-Za-z0-9 ]+', '', desc)  #Remove punc & special chars\n",
    "            desc = desc.replace('   ', ' ').replace('  ', ' ').strip()\n",
    "            clean.append(desc)\n",
    "        dictCaptions[id] = clean\n",
    "\n",
    "#Cleanup text\n",
    "cleanup_text(dictCaptions)\n",
    "if DEBUG:\n",
    "    counter = 0\n",
    "    for key,val in dictCaptions.items():\n",
    "        print(key,val)\n",
    "        if counter>2:\n",
    "            break;\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53033,
     "status": "ok",
     "timestamp": 1588081869369,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0f9XTjDPAo8J",
    "outputId": "0283e774-2e57-4a69-90bf-694433331648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total words across all captions: 33019\n",
      "# of unique words: 2448 \n",
      "\n",
      "leafcovered: 1\n",
      "structure: 8\n",
      "swords: 3\n",
      "wet: 14\n",
      "waitress: 1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Count frequency of all words\n",
    "def count_word_freq(dictCaptions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Aggregate words across all captions\n",
    "    allWordsInCaptions = agg_words(dictCaptions)\n",
    "    numWords = len(allWordsInCaptions)\n",
    "    \n",
    "    #Count\n",
    "    dictWords = {}\n",
    "    for word in allWordsInCaptions:\n",
    "        if word not in dictWords:\n",
    "            dictWords[word] = 0\n",
    "        dictWords[word] += 1\n",
    "    return numWords, dictWords\n",
    "\n",
    "#Count frequency of all words\n",
    "numWords, dictWords = count_word_freq(dictCaptions)\n",
    "print('# of total words across all captions:', numWords)\n",
    "print('# of unique words:', len(dictWords), '\\n')\n",
    "if DEBUG:\n",
    "    for key in random.sample(list(dictWords.keys()), 5):\n",
    "        print(key+':', dictWords[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53328,
     "status": "ok",
     "timestamp": 1588081869678,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0p16yVjgA44w",
    "outputId": "e47bc0e4-e266-4628-fd8a-507b42e75332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 600\n",
      "# of total words across all captions: 31909\n",
      "# of unique words: 1338 \n",
      "\n",
      "1067180831.jpg ['a black and white dog is attempting to catch a yellow and purple object in a low cut yard', 'a black and white dog jumps after a yellow toy', 'a black and white dog jumps to get the frisbee', 'a black dog is jumping up to catch a purple and green toy', 'a dog jumps to catch a toy']\n",
      "101669240.jpg ['a man in a hat is displaying pictures next to a skier in a blue hat', 'a man skis past another man displaying in the snow', 'a person wearing skis looking at framed pictures set up in the snow', 'a skier looks at framed pictures in the snow next to trees', 'man on skis looking at for sale in the snow']\n",
      "1227655020.jpg ['a black dog and a brown dog playing in tall', 'a brown dog and a black dog are standing against each other in some grass', 'two dogs are playing outside in a field', 'two dogs play in tall grass', 'two dogs wrestle together in the long grass']\n",
      "129599450.jpg ['a backpacker in the mountains wearing an american flag', 'a hiker an american flag', 'a hiker with an american flag on his backpack walks through the woods', 'a man is hiking on a mountain path with an american flag on his backpack', 'a person hikes on a mountain']\n",
      "1469358746.jpg ['a man and his dog in the mountains', 'a man and his dog on the top of a mountain', 'a man holding a camera is sitting on a mountain with a black dog', 'a man poses with his black dog at the top of a mountain', 'a man with a dog']\n"
     ]
    }
   ],
   "source": [
    "#Drop infrequent words from captions, reducing vocabulary size\n",
    "def drop_infrequent(dictCaptions, dictWords):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Form dict of frequent words\n",
    "    truncated = []\n",
    "    for word in dictWords.keys():\n",
    "        if dictWords[word]/numWords > 0.00005:  #0.005%\n",
    "            truncated.append(word)\n",
    "    dictWordsTrunc = dict.fromkeys(truncated , 0)\n",
    "\n",
    "    #Drop infrequent\n",
    "    for id, captions in dictCaptions.items():\n",
    "        newCaptions = []\n",
    "        for caption in captions:\n",
    "            newCaption = []\n",
    "            for word in caption.split(' '):\n",
    "                if word in dictWordsTrunc:\n",
    "                    newCaption.append(word)\n",
    "            newCaptions.append(' '.join(newCaption))\n",
    "        dictCaptions[id] = newCaptions\n",
    "\n",
    "#Drop infrequent words\n",
    "drop_infrequent(dictCaptions, dictWords)\n",
    "print('Total samples:', len(dictCaptions))\n",
    "print('# of total words across all captions:', len(agg_words(dictCaptions)))\n",
    "print('# of unique words:', len(set(agg_words(dictCaptions))), '\\n')\n",
    "if DEBUG:\n",
    "    for key in random.sample(list(dictCaptions.keys()), 5):\n",
    "        print(key, dictCaptions[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54232,
     "status": "ok",
     "timestamp": 1588081870593,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "M3_M8jPqCRWX",
    "outputId": "80b91ba9-9850-479e-e53d-c2045bb846dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 : 432 120 48\n",
      "600 : 432 120 48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Create train, dev & test datasets\n",
    "def split_dataset():\n",
    "    #Split\n",
    "    X = list(dictCaptions.keys())\n",
    "    y = list(dictCaptions.values())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)  #Train+Dev vs. Test\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.10, random_state=42)  #Train vs. Dev\n",
    "    return X_train, X_dev, X_test, y_train, y_dev, y_test\n",
    "\n",
    "#Split\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = split_dataset()\n",
    "print(len(X_train)+len(X_test)+len(X_dev), ':', len(X_train), len(X_test), len(X_dev))\n",
    "print(len(y_train)+len(y_test)+len(y_dev), ':', len(y_train), len(y_test), len(y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence length: 35\n",
      "Vocabulary size: 1247 \n",
      "\n",
      "2160 \t 240 \t 600\n",
      "432 \t 48 \t 120\n",
      "(25254, 2048) \t (25254, 35) \t (25254, 1247)\n",
      "(2670, 2048) \t (2670, 35) \t (2670, 1247)\n",
      "(6717, 2048) \t (6717, 35) \t (6717, 1247)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Flatten sets and add startseq endseq bookends\n",
    "def flatten_add_bookends(X, y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Xy = []\n",
    "    idx = 0\n",
    "    for captions in y:\n",
    "        Xy.extend([[X[idx], 'startseq ' + cap + ' endseq'] for cap in captions])\n",
    "        idx += 1\n",
    "    return Xy\n",
    "\n",
    "#Create inputs images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_seq_len, Xy, features, vocab_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Foreach sample...\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for key, value in Xy:\n",
    "        #Encode: text to integers\n",
    "        seq = tokenizer.texts_to_sequences([value])[0]\n",
    "        \n",
    "        #Split sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            #Split\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            \n",
    "            #Pad input sequences\n",
    "            in_seq = pad_sequences([in_seq], maxlen = max_seq_len)[0]\n",
    "            \n",
    "            #Encode output sequence: \n",
    "            out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n",
    "            \n",
    "            #Store\n",
    "            X1.append(features[key][0])\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "#Convert the dataset into the right format for our custom net + LSTM\n",
    "def process_dataset(X_train, X_dev, X_test, y_train, y_dev, y_test):\n",
    "    #Flatten sets ++ startseq endseq bookends\n",
    "    Xy_train = flatten_add_bookends(X_train, y_train)\n",
    "    Xy_dev = flatten_add_bookends(X_dev, y_dev)\n",
    "    Xy_test = flatten_add_bookends(X_test, y_test)\n",
    "    \n",
    "    #Compute max length of tokens in train set\n",
    "    max_seq_len = max([len(token.split(' ')) for val in Xy_train for token in val])\n",
    "    \n",
    "    #Create tokenizer and fit on train set\n",
    "    only_captions = [val[1] for val in Xy_train]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(only_captions)\n",
    "    \n",
    "    #Load transfer learning features from pickle file\n",
    "    features_all = load(open(dictInit['FN_GEN_RESULT'], 'rb'))\n",
    "    features_train = {k: features_all[k] for k in X_train}\n",
    "    features_dev = {k: features_all[k] for k in X_dev}\n",
    "    features_test = {k: features_all[k] for k in X_test}\n",
    "    \n",
    "    #Create training sequences\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    X1train, X2train, ytrain = create_sequences(tokenizer, max_seq_len, Xy_train, features_train, vocab_size)\n",
    "    \n",
    "    #Create dev sequences\n",
    "    X1dev, X2dev, ydev = create_sequences(tokenizer, max_seq_len, Xy_dev, features_dev, vocab_size)\n",
    "    \n",
    "    #Create test sequences\n",
    "    X1test, X2test, ytest = create_sequences(tokenizer, max_seq_len, Xy_test, features_test, vocab_size)\n",
    "    \n",
    "    #Return\n",
    "    return tokenizer, max_seq_len, vocab_size, \\\n",
    "            Xy_train, Xy_dev, Xy_test, \\\n",
    "            features_train, features_dev, features_test, \\\n",
    "            X1train, X2train, ytrain, X1dev, X2dev, ydev, X1test, X2test, ytest\n",
    "\n",
    "#Process dataset\n",
    "tokenizer, max_seq_len, vocab_size, Xy_train, Xy_dev, Xy_test, features_train, features_dev, features_test, X1train, X2train, ytrain, X1dev, X2dev, ydev, X1test, X2test, ytest = process_dataset(X_train, X_dev, X_test, y_train, y_dev, y_test)\n",
    "print('Longest sequence length:', max_seq_len)\n",
    "print('Vocabulary size:', vocab_size, '\\n')\n",
    "\n",
    "print(len(Xy_train), '\\t', len(Xy_dev), '\\t', len(Xy_test))\n",
    "print(len(features_train), '\\t', len(features_dev), '\\t', len(features_test))\n",
    "print(X1train.shape, '\\t', X2train.shape, '\\t', ytrain.shape)\n",
    "print(X1dev.shape, '\\t', X2dev.shape, '\\t', ydev.shape)\n",
    "print(X1test.shape, '\\t', X2test.shape, '\\t', ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ug3D8Im8HQqZ"
   },
   "source": [
    "## Create & Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30820,
     "status": "ok",
     "timestamp": 1588082067473,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hRp9_ly-8urm",
    "outputId": "7ee7d535-d2f6-46cf-d321-022145a64fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of vectors loaded: 400000\n",
      "Time taken: 41.016533851623535\n",
      "(1247, 300)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#Create a matrix of GloVe word vectors of all words in our vocabulary\n",
    "def createEmbeddingMatrix(dict_word_index, vocab_size):\n",
    "    #Load GloVe word vectors\n",
    "    start = time.time()\n",
    "    dict_glove = {}\n",
    "    lines = read_file(dictInit['FN_EMBEDDING'])\n",
    "    with open(dictInit['FN_EMBEDDING']) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            dict_glove[word] = coefs\n",
    "    print('# of vectors loaded:', len(dict_glove))\n",
    "    print('Time taken:', time.time()-start)\n",
    "\n",
    "    #Create embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, dictInit['EMBEDDING_DIM']))\n",
    "    for word, i in dict_word_index.items():\n",
    "        embedding_vector = dict_glove.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  #Words not found in GloVe will be all-zeros\n",
    "    return embedding_matrix\n",
    "\n",
    "#Create a matrix of GloVe word vectors of all words in our vocabulary\n",
    "if dictInit['TRAIN_MODEL']:\n",
    "    embedding_matrix = createEmbeddingMatrix(tokenizer.word_index, vocab_size)\n",
    "    print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63520,
     "status": "ok",
     "timestamp": 1588082100188,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "T_EtOFVe2egn",
    "outputId": "296df2c5-928d-48b3-f17b-681bf377556a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 35, 300)      374100      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 300)          614700      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 300)          721200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 300)          0           dense_7[0][0]                    \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 300)          90300       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1247)         375347      dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,175,647\n",
      "Trainable params: 1,801,547\n",
      "Non-trainable params: 374,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25254 samples, validate on 2670 samples\n",
      "Epoch 1/3\n",
      " - 205s - loss: 4.1887 - val_loss: 3.8450\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.84502, saving model to ./Data/Weights/weights.best_Inceptionv3_Flickr_8K.hdf5\n",
      "Epoch 2/3\n",
      " - 203s - loss: 3.3241 - val_loss: 3.6206\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.84502 to 3.62058, saving model to ./Data/Weights/weights.best_Inceptionv3_Flickr_8K.hdf5\n",
      "Epoch 3/3\n"
     ]
    }
   ],
   "source": [
    "if dictInit['TRAIN_MODEL']:\n",
    "    from keras.utils import plot_model\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Embedding\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers.merge import add\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    \n",
    "    #Define the captioning model\n",
    "    def define_model(vocab_size, max_seq_len):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # feature extractor model\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            inputs1 = Input(shape=(2048,))\n",
    "        else:\n",
    "            inputs1 = Input(shape=(4096,))\n",
    "        fe1 = inputs1  #Dropout(0.2)(inputs1)\n",
    "        fe2 = Dense(300, activation='relu')(fe1)\n",
    "        \n",
    "        # sequence model\n",
    "        inputs2 = Input(shape=(max_seq_len,))\n",
    "        # se1 = Embedding(vocab_size, 300, mask_zero=True)(inputs2)\n",
    "        se1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_len, trainable=False)(inputs2)\n",
    "        se2 = se1  #Dropout(0.2)(se1)\n",
    "        se3 = LSTM(300)(se2)\n",
    "        \n",
    "        # decoder model\n",
    "        decoder1 = add([fe2, se3])\n",
    "        decoder2 = Dense(300, activation='relu')(decoder1)\n",
    "        outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "        \n",
    "        # tie it together [image, seq] [word]\n",
    "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        \n",
    "        # summarize model\n",
    "        print(model.summary())\n",
    "        plot_model(model, to_file=dictInit['FN_TRAIN_MODEL'], show_shapes=True)\n",
    "        return model\n",
    "\n",
    "    # fit model\n",
    "\n",
    "    # define the model & checkpoint callback\n",
    "    model = define_model(vocab_size, max_seq_len)\n",
    "    checkpoint = ModelCheckpoint(dictInit['FN_WEIGHTS'], monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # fit model\n",
    "    model.fit([X1train, X2train], ytrain, epochs=3, verbose=2, callbacks=[checkpoint], validation_data=([X1dev, X2dev], ydev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KJvdPpDD19E"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67278,
     "status": "ok",
     "timestamp": 1588082103956,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hP4meSU1vNQc",
    "outputId": "a0950deb-daaa-4187-c52c-d5354af6bcba"
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo_features, max_seq_len):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_seq_len):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen = max_seq_len)\n",
    "        \n",
    "        # predict next word\n",
    "        yhat = model.predict([photo_features,sequence], verbose = 0)\n",
    "        \n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        \n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        \n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        \n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "#Evaluate model\n",
    "def evaluate_model(model, Xy_test, features_test, tokenizer, max_seq_len):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Foreach test case...\n",
    "    actual, predicted = list(), list()\n",
    "    idx = 0\n",
    "    for idx in range(len(Xy_test)):\n",
    "        #Predict\n",
    "        photo_id = Xy_test[idx][0]\n",
    "        actual_description = Xy_test[idx][1]\n",
    "        yhat = generate_desc(model, tokenizer, features_test[photo_id], max_seq_len)  #predicted_description\n",
    "    \n",
    "        #Store\n",
    "#         actual.append(actual_description.replace('startseq ','').replace(' endseq','').split(' '))\n",
    "#         predicted.append(yhat.replace('startseq ','').replace(' endseq','').split(' '))\n",
    "        actual.append(actual_description.split(' '))\n",
    "        predicted.append(yhat.split(' '))\n",
    "    \n",
    "    #Compute BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "# load the model\n",
    "model_custom_from_weights = load_model(dictInit['FN_WEIGHTS'])\n",
    "# print(model_custom_from_weights.summary())\n",
    "plot_model(model_custom_from_weights, to_file = dictInit['FN_TRAIN_REMODEL'], show_shapes=True)\n",
    "\n",
    "# evaluate model\n",
    "evaluate_model(model_custom_from_weights, Xy_test, features_test, tokenizer, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 105216,
     "status": "ok",
     "timestamp": 1588082141905,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "xW19WkiyvOOs",
    "outputId": "b30bd86f-5050-4005-aba4-2a8b2abb2406"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "\n",
    "#Config model imports\n",
    "if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "    from keras.applications.inception_v3 import InceptionV3\n",
    "    from keras.applications.inception_v3 import preprocess_input\n",
    "else:\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "#Load Model\n",
    "if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "    model_TL = InceptionV3(include_top=False, weights='imagenet')\n",
    "else:\n",
    "    model_TL = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "#Restructure the model\n",
    "# model_TL.layers.pop()\n",
    "model_TL = Model(inputs = model_TL.inputs, outputs = model_TL.layers[-1].output)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1LmTvQ8D4ONvEnj3npjPvpRvgQTOLdUGi"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21650,
     "status": "ok",
     "timestamp": 1588084779903,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "DJprBlgr--wM",
    "outputId": "e6f11dd4-9cd6-4a34-9dae-b78e47b51de3"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from textwrap import wrap\n",
    "\n",
    "#Plot sample test images\n",
    "def plotSamples(X_test):\n",
    "    #Get random images\n",
    "    testIDs = random.sample(X_test, 16)\n",
    "    # testIDs = [id.split('_')[0] + '.' + id.split('_')[1].split('.')[1] for id in testIDs]\n",
    "    print(testIDs)\n",
    "\n",
    "    #Init plot\n",
    "    plt.rcParams['figure.facecolor'] = 'black'\n",
    "    fig=plt.figure(figsize=(22, 24))\n",
    "    fig.patch.set_facecolor('black')\n",
    "    columns = 4\n",
    "    rows = 4\n",
    "\n",
    "    #Plot\n",
    "    for i in range(1, columns*rows+1):\n",
    "        #Predict description\n",
    "        photo_features = features_test[testIDs[i-1]]\n",
    "        print(testIDs[i-1], photo_features)\n",
    "        yhat = generate_desc(model_custom_from_weights, tokenizer, photo_features, max_seq_len)\n",
    "        yhat = yhat.replace('startseq ', '').replace(' endseq', '')\n",
    "\n",
    "        #Plot image and description together\n",
    "        img = mpimg.imread(dictInit['DIR_ALL_IMAGES']+testIDs[i-1])\n",
    "        ax = fig.add_subplot(rows, columns, i)\n",
    "        ax.set_title(\"\\n\".join(wrap(yhat, 50)), color='white')\n",
    "        ax.set_facecolor('xkcd:black')\n",
    "        plt.imshow(img)\n",
    "    plt.savefig(dictInit['FN_TEST_RESULTS']+'_'+datetime.today().strftime('%Y-%m-%d-%H:%M:%S')+'.png', bbox_inches='tight', facecolor='black', dpi = 50)\n",
    "    plt.show()\n",
    "    \n",
    "#Plot sample test images\n",
    "plotSamples(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Papineni K, Roukos S, Ward T, Zhu W, (Jul 2002), [BLEU: a method for automatic evaluation of machine translation, Proceedings of the 40th Annual Meeting on Association for Computational Linguistics](https://dl.acm.org/doi/10.3115/1073083.1073135)\n",
    "\n",
    "Szegedy C et al., (Sep 2014), [Going Deeper with Convolutions,  arXiv:1409.4842 cs.CV](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "Simonyan K, Zisserman A, (Sep 2014), [Very Deep Convolutional Networks for Large-Scale Image Recognition, arXiv:1409.1556 cs.CV](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Tanti M, Gatt A, Camilleri K.P., (Aug 2017), [What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?, arXiv:1708.02043 cs.CL](https://arxiv.org/abs/1708.02043)\n",
    "\n",
    "Brownlee J., (Jun 2019), [How to Develop a Deep Learning Photo Caption Generator from Scratch, Machine Learning Mastery](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/)\n",
    "\n",
    "Plummer B. et al., (May 2015), [Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models, arXiv:1505.04870 cs.CV](https://arxiv.org/abs/1505.04870)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Image_Caption_v1.0.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
