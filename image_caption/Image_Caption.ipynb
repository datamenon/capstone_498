{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drRBfB9mX-Ki"
   },
   "source": [
    "Replaced embedding layer with GloVe word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xoAN6_wV9DeM"
   },
   "outputs": [],
   "source": [
    "# Clear all vars\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxhlmvWsOdmX"
   },
   "source": [
    "## 1.0 Running on Colab / GCP / Local\n",
    "If running on Colab, ensure the data is hosted on your Google drive in the identical directory structure as listed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49176,
     "status": "ok",
     "timestamp": 1588081865462,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "qTcM6SLmMe7Z",
    "outputId": "32eb9908-2a65-410d-9f76-34462a955449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB False\n",
      "DIR_PREFIX ./Data/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# If running in Colab, change directory access paths\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    #Access google drive to load data\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DIR_PREFIX = 'drive/My Drive/Colab Notebooks/image_caption/Data/'\n",
    "else:\n",
    "    DIR_PREFIX = './Data/'\n",
    "print('IN_COLAB', IN_COLAB)\n",
    "print('DIR_PREFIX', DIR_PREFIX)\n",
    "\n",
    "# Init\n",
    "GENERATE_FEATURES = False  #Set to True if features need to be generated through the underlying pre-trained net\n",
    "LEARN = True  #Set to True if our custom net needs to learn the dataset\n",
    "TL_MODEL = 'Inceptionv3'  #Pre-trained model used for recognizing obects within images. One of Inceptionv3 or VGG16 trained on ImageNet.\n",
    "MAX_SAMPLES = 50  #Maximum number of samples to train & test with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-e-Bo4GMmJn"
   },
   "source": [
    "## Directories Required\n",
    "Ensure you have the following directory structure no matter where you are hosting the data: GCP Storage, GCP Compute Engine local disk, Google Drive or your local desktop/laptop.\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://drive.google.com/uc?id=1GDrk6lxt_AXAkU2jvv6kFGdnC1D4x1iq\" alt=\"Broken URL. Contact Anand Menon.\" width=\"60%\"/>\n",
    "</div>\n",
    "\n",
    "The root folder is 'image_caption'. The rest follow from it.<br/>\n",
    "For Colab, there is one additional requirement. Ensure 'image_caption' is placed under 'Colab Notebooks' folder (which is created by Google automatically the first time you create a Colab notebook). Why? Since files within Google Colab are referenced with their full path starting all the way back to \"drive\".<br/>\n",
    "For example:\n",
    "filename = 'drive/My Drive/Colab Notebooks/Image Caption/Data/Flickr8k/Labels/Flickr_8K.token.txt'\n",
    "\n",
    "## Files Required\n",
    "The following files are required to run the code. All the files are shared, so you should ideally be able to copy the entire directory structure. Warning: It is over 2 GB, soon to be 10 GB.<br/>\n",
    "You will need to recreate the folder structure locally (with files and all) since the code writes files back to it.\n",
    "\n",
    "[Link to Data folder](https://drive.google.com/open?id=1bQtuMVTyaXCMGMC0XifOC2aKb1UGTSo6)\n",
    "\n",
    "### Dataset Files: Train, test, validate\n",
    "* Data/Flickr_8K/Images/Images/&lt;Images&gt;<br/>\n",
    "<!-- <div>\n",
    "    <img src=\"https://drive.google.com/uc?id=1le5RBL4d01JMch_lGJXQMYjRseX06m18\" alt=\"Broken URL. Contact Anand Menon.\" width=\"40%\"/>\n",
    "</div>\n",
    " -->\n",
    "* Data/Flickr_8K/Images/Labels/&lt;Labels&gt;<br/>\n",
    "<!-- <div>\n",
    "    <img src=\"https://drive.google.com/uc?id=1wXMKnKfbsixjJ41IQbJsVE7K1tCF-vfc\" alt=\"Broken URL. Contact Anand Menon.\" width=\"40%\"/>\n",
    "</div>\n",
    " -->\n",
    " \n",
    "### Word Vectors File\n",
    "\n",
    "* Data/Flickr_8K/Images/Labels/&lt;Word vectors&gt;<br/>\n",
    "You only need one of these depending upon how well trained the vectors need to be for your need. The 300D is better (and much bigger) than the 100D file.\n",
    "    * glove.6B.100d.txt\n",
    "    * glove.6B.300d.txt\n",
    "<!-- <div>\n",
    "    <img src=\"https://drive.google.com/uc?id=1-8Sdehe9n5TTHSurjbgn6P7jv8W-Bff_\" alt=\"Broken URL. Contact Anand Menon.\" width=\"50%\"/>\n",
    "</div>\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-K9QSJM3Pka"
   },
   "outputs": [],
   "source": [
    "# Generate features via underlying pre-trained net\n",
    "if GENERATE_FEATURES:\n",
    "    from pickle import dump\n",
    "    from keras.preprocessing.image import load_img\n",
    "    from keras.preprocessing.image import img_to_array\n",
    "    from keras.models import Model\n",
    "    from keras.utils import plot_model\n",
    "    from os import listdir\n",
    "    import time\n",
    "    \n",
    "    # Config\n",
    "    if TL_MODEL == 'Inceptionv3':\n",
    "        from keras.applications import InceptionV3\n",
    "        from keras.applications.inception_v3 import preprocess_input\n",
    "    else:        \n",
    "        from keras.applications.vgg16 import VGG16\n",
    "        from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "    # Extract features from each photo in the directory\n",
    "    def extract_features(directory):\n",
    "        # Load Model\n",
    "        if TL_MODEL == 'Inceptionv3':\n",
    "            model = InceptionV3(include_top=True, weights='imagenet')\n",
    "        else:\n",
    "            model = VGG16(include_top=True, weights='imagenet')\n",
    "\n",
    "        # Remove the top layer, retaining the features generated up-to the layer below\n",
    "        model.layers.pop()\n",
    "        model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "        print(model.summary())\n",
    "        plot_model(model, to_file=DIR_PREFIX + 'Model/model_transfer_learning_'+TL_MODEL+'.png', show_shapes=True)\n",
    "\n",
    "        # Extract features from each photo\n",
    "        counter = 0\n",
    "        features = dict()\n",
    "        for name in listdir(directory):\n",
    "            # Load an image from file\n",
    "            filename = directory + '/' + name\n",
    "            if TL_MODEL == 'Inceptionv3':\n",
    "                image = load_img(filename, target_size=(299, 299), interpolation='bicubic')\n",
    "            else:\n",
    "                image = load_img(filename, target_size=(224, 224), interpolation='bicubic')\n",
    "    \n",
    "            # Reshape data for the model\n",
    "            image = img_to_array(image)\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "            # Prepare image for the model\n",
    "            image = preprocess_input(image)\n",
    "    \n",
    "            # Predict\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            features[name] = feature\n",
    "            if counter % 1000 == 0:\n",
    "                print(counter, '%s' % name, time.time())\n",
    "            counter += 1\n",
    "        return features\n",
    "\n",
    "    # Extract features from all images\n",
    "    directory = DIR_PREFIX + 'Flickr_8K/Images'\n",
    "    features = extract_features(directory)\n",
    "    print('Extracted Features: %d' % len(features))\n",
    "\n",
    "    # Save to file\n",
    "    dump(features, open(DIR_PREFIX + 'Model/features_'+TL_MODEL+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52242,
     "status": "ok",
     "timestamp": 1588081868543,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "SvDs-f-E7_Qx",
    "outputId": "8830b1fc-62fc-44f0-cbe3-eb0ef68b093b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .\\n1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .\\n1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .\\n1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her playhouse .\\n10002682'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load provided filename\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "#Load captions\n",
    "filename = DIR_PREFIX + 'Flickr_8K/Labels/Flickr_8K.token.txt'\n",
    "doc = load_doc(filename)\n",
    "doc[0:333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52230,
     "status": "ok",
     "timestamp": 1588081868544,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "WUe0dQdu8qYd",
    "outputId": "d44d27c4-89b3-4c36-aeb5-6884b7974574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8092\n",
      "2271131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a child in a pink dress is climbing up a set of stairs in an entry way . a girl going into a wooden building . a little girl climbing into a wooden playhouse . a little girl climbing the stairs to her playhouse . a little girl in a pink dress going into a wooden cabin . a black dog and a spotted dog are fighting a black dog and a t'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_captions(doc):\n",
    "    #Split on \\n\n",
    "    doc = doc.split('\\n')\n",
    "\n",
    "    #Split on id vs. description\n",
    "    dictCaptions = {}  #Key = Photo identifier, Value = List of captions\n",
    "    fullText = []  #For cleaning & reducing vocabulary size\n",
    "    for line in doc:\n",
    "        if line.strip() == '':\n",
    "            continue\n",
    "        tokens = line.split('\\t')\n",
    "        photo_id = tokens[0][0:-2]  #Up to .jpg\n",
    "        photo_caption = tokens[1:][0].lower()  #Rest of words\n",
    "        fullText.extend([photo_caption])\n",
    "        if photo_id not in dictCaptions:\n",
    "            dictCaptions[photo_id] = []\n",
    "        dictCaptions[photo_id].extend([photo_caption])\n",
    "    fullText = ' '.join(fullText)\n",
    "    return dictCaptions, fullText\n",
    "\n",
    "#Extract all words and captions\n",
    "dictCaptions, fullText = extract_captions(doc)\n",
    "print(len(dictCaptions))\n",
    "print(len(fullText))\n",
    "# print(dictCaptions)\n",
    "fullText[0:333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53043,
     "status": "ok",
     "timestamp": 1588081869368,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "nWkwIQrS9mKR",
    "outputId": "1f4a8819-0f8f-4281-bc26-7ec9d70ffb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518387\n"
     ]
    }
   ],
   "source": [
    "#Check if gensim already exists\n",
    "try:\n",
    "    from gensim.parsing.preprocessing import strip_punctuation\n",
    "except ImportError as e:\n",
    "    !pip install gensim\n",
    "    from gensim.parsing.preprocessing import strip_punctuation\n",
    "\n",
    "#Remove punctuation\n",
    "fullText_nopunc = strip_punctuation(fullText)\n",
    "words = fullText_nopunc.split(' ')\n",
    "numWords = len(words)\n",
    "print(numWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53033,
     "status": "ok",
     "timestamp": 1588081869369,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0f9XTjDPAo8J",
    "outputId": "0283e774-2e57-4a69-90bf-694433331648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8489\n",
      "['phrase', 'leggings', 'priest', 'pump', 'waitresses', 'older', 'longeared', 'buys', 'licked', 'hippie']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Count frequency of all words\n",
    "dictWords = {}\n",
    "for word in words:\n",
    "    if word not in dictWords:\n",
    "        dictWords[word] = 0\n",
    "    dictWords[word] += 1\n",
    "print(len(dictWords))\n",
    "print(random.sample(list(dictWords.keys()), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53328,
     "status": "ok",
     "timestamp": 1588081869678,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0p16yVjgA44w",
    "outputId": "e47bc0e4-e266-4628-fd8a-507b42e75332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518387\n",
      "488975\n",
      "1106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'child', 'in', 'pink', 'dress', 'is', 'climbing', 'up', 'set', 'of']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Form dict of frequent words; in order to reduce vocabulary size\n",
    "truncated = []\n",
    "for word in words:\n",
    "    if word in dictWords:\n",
    "        if dictWords[word]/numWords > 0.00005:  #0.005%\n",
    "            truncated.append(word)\n",
    "print(len(words))\n",
    "print(len(truncated))\n",
    "\n",
    "dictWordsTrunc = {}\n",
    "for word in truncated:\n",
    "    if word not in dictWordsTrunc:\n",
    "        dictWordsTrunc[word] = 0\n",
    "    dictWordsTrunc[word] += 1\n",
    "print(len(dictWordsTrunc))\n",
    "list(dictWordsTrunc.keys())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54232,
     "status": "ok",
     "timestamp": 1588081870593,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "M3_M8jPqCRWX",
    "outputId": "80b91ba9-9850-479e-e53d-c2045bb846dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 8092 \n"
     ]
    }
   ],
   "source": [
    "# Drop infrequent words from captions, reducing vocabulary size\n",
    "def drop_infrequent(dictCaptions):\n",
    "    dictTemp = {}\n",
    "    for id, captions in dictCaptions.items():\n",
    "        if id not in dictTemp:\n",
    "            dictTemp[id] = []\n",
    "        for caption in captions:\n",
    "            newCaption = []\n",
    "            for word in caption.split(' '):\n",
    "                if word in dictWordsTrunc:\n",
    "                    newCaption.append(word)\n",
    "            newCaption = ' '.join(newCaption)\n",
    "            dictTemp[id].append(newCaption)\n",
    "    return dictTemp\n",
    " \n",
    "#Drop infrequent words\n",
    "descriptions = drop_infrequent(dictCaptions)\n",
    "print('Total samples: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4438,
     "status": "ok",
     "timestamp": 1588082012271,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "3Ng4JnMkY1Bx",
    "outputId": "3167d455-4e03-4ab1-8d53-986ddca38bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLines loaded: 51\n",
      "Descriptions: train=52\n",
      "Photos: train=52\n",
      "Vocabulary Size: 421\n",
      "Max Description Length: 24\n",
      "(3003, 2048)\n",
      "(3003, 24)\n",
      "(3003, 421)\n",
      "\tLines loaded: 26\n",
      "Dataset: 27\n",
      "Descriptions: test=27\n",
      "Photos: test=27\n",
      "(1350, 2048)\n",
      "(1350, 24)\n",
      "(1350, 421)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename, max_samples=-1):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    counter = 0\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        dataset.append(line)\n",
    "        if (max_samples > -1) and (counter > max_samples):\n",
    "            break\n",
    "        counter += 1\n",
    "    print('\\tLines loaded:', counter)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(all_descriptions, dataset):\n",
    "    descriptions = dict()\n",
    "    for image_id,image_desc_list in all_descriptions.items():\n",
    "        if image_id not in dataset: \n",
    "            continue;  #Skip if not in list of dataset id's\n",
    "        if image_id not in descriptions:\n",
    "            descriptions[image_id] = list()\n",
    "        # print(image_id, image_desc_list)\n",
    "        for image_desc in image_desc_list:\n",
    "            # print('\\t', image_desc)\n",
    "            desc = 'startseq ' + image_desc + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each image identifier\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "# train dataset\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = DIR_PREFIX + 'Flickr_8K/Labels/Flickr_8K.trainImages.txt'\n",
    "train = load_set(filename, MAX_SAMPLES)\n",
    "\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions(descriptions, train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "\n",
    "# photo features\n",
    "train_features = load_photo_features(DIR_PREFIX + 'Model/features_'+TL_MODEL+'.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "\n",
    "# save the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "\n",
    "print('Max Description Length: %d' % max_length)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n",
    "print(X1train.shape)\n",
    "print(X2train.shape)\n",
    "print(ytrain.shape)\n",
    "\n",
    "# dev dataset\n",
    "\n",
    "# load test set\n",
    "filename = DIR_PREFIX + 'Flickr_8K/Labels/Flickr_8K.devImages.txt'\n",
    "test = load_set(filename, MAX_SAMPLES/2)\n",
    "print('Dataset: %d' % len(test))\n",
    "\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions(descriptions, test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "\n",
    "# photo features\n",
    "test_features = load_photo_features(DIR_PREFIX + 'Model/features_'+TL_MODEL+'.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)\n",
    "print(X1test.shape)\n",
    "print(X2test.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ug3D8Im8HQqZ"
   },
   "source": [
    "## Create & Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30820,
     "status": "ok",
     "timestamp": 1588082067473,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hRp9_ly-8urm",
    "outputId": "7ee7d535-d2f6-46cf-d321-022145a64fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "(421, 300)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if LEARN:\n",
    "    #Load GloVe word vectors\n",
    "    GLOVE_DIR = DIR_PREFIX + 'Vectors/'\n",
    "\n",
    "    #Create embedding matrix\n",
    "    EMBEDDING_DIM = 300\n",
    "\n",
    "    embeddings_index = {}\n",
    "    if (EMBEDDING_DIM == 100):\n",
    "        embedding_filename = 'glove.6B.100d.txt'\n",
    "    else:\n",
    "        embedding_filename = 'glove.6B.300d.txt'\n",
    "    f = open(os.path.join(GLOVE_DIR, embedding_filename))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63520,
     "status": "ok",
     "timestamp": 1588082100188,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "T_EtOFVe2egn",
    "outputId": "296df2c5-928d-48b3-f17b-681bf377556a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 24, 300)      126300      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 24, 300)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          614700      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 300)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 300)          90300       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 421)          126721      dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,679,221\n",
      "Trainable params: 1,552,921\n",
      "Non-trainable params: 126,300\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "./Data/Model/model_custom_Inceptionv3.png\n",
      "Train on 3003 samples, validate on 1350 samples\n",
      "Epoch 1/3\n",
      " - 24s - loss: 4.8251 - val_loss: 4.1816\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.18164, saving model to ./Data/Weights/weights.best_Inceptionv3.hdf5\n",
      "Epoch 2/3\n",
      " - 22s - loss: 3.9185 - val_loss: 3.8946\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.18164 to 3.89458, saving model to ./Data/Weights/weights.best_Inceptionv3.hdf5\n",
      "Epoch 3/3\n",
      " - 22s - loss: 3.1955 - val_loss: 3.7096\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.89458 to 3.70961, saving model to ./Data/Weights/weights.best_Inceptionv3.hdf5\n"
     ]
    }
   ],
   "source": [
    "if LEARN:\n",
    "    from keras.utils import plot_model\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Embedding\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers.merge import add\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "    # define the captioning model\n",
    "    def define_model(vocab_size, max_length):\n",
    "        # feature extractor model\n",
    "        if TL_MODEL == 'Inceptionv3':\n",
    "            inputs1 = Input(shape=(2048,))\n",
    "        else:\n",
    "            inputs1 = Input(shape=(4096,))\n",
    "        fe1 = Dropout(0.2)(inputs1)\n",
    "        fe2 = Dense(300, activation='relu')(fe1)\n",
    "        \n",
    "        # sequence model\n",
    "        inputs2 = Input(shape=(max_length,))\n",
    "        # se1 = Embedding(vocab_size, 300, mask_zero=True)(inputs2)\n",
    "        se1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False)(inputs2)\n",
    "        se2 = Dropout(0.2)(se1)\n",
    "        se3 = LSTM(300)(se2)\n",
    "\n",
    "        # decoder model\n",
    "        decoder1 = add([fe2, se3])\n",
    "        decoder2 = Dense(300, activation='relu')(decoder1)\n",
    "        outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "        \n",
    "        # tie it together [image, seq] [word]\n",
    "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        \n",
    "        # summarize model\n",
    "        print(model.summary())\n",
    "        print(DIR_PREFIX + 'Model/model_custom_'+TL_MODEL+'.png')\n",
    "        plot_model(model, to_file=DIR_PREFIX + 'Model/model_custom_'+TL_MODEL+'.png', show_shapes=True)\n",
    "        return model\n",
    "\n",
    "    # fit model\n",
    "\n",
    "    # define the model\n",
    "    model = define_model(vocab_size, max_length)\n",
    "\n",
    "    # define checkpoint callback\n",
    "    filepath = DIR_PREFIX + 'Weights/weights.best_'+TL_MODEL+'.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # fit model\n",
    "    model.fit([X1train, X2train], ytrain, epochs=3, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KJvdPpDD19E"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67278,
     "status": "ok",
     "timestamp": 1588082103956,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hP4meSU1vNQc",
    "outputId": "a0950deb-daaa-4187-c52c-d5354af6bcba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 24)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 24, 300)      126300      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 24, 300)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          614700      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          721200      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 300)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 300)          90300       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 421)          126721      dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,679,221\n",
      "Trainable params: 1,552,921\n",
      "Non-trainable params: 126,300\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "BLEU-1: 0.573248\n",
      "BLEU-2: 0.337418\n",
      "BLEU-3: 0.212125\n",
      "BLEU-4: 0.083607\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "\n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "# load the model\n",
    "filename = DIR_PREFIX + 'Weights/weights.best_'+TL_MODEL+'.hdf5'\n",
    "model_custom_from_weights = load_model(filename)\n",
    "print(model_custom_from_weights.summary())\n",
    "plot_model(model_custom_from_weights, to_file=DIR_PREFIX + 'Model/model_custom_from_weights_'+TL_MODEL+'.png', show_shapes=True)\n",
    "\n",
    "# evaluate model\n",
    "evaluate_model(model_custom_from_weights, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 105216,
     "status": "ok",
     "timestamp": 1588082141905,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "xW19WkiyvOOs",
    "outputId": "b30bd86f-5050-4005-aba4-2a8b2abb2406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 1s 0us/step\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "\n",
    "#Config model imports\n",
    "if TL_MODEL == 'Inceptionv3':\n",
    "    from keras.applications.inception_v3 import InceptionV3\n",
    "    from keras.applications.inception_v3 import preprocess_input\n",
    "else:\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(model, filename):\n",
    "    # load the photo\n",
    "    if TL_MODEL == 'Inceptionv3':\n",
    "        image = load_img(filename, target_size=(299, 299), interpolation='bicubic')\n",
    "    else:\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    \n",
    "    # prepare the image for the model\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "# Load Model\n",
    "if TL_MODEL == 'Inceptionv3':\n",
    "    model_TL = InceptionV3(include_top=True, weights='imagenet')\n",
    "else:\n",
    "    model_TL = VGG16(include_top=True, weights='imagenet')\n",
    "\n",
    "# re-structure the model\n",
    "model_TL.layers.pop()\n",
    "model_TL = Model(inputs=model_TL.inputs, outputs=model_TL.layers[-1].output)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1LmTvQ8D4ONvEnj3npjPvpRvgQTOLdUGi"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21650,
     "status": "ok",
     "timestamp": 1588084779903,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "DJprBlgr--wM",
    "outputId": "e6f11dd4-9cd6-4a34-9dae-b78e47b51de3"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/Flickr_8K/Images/3652150541_8fb5a3a5d1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-40cfb4bd69fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#Predict description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mphoto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_TL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIR_PREFIX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Flickr_8K/Images/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtestIDs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_custom_from_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'startseq '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' endseq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-46ff10b9ecda>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# load the photo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mTL_MODEL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Inceptionv3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bicubic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/Flickr_8K/Images/3652150541_8fb5a3a5d1.jpg'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1584x1728 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from textwrap import wrap\n",
    "\n",
    "#Get random images\n",
    "testIDs = random.sample(test, 16)\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = 'black'\n",
    "fig=plt.figure(figsize=(22, 24))\n",
    "fig.patch.set_facecolor('black')\n",
    "columns = 4\n",
    "rows = 4\n",
    "for i in range(1, columns*rows+1):\n",
    "    #Predict description\n",
    "    photo = extract_features(model_TL, DIR_PREFIX + 'Flickr_8K/Images/'+testIDs[i-1])\n",
    "    description = generate_desc(model_custom_from_weights, tokenizer, photo, max_length)\n",
    "    description = description.replace('startseq ', '').replace(' endseq', '')\n",
    "\n",
    "    #Plot image and description together\n",
    "    img = mpimg.imread(DIR_PREFIX + 'Flickr_8K/Images/'+testIDs[i-1])\n",
    "    ax = fig.add_subplot(rows, columns, i)\n",
    "    ax.set_title(\"\\n\".join(wrap(description, 50)), color='white')\n",
    "    ax.set_facecolor('xkcd:black')\n",
    "    plt.imshow(img)\n",
    "plt.savefig(DIR_PREFIX + 'Model/results_'+TL_MODEL+'_'+datetime.today().strftime('%Y-%m-%d-%H:%M:%S'), bbox_inches='tight', facecolor='black', dpi = 50)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Image_Caption_v1.0.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
