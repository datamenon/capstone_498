{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-e-Bo4GMmJn"
   },
   "source": [
    "## Directories Required\n",
    "Ensure you have the following directory structure no matter where you are hosting the data: GCP Storage, GCP Compute Engine local disk, Google Drive or your local desktop/laptop.\n",
    "\n",
    "The root folder on your local drive should be 'image_caption'. The rest follow from it.<br/>\n",
    "\n",
    "For Colab, there is one additional requirement. Ensure 'image_caption' is placed under 'Colab Notebooks' folder (which is created by Google automatically the first time you create a Colab notebook). Why? Since files within Google Colab are referenced with their full path starting all the way back to \"drive\".<br/>\n",
    "For example:<br/>\n",
    "filename = 'drive/My Drive/Colab Notebooks/Image Caption/Data/Flickr_8K/Labels/Flickr_8K.token.txt'<br/>\n",
    "filename = 'drive/My Drive/Colab Notebooks/Image Caption/Data/Flickr_30K/Labels/results.txt'<br/>\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://drive.google.com/uc?id=1DrVeLA0fz0XYrPq0CZs6eAvRZbsB4GE1\" alt=\"Broken URL. Contact Anand Menon.\" width=\"60%\"/>\n",
    "</div>\n",
    "\n",
    "## Files Required\n",
    "The following files are required to run the code. All the files are shared, so you should ideally be able to copy the entire directory structure. Warning: It is over 2 GB, soon to be 10 GB.<br/>\n",
    "You will need to recreate the folder structure locally (with files and all) since the code writes files back to it.\n",
    "\n",
    "[Link to Data folder](https://drive.google.com/open?id=1bQtuMVTyaXCMGMC0XifOC2aKb1UGTSo6)\n",
    "\n",
    "### Dataset Files: Train, test, validate\n",
    "* Flickr_8K\n",
    "  * Data/Flickr_8K/Images/&lt;Images&gt;<br/>\n",
    "  * Data/Flickr_8K/Labels/&lt;Labels&gt;<br/>\n",
    "* Flickr_30K\n",
    "   * Data/Flickr_30K/Images/&lt;Images&gt;<br/>\n",
    "   * Data/Flickr_30K/Labels/&lt;Labels&gt;<br/>\n",
    "\n",
    "### Word Vectors File\n",
    "\n",
    "* Data/Vectors/&lt;Word vector file&gt;<br/>\n",
    "You only need one of these depending upon how well trained the vectors need to be for your need. The 300D is better (and much bigger) than the 100D file. Alternatively, feel free to use your own.\n",
    "    * glove.6B.100d.txt\n",
    "    * glove.6B.300d.txt\n",
    "\n",
    "### Transfer Learning Net: Features File\n",
    "In order to speed up training & testing, all features from our underlying transfer learning net are generated in-advance of the train/test code blocks. The first time you run this code, generate the features once by setting GEN_FEATURES = True above. Then set it back to False. The generated file is large, and it will take ~30 minutes to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "drRBfB9mX-Ki"
   },
   "source": [
    "## Change Log\n",
    "1. 13 Apr: Switched to using GloVe encoding from a custom Embedding layer. The latter has the downside that if the number of training inputs are insufficient, the word embeddings it creates will be bad...and there is no easy way to tell if the number of training inputs are insufficient for this particular part of the net (the learning curves will give a global answer). Using GloVe word vectors eliminates this problem. In addition, it means the net is not having to learn the 'meaning' of words at the same time as it is learning to generate captions; hence less distractions for its nano-brain.\n",
    "2. 20 Apr: Dropped words that occur less frequently than 0.005% of the time in the corpus consisting of all captions. This tiny filter dramatically dropped the vocabulary from ~7500 words down to ~1100 words. Naturally, performance went up a bit with very little impact to the English of it all.\n",
    "3. 24 Apr: Switched the underlying pre-trained net over from VGG16 over to Inception v3. VGG16 is a big, plodding net and its architecture older than Inception. Inception also outperformed VGG16 in ImageNet, so overall it helped the performance a bit.\n",
    "4. 30 Apr: Cleaned up the code a bit and switched over to ResNet. The experiment here was to try and mimic human behaviour a bit more. Humans are great at fill in the blanks: The car is turning &lt;blank&gt;. So what is needed here is an identity function that helps recall prior learned patterns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49176,
     "status": "ok",
     "timestamp": 1588081865462,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "qTcM6SLmMe7Z",
    "outputId": "32eb9908-2a65-410d-9f76-34462a955449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN_FEATURES \t False\n",
      "TRAIN_MODEL \t True\n",
      "TL_MODEL \t Inceptionv3\n",
      "MAX_SAMPLES \t 100\n",
      "DATASET \t Flickr_8K\n",
      "IN_COLAB \t False\n",
      "DIR_PREFIX \t ./Data/\n",
      "DIR_ALL_IMAGES \t ./Data/Flickr_8K/Images/\n",
      "FN_ALL_LABELS \t ./Data/Flickr_8K/Labels/Flickr_8K.captions.txt\n",
      "FN_GEN_MODEL \t ./Data/Model/model_transfer_learning_Inceptionv3.png\n",
      "FN_GEN_RESULT \t ./Data/Model/features_Inceptionv3_Flickr_8K.pkl\n",
      "FN_TRAIN_MODEL \t ./Data/Model/model_custom_Inceptionv3.png\n",
      "FN_WEIGHTS \t ./Data/Weights/weights.best_Inceptionv3_Flickr_8K.hdf5\n",
      "FN_TRAIN_REMODEL \t ./Data/Model/model_custom_from_weights_Inceptionv3.png\n",
      "FN_TEST_RESULTS \t ./Data/Model/results_Inceptionv3_Flickr_8K_20.0\n",
      "FN_LEARN_CURVES \t ./Data/Model/results_lc_Inceptionv3_Flickr_8K_20.0\n",
      "EMBEDDING_DIM \t 300\n",
      "FN_EMBEDDING \t ./Data/Vectors/glove.6B.300d.txt\n",
      "DEBUG \t\t True\n"
     ]
    }
   ],
   "source": [
    "#Clear all vars\n",
    "%reset -f\n",
    "\n",
    "#Init code\n",
    "def initialize():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    import sys\n",
    "\n",
    "    # Init\n",
    "    dictInit = {}\n",
    "    dictInit['GEN_FEATURES'] = False  #Set to True if features need to be generated through the underlying pre-trained net\n",
    "    dictInit['TRAIN_MODEL'] = True  #Set to True if our custom net needs to learn the dataset\n",
    "    dictInit['TL_MODEL'] = 'Inceptionv3'  #Pre-trained model used for recognizing obects within images. One of Inceptionv3 or VGG16 trained on ImageNet.\n",
    "    dictInit['MAX_SAMPLES'] = 100  #Maximum number of samples to train & test with. Set to -1 for training on full train set.\n",
    "    dictInit['DATASET'] = 'Flickr_8K'  #One of 'Flickr_8K', 'Flickr_30K'\n",
    "\n",
    "    #Change directory access paths depending upon where you are running\n",
    "    dictInit['IN_COLAB'] = 'google.colab' in sys.modules\n",
    "    if dictInit['IN_COLAB']:\n",
    "        from google.colab import drive  #Access google drive to load data\n",
    "        drive.mount('/content/drive')\n",
    "        dictInit['DIR_PREFIX'] = 'drive/My Drive/Colab Notebooks/image_caption/Data/'\n",
    "    else:\n",
    "        dictInit['DIR_PREFIX'] = './Data/'  #Local\n",
    "\n",
    "    #All directory & file names; Train, Validate, Test etc.\n",
    "    dictInit['DIR_ALL_IMAGES'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Images/'\n",
    "    dictInit['FN_ALL_LABELS'] = dictInit['DIR_PREFIX'] + dictInit['DATASET'] + '/Labels/' + dictInit['DATASET'] + '.captions.txt'\n",
    "    dictInit['FN_GEN_MODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_transfer_learning_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_GEN_RESULT'] = dictInit['DIR_PREFIX'] + 'Model/features_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '.pkl'\n",
    "    dictInit['FN_TRAIN_MODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_custom_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_WEIGHTS'] = dictInit['DIR_PREFIX'] + 'Weights/weights.best_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '.hdf5'\n",
    "    dictInit['FN_TRAIN_REMODEL'] = dictInit['DIR_PREFIX'] + 'Model/model_custom_from_weights_' + dictInit['TL_MODEL'] + '.png'\n",
    "    dictInit['FN_TEST_RESULTS'] = dictInit['DIR_PREFIX'] + 'Model/results_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '_' + str(dictInit['MAX_SAMPLES']/5)\n",
    "    dictInit['FN_LEARN_CURVES'] = dictInit['DIR_PREFIX'] + 'Model/results_lc_' + dictInit['TL_MODEL'] + '_' + dictInit['DATASET'] + '_' + str(dictInit['MAX_SAMPLES']/5)\n",
    "\n",
    "    #GloVe embeddings\n",
    "    dictInit['EMBEDDING_DIM'] = 300\n",
    "    if dictInit['EMBEDDING_DIM'] == 100:\n",
    "        dictInit['FN_EMBEDDING'] = dictInit['DIR_PREFIX'] + 'Vectors/' + 'glove.6B.100d.txt'\n",
    "    elif dictInit['EMBEDDING_DIM'] == 300:\n",
    "        dictInit['FN_EMBEDDING'] = dictInit['DIR_PREFIX'] + 'Vectors/' + 'glove.6B.300d.txt'\n",
    "    \n",
    "    #Return\n",
    "    return dictInit\n",
    "\n",
    "#Initialize key variables\n",
    "dictInit = initialize()\n",
    "for key, val in dictInit.items():\n",
    "    print(key, '\\t', val)\n",
    "if (dictInit['MAX_SAMPLES'] == -1) or (dictInit['MAX_SAMPLES'] > 100):\n",
    "    DEBUG = False\n",
    "else:\n",
    "    DEBUG = True\n",
    "print('DEBUG', '\\t\\t', DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate features via underlying pre-trained net\n",
    "if dictInit['GEN_FEATURES']:\n",
    "    from pickle import dump\n",
    "    from keras.preprocessing.image import load_img\n",
    "    from keras.preprocessing.image import img_to_array\n",
    "    from keras.models import Model\n",
    "    from keras.utils import plot_model\n",
    "    from os import listdir\n",
    "    import time\n",
    "    \n",
    "    #Config\n",
    "    if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "        from keras.applications import InceptionV3\n",
    "        from keras.applications.inception_v3 import preprocess_input\n",
    "    else:        \n",
    "        from keras.applications.vgg16 import VGG16\n",
    "        from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "    #Extract features from each photo in the directory\n",
    "    def extract_features(directory):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #Load Model\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            model = InceptionV3(include_top=False, weights='imagenet')\n",
    "        else:\n",
    "            model = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "        #Remove the top layer, retaining the features generated up-to the layer below\n",
    "        #model.layers.pop()\n",
    "        model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "        #print(model.summary())\n",
    "        plot_model(model, to_file = dictInit['FN_GEN_MODEL'], show_shapes=True)\n",
    "\n",
    "        #Extract features from each photo\n",
    "        TARGET_SIZE = (0,0)\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            TARGET_SIZE = (299, 299)\n",
    "        else:\n",
    "            TARGET_SIZE=(224, 224)\n",
    "        counter = 0\n",
    "        features = dict()\n",
    "        start = time.time()\n",
    "        current = start\n",
    "        for fn in listdir(directory):\n",
    "            #Load image\n",
    "            image = load_img(directory+'/'+fn, target_size=TARGET_SIZE, interpolation='bicubic')\n",
    "\n",
    "            #Expand dims to include batch size\n",
    "            image = img_to_array(image)\n",
    "            image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "            #Prepare & predict\n",
    "            image = preprocess_input(image)\n",
    "            feature = model.predict(image, verbose=0)\n",
    "            features[fn] = feature\n",
    "            if counter % 1000 == 0:\n",
    "                if counter == 0:\n",
    "                    print(counter, 0, fn)\n",
    "                else:\n",
    "                    print(counter, 1000.0/(time.time()-current), (time.time()-current), time.time(), fn)\n",
    "                current = time.time()\n",
    "            counter += 1\n",
    "        return features\n",
    "\n",
    "    # Extract features from all images\n",
    "    features = extract_features(dictInit['DIR_ALL_IMAGES'])\n",
    "    print('# of Extracted Features: %d' % len(features), (current-start)/60.0)\n",
    "\n",
    "    # Save to file\n",
    "    dump(features, open(dictInit['FN_GEN_RESULT'], 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52242,
     "status": "ok",
     "timestamp": 1588081868543,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "SvDs-f-E7_Qx",
    "outputId": "8830b1fc-62fc-44f0-cbe3-eb0ef68b093b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 100 \n",
      "\n",
      "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .\\n', '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .\\n', '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .\\n', '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her playhouse .\\n', '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a wooden cabin .\\n', '1001773457_577c3a7d70.jpg#0\\tA black dog and a spotted dog are fighting\\n', '1001773457_577c3a7d70.jpg#1\\tA black dog and a tri-colored dog playing with each other on the road .\\n', '1001773457_577c3a7d70.jpg#2\\tA black dog and a white dog with brown spots are staring at each other in the street .\\n', '1001773457_577c3a7d70.jpg#3\\tTwo dogs of different breeds looking at each other on the road .\\n', '1001773457_577c3a7d70.jpg#4\\tTwo dogs on pavement moving toward each other .\\n']\n"
     ]
    }
   ],
   "source": [
    "#Load provided filename\n",
    "def read_file(filename, max_lines=-1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        if max_lines > 0:\n",
    "            doc = [next(f) for x in range(max_lines)]  #Read only the # of lines specified\n",
    "        else:\n",
    "            doc = f.read().splitlines()  #Read all lines\n",
    "        return doc\n",
    "\n",
    "#Load captions\n",
    "doc = read_file(dictInit['FN_ALL_LABELS'], dictInit['MAX_SAMPLES'])\n",
    "print(type(doc), len(doc), '\\n')\n",
    "if DEBUG:\n",
    "    print(doc[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52230,
     "status": "ok",
     "timestamp": 1588081868544,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "WUe0dQdu8qYd",
    "outputId": "d44d27c4-89b3-4c36-aeb5-6884b7974574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of samples: 20 \n",
      "\n",
      "1000268201.jpg ['A child in a pink dress is climbing up a set of stairs in an entry way .\\n', 'A girl going into a wooden building .\\n', 'A little girl climbing into a wooden playhouse .\\n', 'A little girl climbing the stairs to her playhouse .\\n', 'A little girl in a pink dress going into a wooden cabin .\\n']\n",
      "1001773457.jpg ['A black dog and a spotted dog are fighting\\n', 'A black dog and a tri-colored dog playing with each other on the road .\\n', 'A black dog and a white dog with brown spots are staring at each other in the street .\\n', 'Two dogs of different breeds looking at each other on the road .\\n', 'Two dogs on pavement moving toward each other .\\n']\n",
      "1002674143.jpg ['A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\\n', 'A little girl is sitting in front of a large painted rainbow .\\n', 'A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .\\n', 'There is a girl with pigtails sitting in front of a rainbow painting .\\n', 'Young girl with pigtails painting outside in the grass .\\n']\n",
      "1003163366.jpg ['A man lays on a bench while his dog sits by him .\\n', 'A man lays on the bench to which a white dog is also tied .\\n', 'a man sleeping on a bench outside with a white and black dog sitting next to him .\\n', 'A shirtless man lies on a park bench with his dog .\\n', 'man laying on bench holding leash of dog sitting on ground\\n']\n"
     ]
    }
   ],
   "source": [
    "#Extracts name and captions associated with each image\n",
    "def extract_captions(doc):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #If dataset Flickr_8K...\n",
    "    lines = []\n",
    "    if dictInit['DATASET'] == 'Flickr_8K':\n",
    "        #Split on tabs\n",
    "        for line in doc:\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            tokens = line.split('\\t')\n",
    "            #Skip '_577c3a7d70....#1' part of name  #1001773457_577c3a7d70.jpg#1\tTwo dogs are playing with each other .\n",
    "            photo_id = tokens[0].split('_')[0]+'.'+tokens[0].split('.')[1][0:-2]\n",
    "            photo_caption = tokens[1]  #Rest of words\n",
    "            lines.append((photo_id, photo_caption))\n",
    "    elif dictInit['DATASET'] == 'Flickr_30K':\n",
    "        #Split on pipes\n",
    "        first = True  #Skip header \"image_name| comment_number| comment\"\n",
    "        for line in doc:\n",
    "            if (first) or (line.strip() == \"\"):\n",
    "                first = False\n",
    "                continue\n",
    "            tokens = line.split('|')\n",
    "            photo_id = tokens[0]\n",
    "            photo_caption = tokens[2]\n",
    "            lines.append((photo_id, photo_caption))\n",
    "\n",
    "    #Concatenate descriptions by image\n",
    "    dictCaptions = {}  #Key = Photo identifier, Value = List of captions\n",
    "    for tokens in lines:\n",
    "        photo_id = tokens[0]\n",
    "        photo_caption = tokens[1]\n",
    "        if photo_id not in dictCaptions:\n",
    "            dictCaptions[photo_id] = []\n",
    "        dictCaptions[photo_id].extend([photo_caption])\n",
    "    return dictCaptions\n",
    "\n",
    "#Extract all words and captions\n",
    "dictCaptions = extract_captions(doc)\n",
    "print('# of samples:', len(dictCaptions), '\\n')\n",
    "if DEBUG:\n",
    "    counter = 0\n",
    "    for key,val in dictCaptions.items():\n",
    "        print(key,val)\n",
    "        if counter>2:\n",
    "            break;\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53043,
     "status": "ok",
     "timestamp": 1588081869368,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "nWkwIQrS9mKR",
    "outputId": "1f4a8819-0f8f-4281-bc26-7ec9d70ffb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201.jpg ['a child in a pink dress is climbing up a set of stairs in an entry way', 'a girl going into a wooden building', 'a little girl climbing into a wooden playhouse', 'a little girl climbing the stairs to her playhouse', 'a little girl in a pink dress going into a wooden cabin']\n",
      "1001773457.jpg ['a black dog and a spotted dog are fighting', 'a black dog and a tricolored dog playing with each other on the road', 'a black dog and a white dog with brown spots are staring at each other in the street', 'two dogs of different breeds looking at each other on the road', 'two dogs on pavement moving toward each other']\n",
      "1002674143.jpg ['a little girl covered in paint sits in front of a painted rainbow with her hands in a bowl', 'a little girl is sitting in front of a large painted rainbow', 'a small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it', 'there is a girl with pigtails sitting in front of a rainbow painting', 'young girl with pigtails painting outside in the grass']\n",
      "1003163366.jpg ['a man lays on a bench while his dog sits by him', 'a man lays on the bench to which a white dog is also tied', 'a man sleeping on a bench outside with a white and black dog sitting next to him', 'a shirtless man lies on a park bench with his dog', 'man laying on bench holding leash of dog sitting on ground']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#Cleanup text\n",
    "def cleanup_text(dictCaptions):\n",
    "    \"\"\"\n",
    "    Remove punctuation & special characters, lowercase captions, remove hanging 's, remove extra spaces.\n",
    "    Returns the sum of # of words across all captions. Includes duplicate counts.\n",
    "    \"\"\"\n",
    "    #Pre-process\n",
    "    for id, captions in dictCaptions.items():\n",
    "        clean = []\n",
    "        for caption in captions:\n",
    "            desc = caption.lower().replace(\" 's\", \"s\")\n",
    "            desc = re.sub('[^A-Za-z0-9 ]+', '', desc)  #Remove punc & special chars\n",
    "            desc \n",
    "            desc = desc.replace('   ', ' ').replace('  ', ' ').strip()\n",
    "            clean.append(desc)\n",
    "        dictCaptions[id] = clean\n",
    "\n",
    "#Cleanup text\n",
    "cleanup_text(dictCaptions)\n",
    "if DEBUG:\n",
    "    counter = 0\n",
    "    for key,val in dictCaptions.items():\n",
    "        print(key,val)\n",
    "        if counter>2:\n",
    "            break;\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53033,
     "status": "ok",
     "timestamp": 1588081869369,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0f9XTjDPAo8J",
    "outputId": "0283e774-2e57-4a69-90bf-694433331648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total words across all captions: 1167\n",
      "# of unique words: 274 \n",
      "\n",
      "its: 2\n",
      "from: 2\n",
      "being: 1\n",
      "chases: 1\n",
      "lush: 1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Aggregate words across all captions\n",
    "def agg_words(dictCaptions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    allWordsInCaptions = []\n",
    "    for id, captions in dictCaptions.items():\n",
    "        for caption in captions:\n",
    "            allWordsInCaptions.extend(caption.split(' '))\n",
    "    return allWordsInCaptions\n",
    "\n",
    "#Count frequency of all words\n",
    "def count_word_freq(dictCaptions):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Aggregate words across all captions\n",
    "    allWordsInCaptions = agg_words(dictCaptions)\n",
    "    numWords = len(allWordsInCaptions)\n",
    "    \n",
    "    #Count\n",
    "    dictWords = {}\n",
    "    for word in allWordsInCaptions:\n",
    "        if word not in dictWords:\n",
    "            dictWords[word] = 0\n",
    "        dictWords[word] += 1\n",
    "    return numWords, dictWords\n",
    "\n",
    "#Count frequency of all words\n",
    "numWords, dictWords = count_word_freq(dictCaptions)\n",
    "print('# of total words across all captions:', numWords)\n",
    "print('# of unique words:', len(dictWords), '\\n')\n",
    "if DEBUG:\n",
    "    for key in random.sample(list(dictWords.keys()), 5):\n",
    "        print(key+':', dictWords[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53328,
     "status": "ok",
     "timestamp": 1588081869678,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "0p16yVjgA44w",
    "outputId": "e47bc0e4-e266-4628-fd8a-507b42e75332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 20\n",
      "# of total words across all captions: 1167\n",
      "# of unique words: 274 \n",
      "\n",
      "101669240.jpg ['a man in a hat is displaying pictures next to a skier in a blue hat', 'a man skis past another man displaying paintings in the snow', 'a person wearing skis looking at framed pictures set up in the snow', 'a skier looks at framed pictures in the snow next to trees', 'man on skis looking at artwork for sale in the snow']\n",
      "1015118661.jpg ['a boy smiles in front of a stony wall in a city', 'a little boy is standing on the street while a man in overalls is working on a stone wall', 'a young boy runs aross the street', 'a young child is walking on a stone paved street with a metal pole and a man behind him', 'smiling boy in white shirt and blue jeans in front of rock wall with man in overalls behind him']\n",
      "1012212859.jpg ['a dog shakes its head near the shore a red ball next to it', 'a white dog shakes on the edge of a beach with an orange ball', 'dog with orange ball at feet stands on shore shaking off water', 'white dog playing with a red ball on the shore near the water', 'white dog with brown ears standing near water with head turned to one side']\n",
      "1007320043.jpg ['a child playing on a rope net', 'a little girl climbing on red roping', 'a little girl in pink climbs a rope bridge at the park', 'a small child grips onto the red ropes at the playground', 'the small child climbs on a red ropes on a playground']\n",
      "1001773457.jpg ['a black dog and a spotted dog are fighting', 'a black dog and a tricolored dog playing with each other on the road', 'a black dog and a white dog with brown spots are staring at each other in the street', 'two dogs of different breeds looking at each other on the road', 'two dogs on pavement moving toward each other']\n"
     ]
    }
   ],
   "source": [
    "#Drop infrequent words from captions, reducing vocabulary size\n",
    "def drop_infrequent(dictCaptions, dictWords):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Form dict of frequent words\n",
    "    truncated = []\n",
    "    for word in dictWords.keys():\n",
    "        if dictWords[word]/numWords > 0.00005:  #0.005%\n",
    "            truncated.append(word)\n",
    "    dictWordsTrunc = dict.fromkeys(truncated , 0)\n",
    "\n",
    "    #Drop infrequent\n",
    "    for id, captions in dictCaptions.items():\n",
    "        newCaptions = []\n",
    "        for cap in captions:\n",
    "            newCaptions.append(' '.join([word for word in cap.split(' ') if word in dictWordsTrunc]))\n",
    "        dictCaptions[id] = newCaptions\n",
    "\n",
    "#Drop infrequent words\n",
    "drop_infrequent(dictCaptions, dictWords)\n",
    "print('Total samples:', len(dictCaptions))\n",
    "print('# of total words across all captions:', len(agg_words(dictCaptions)))\n",
    "print('# of unique words:', len(set(agg_words(dictCaptions))), '\\n')\n",
    "if DEBUG:\n",
    "    for key in random.sample(list(dictCaptions.keys()), 5):\n",
    "        print(key, dictCaptions[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 sequence lengths: [20, 20, 19]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbYAAAE9CAYAAABnfkdrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAfEElEQVR4nO3df3zO9eL/8efFzE1+lWYZs8bZstlmM2uoDsKok6QZtfweTZ1SdHDTrXNOKQ79OFGUuiTtlm4o/SA/K4sQJ9dGkmMprpol2rH50YjN+/uHb9cnmbk2ruttL4/77dbt5npf7+v9eo6unr1/eL0clmVZAgDAEDXsDgAAwMVEsQEAjEKxAQCMQrEBAIxCsQEAjEKxAQCMUi2Kze122x0BAFBNVItiKy0ttTsCAKCaqBbFBgCAtyg2AIBRKDYAgFEoNgCAUSg2AIBRKDYAgFEoNgCAUSg2AIBRKDYAgFEoNgCAUSg2AIBRAuwOAFzqwicsO+8+7qm3+SEJAG9wxgYAMArFBgAwCsUGADAKxQYAMArFBgAwCsUGADAKxQYAMArFBgAwCsUGADAKxQYAMIrPii0jI0PBwcGKjY09Y/uMGTPUqlUrxcTEaPz48b4aHgBwmfJZsQ0dOlQrV648Y9unn36qxYsXa9u2bfr66681duxYXw0PALhM+azYOnXqpEaNGp2xbdasWZowYYJq164tSQoODvbV8ACAy5Rf77F98803Wrdundq3b6/OnTtr8+bN/hweAHAZ8GuxlZaWqqioSJs2bdKzzz6r/v37y7Kscvd1Op1KSkpSUlKSCgsL/RkTAFCN+bXYQkNDlZqaKofDoeTkZNWoUeOcpZWZmSmXyyWXy6WgoCB/xgQAVGN+LbY+ffooOztb0unLkidOnKC0AAAXlc9W0E5PT9eaNWtUWFio0NBQTZw4URkZGcrIyFBsbKwCAwOVlZUlh8PhqwgAgMuQz4pt/vz55W6fN2+er4YEAICZRwAAZqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARvFZsWVkZCg4OFixsbFnvffcc8/J4XCosLDQV8MDAC5TPiu2oUOHauXKlWdtz8/P18cff6ywsDBfDQ0AuIz5rNg6deqkRo0anbV9zJgxeuaZZ+RwOHw1NADgMubXe2xLlixRs2bNFB8f789hAQCXEb8VW0lJiSZPnqwnn3zSq/2dTqeSkpKUlJTEvTgAgNf8Vmzfffed9uzZo/j4eIWHh2vv3r1KTEzUTz/9VO7+mZmZcrlccrlcCgoK8ldMAEA1F+CvgeLi4nTgwAHP6/DwcEoLAHDR+eyMLT09XR07dlReXp5CQ0M1Z84cXw0FAICHz87Y5s+fX+H7brfbV0MDAC5jzDwCADAKxQYAMArFBgAwCsUGADAKxQYAMArFBgAwCsUGADAKxQYAMArFBgAwCsUGADAKxQYAMArFBgAwCsUGADAKxQYAMArFBgAwCsUGADAKxQYAMArFBgAwCsUGADAKxQYAMArFBgAwis+KLSMjQ8HBwYqNjfVsGzdunKKiotSmTRvdeeedKi4u9tXwAIDLlM+KbejQoVq5cuUZ21JSUrR9+3Zt27ZN1113naZMmeKr4QEAlymfFVunTp3UqFGjM7b16NFDAQEBkqQOHTpo7969vhoeAHCZsu0e2+uvv65bb73VruEBAIYKsGPQyZMnKyAgQAMGDDjnPk6nU06nU5I0c+ZMRURE+CveJSN8wrLz7uOeepsfkuBC8WcJ+I/fiy0rK0tLly7V6tWr5XA4zrlfZmamMjMzJUnffvutv+IBAKo5vxbbypUr9fTTT2vt2rW64oor/Dk0AOAy4bN7bOnp6erYsaPy8vIUGhqqOXPm6MEHH9SRI0eUkpKihIQE3Xfffb4aHgBwmfLZGdv8+fPP2jZ8+HBfDQcAgCRmHgEAGIZiAwAYhWIDABiFYgMAGIViAwAYhWIDABiFYgMAGIViAwAYhWIDABiFYgMAGIViAwAYhWIDABiFYgMAGIViAwAYhWIDABiFYgMAGIViAwAYhWIDABjFq2Lbvn27r3MAAHBReFVs9913n5KTk/Xyyy+ruLjY15kAAKgyr4pt/fr1euutt5Sfn6+kpCTdc889+vjjj32dDQCASvP6HltkZKQmTZqkp59+WmvXrtVDDz2kqKgovffee77MBwBApXhVbNu2bdOYMWMUHR2t7Oxsffjhh/rvf/+r7OxsjRkzptzPZGRkKDg4WLGxsZ5tBw8eVEpKiiIjI5WSkqKioqKL81MAAPD/eVVsDz74oBITE/Xll1/qpZdeUmJioiSpadOmmjRpUrmfGTp0qFauXHnGtqlTp6pbt27atWuXunXrpqlTp15gfAAAzuRVsS1fvlz33HOP6tSpI0k6deqUSkpKJEmDBg0q9zOdOnVSo0aNzti2ePFiDRkyRJI0ZMgQffDBB1UODgBAebwqtu7du+vYsWOe1yUlJerevXulB9u/f79CQkIkSSEhITpw4ECljwEAQEW8Krbjx4+rXr16ntf16tXznLH5itPpVFJSkpKSklRYWOjTsQAA5vCq2OrWravc3FzP65ycHM9lycq45pprtG/fPknSvn37FBwcfM59MzMz5XK55HK5FBQUVOmxAACXpwBvdpo+fbr69eunpk2bSjpdSgsXLqz0YL1791ZWVpYmTJigrKws3XHHHZU+BgAAFfGq2K6//nrt3LlTeXl5sixLUVFRqlWrVoWfSU9P15o1a1RYWKjQ0FBNnDhREyZMUP/+/TVnzhyFhYXpnXfeuSg/BAAAv/Gq2CRp8+bNcrvdKi0t1ZYtWyRJgwcPPuf+8+fPL3f76tWrKxkRAADveVVsgwYN0nfffaeEhATVrFlTkuRwOCosNgAA7OBVsblcLu3YsUMOh8PXeQAAuCBePRUZGxurn376yddZAAC4YF6dsRUWFqp169ZKTk5W7dq1PduXLFnis2AAAFSFV8X2xBNP+DgGAAAXh1fF1rlzZ33//ffatWuXunfvrpKSEpWVlfk6GwAAlebVPbbZs2crLS1NI0eOlCQVFBSoT58+Pg0GAEBVeFVsL730kjZs2KAGDRpIOr3oKBMYAwAuRV4VW+3atRUYGOh5XVpayqP/AIBLklfF1rlzZ/3rX//SsWPH9PHHH6tfv366/fbbfZ0NAIBK8+rhkalTp2rOnDmKi4vTq6++qr/85S8aMWKEr7MBqILwCcvOu4976m1+SALYw6tiq1Gjhu69917de++9vs4DAMAF8arYWrRoUe49td27d1/0QAAAXAiv54r8zfHjx/XOO+/o4MGDPgsFAEBVefXwyNVXX+35p1mzZho9erSys7N9nQ0AgErz6owtNzfX8+tTp07J5XLpyJEjPgsFAEBVeVVsf/vb3/7vAwEBCg8P19tvv+2zUAAAVJVXxfbpp5/6OgcAABeFV8X2/PPPV/j+I488clHCAABwobx+KnLz5s3q3bu3JOnDDz9Up06d1Lx5c5+GAwCgsrxeaDQ3N1f169eXdHp9tn79+um1117zaTgAACrLq8f9f/jhhzMmQQ4MDJTb7a7yoNOmTVNMTIxiY2OVnp6u48ePV/lYAAD8nldnbIMGDVJycrLuvPNOORwOvf/++xo8eHCVBiwoKNCLL76oHTt2qE6dOurfv78WLFigoUOHVul4AAD8nlfF9thjj+nWW2/VunXrJElz585V27ZtqzxoaWmpjh07plq1aqmkpERNmzat8rEAAPg9ry5FSlJJSYkaNGighx9+WKGhodqzZ0+VBmzWrJnGjh2rsLAwhYSEqGHDhurRo0eVjgUAwB95dcY2ceJEuVwu5eXladiwYTp58qQGDhyoDRs2VHrAoqIiLV68WHv27NGVV16pfv36ad68eRo4cOAZ+zmdTjmdTknSzJkzFRERUemx4L2LsdQJy6Vcfvgzx6XIqzO2999/X0uWLFHdunUlSU2bNq3ylFqffPKJWrRoocaNG6tWrVpKTU3V559/ftZ+mZmZcrlccrlcCgoKqtJYAIDLj1fFFhgYKIfD4Vm65pdffqnygGFhYdq0aZNKSkpkWZZWr16t6OjoKh8PAIDf86rY+vfvr5EjR6q4uFizZ89W9+7dq7zoaPv27ZWWlqbExETFxcXp1KlTyszMrNKxAAD4o/PeY7MsS3fddZd27typBg0aKC8vT08++aRSUlKqPOjEiRM1ceLEKn8eAIBzOW+xORwO9enTRzk5ORdUZgAA+INXlyI7dOigzZs3+zoLAAAXzOtla1555RWFh4erbt26sixLDodD27Zt83U+AAAqpcJi++GHHxQWFqYVK1b4Kw8AABekwmLr06ePcnNzde2116pv37569913/ZULAIAqqfAem2VZnl/v3r3b52EAALhQFRbbb38h+4+/BgDgUlXhpcgvv/xSDRo0kGVZOnbsmBo0aCBJnodHDh8+7JeQAAB4q8JiKysr81cOAAAuCq+XrQEAoDqg2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEaxpdiKi4uVlpamqKgoRUdHa+PGjXbEAAAYqMJla3zl4Ycf1i233KJFixbpxIkTKikpsSMGAMBAfi+2w4cP67PPPtMbb7whSQoMDFRgYKC/YwAADOX3S5G7d+9W48aNNWzYMLVt21YjRozQL7/8ctZ+TqdTSUlJSkpKUmFhob9jAgCqKb8XW2lpqXJzc3X//fdry5Ytqlu3rqZOnXrWfpmZmXK5XHK5XAoKCvJ3TABANeX3YgsNDVVoaKjat28vSUpLS1Nubq6/YwAADOX3YmvSpImaN2+uvLw8SdLq1avVunVrf8cAABjKlqciZ8yYoQEDBujEiRNq2bKl5s6da0cMAICBbCm2hIQEuVwuO4YGABiOmUcAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEaxZUotAMClJXzCsvPu4556mx+SXDjO2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARrGt2MrKytS2bVv16tXLrggAAAPZVmwvvPCCoqOj7RoeAGAoW4pt7969WrZsmUaMGGHH8AAAg9lSbKNHj9YzzzyjGjW4xQcAuLj8vmzN0qVLFRwcrHbt2mnNmjXn3M/pdMrpdEqSZs6cqYiICD8lRHVg0hIb8B/+vbk8+P2UacOGDVqyZInCw8N19913Kzs7WwMHDjxrv8zMTLlcLrlcLgUFBfk7JgCgmvJ7sU2ZMkV79+6V2+3WggUL1LVrV82bN8/fMQAAhuImFwDAKH6/x/Z7Xbp0UZcuXeyMAAAwDGdsAACjUGwAAKNQbAAAo1BsAACjUGwAAKNQbAAAo1BsAACjUGwAAKNQbAAAo1BsAACj2DqlFgBI519OpjouJcMSOfbhjA0AYBSKDQBgFIoNAGAUig0AYBSKDQBgFIoNAGAUig0AYBSKDQBgFIoNAGAUig0AYBSKDQBgFL8XW35+vm6++WZFR0crJiZGL7zwgr8jAAAM5vdJkAMCAvTvf/9biYmJOnLkiNq1a6eUlBS1bt3a31EAAAby+xlbSEiIEhMTJUn169dXdHS0CgoK/B0DAGAoW5etcbvd2rJli9q3b3/We06nU06nU5I0c+ZMRUREXPB4LCMBoLqpbkv6XAr/nbXt4ZGjR4+qb9++mj59uho0aHDW+5mZmXK5XHK5XAoKCrIhIQCgOrKl2E6ePKm+fftqwIABSk1NtSMCAMBQfi82y7I0fPhwRUdH65FHHvH38AAAw/m92DZs2KA333xT2dnZSkhIUEJCgpYvX+7vGAAAQ/n94ZGbbrpJlmX5e1gAwGWCmUcAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEah2AAARqHYAABGodgAAEaxdT226uhSWGsIAHBunLEBAIxCsQEAjEKxAQCMQrEBAIxCsQEAjEKxAQCMQrEBAIxCsQEAjEKxAQCMQrEBAIxiS7GtXLlSrVq1UkREhKZOnWpHBACAofxebGVlZXrggQe0YsUK7dixQ/Pnz9eOHTv8HQMAYCi/F9sXX3yhiIgItWzZUoGBgbr77ru1ePFif8cAABjK78VWUFCg5s2be16HhoaqoKDA3zEAAIZyWJZl+XPAd955R6tWrdJrr70mSXrzzTf1xRdfaMaMGWfs53Q65XQ6JUmzZ89W/fr1/Rmz0goLCxUUFGR3jCqrzvnJbp/qnJ/s9rlY+QMCAhQeHn7Wdr8X28aNG/XEE09o1apVkqQpU6ZIkh599FF/xrjokpKS5HK57I5RZdU5P9ntU53zk90+vs7v90uR119/vXbt2qU9e/boxIkTWrBggXr37u3vGAAAQ/l9Be2AgADNnDlTPXv2VFlZmTIyMhQTE+PvGAAAQ9V84oknnvD3oJGRkRo1apQefvhhderUyd/D+0y7du3sjnBBqnN+stunOucnu318md/v99gAAPAlptQCABiFYrsIiouLlZaWpqioKEVHR2vjxo12R/LatGnTFBMTo9jYWKWnp+v48eN2R6pQRkaGgoODFRsb69l28OBBpaSkKDIyUikpKSoqKrIx4bmVl33cuHGKiopSmzZtdOedd6q4uNjGhBUrL/9vnnvuOTkcDhUWFtqQ7PzOlX3GjBlq1aqVYmJiNH78eJvSVay87Fu3blWHDh2UkJCgpKQkffHFFzYmPLf8/HzdfPPNio6OVkxMjF544QVJfvjOWrhggwcPtmbPnm1ZlmX9+uuvVlFRkc2JvLN3714rPDzcKikpsSzLsvr162fNnTvX3lDnsXbtWisnJ8eKiYnxbBs3bpw1ZcoUy7Isa8qUKdb48ePtileh8rKvWrXKOnnypGVZljV+/PhLNrtllZ/fsizrhx9+sHr06GGFhYVZP//8s03pKlZe9uzsbKtbt27W8ePHLcuyrP3799sVr0LlZU9JSbGWL19uWZZlLVu2zOrcubNN6Sr2448/Wjk5OZZlWdbhw4etyMhI6+uvv/b5d5Yztgt0+PBhffbZZxo+fLgkKTAwUFdeeaXNqbxXWlqqY8eOqbS0VCUlJWratKndkSrUqVMnNWrU6Ixtixcv1pAhQyRJQ4YM0QcffGBHtPMqL3uPHj0UEHD64eQOHTpo7969dkTzSnn5JWnMmDF65pln5HA4bEjlnfKyz5o1SxMmTFDt2rUlScHBwXZEO6/ysjscDh0+fFiSdOjQoUv2exsSEqLExERJUv369RUdHa2CggKff2cptgu0e/duNW7cWMOGDVPbtm01YsQI/fLLL3bH8kqzZs00duxYhYWFKSQkRA0bNlSPHj3sjlVp+/fvV0hIiKTTX6QDBw7YnKhqXn/9dd166612x6iUJUuWqFmzZoqPj7c7SqV98803Wrdundq3b6/OnTtr8+bNdkfy2vTp0zVu3Dg1b95cY8eO9Ux0cSlzu93asmWL2rdv7/PvLMV2gUpLS5Wbm6v7779fW7ZsUd26davNUjxFRUVavHix9uzZox9//FG//PKL5s2bZ3esy9LkyZMVEBCgAQMG2B3FayUlJZo8ebKefPJJu6NUSWlpqYqKirRp0yY9++yz6t+/v6xq8pD4rFmzNG3aNOXn52vatGmeK0aXqqNHj6pv376aPn26GjRo4PPxKLYLFBoaqtDQULVv316SlJaWptzcXJtTeeeTTz5RixYt1LhxY9WqVUupqan6/PPP7Y5Vaddcc4327dsnSdq3b98le0npXLKysrR06VK99dZbl/TlvD/67rvvtGfPHsXHxys8PFx79+5VYmKifvrpJ7ujeSU0NFSpqalyOBxKTk5WjRo1LtmHX/4oKytLqampkqR+/fpdsg+PSNLJkyfVt29fDRgwwJPZ199Ziu0CNWnSRM2bN1deXp4kafXq1WrdurXNqbwTFhamTZs2qaSkRJZlafXq1YqOjrY7VqX17t1bWVlZkk5/4e+44w6bE3lv5cqVevrpp7VkyRJdccUVdseplLi4OB04cEBut1tut1uhoaHKzc1VkyZN7I7mlT59+ig7O1vS6cuSJ06cqDYTCzdt2lRr166VJGVnZysyMtLmROWzLEvDhw9XdHS0HnnkEc92n39nL+qjKJepLVu2WO3atbPi4uKsO+64wzp48KDdkbz2z3/+02rVqpUVExNjDRw40POE2KXq7rvvtpo0aWIFBARYzZo1s1577TWrsLDQ6tq1qxUREWF17drV+t///md3zHKVl/1Pf/qTFRoaasXHx1vx8fHWyJEj7Y55TuXl/71rr732kn0qsrzsv/76qzVgwAArJibGatu2rbV69Wq7Y5arvOzr1q2zEhMTrTZt2ljJycmWy+WyO2a51q1bZ0my4uLiPP+OL1u2zOffWWYeAQAYhUuRAACjUGwAAKNQbAAAo1BsAACjUGwAAKNQbDDS5MmTFRMTozZt2ighIUH/+c9/7I50QYYOHapFixb57Phr1qw54y/neztezZo1lZCQoPj4eCUmJnqO8eOPPyotLc1neYGKBNgdALjYNm7cqKVLlyo3N1e1a9dWYWGhTpw4YXesS9qaNWtUr1493XDDDZX6XJ06dbR161ZJ0qpVq/Too49q7dq1atq0qU+LGKgIZ2wwzr59+xQUFOSZtT0oKMgz+3lOTo46d+6sdu3aqWfPnp5pfXJychQfH6+OHTtq3LhxnrWv3njjDT344IOeY/fq1Utr1qyRJH300Ufq2LGjEhMT1a9fPx09elSSFB4erscff1yJiYmKi4vTzp07JZ2eL2/YsGGKi4tTmzZt9O6771Z4nPMpKyvTuHHjdP3116tNmzZ69dVXJZ0uqS5dunjWCBwwYIBnDsTly5crKipKN910kx566CH16tVLbrdbr7zyiqZNm6aEhAStW7dOkvTZZ5/phhtuUMuWLb0qqcOHD+uqq66SdHrC29//HqampuqWW25RZGSkZ92zsrIyDR06VLGxsYqLi9O0adO8+rmB86HYYJwePXooPz9f1113nf761796ph46efKkRo0apUWLFiknJ0cZGRl67LHHJEnDhg3Tiy++6PUisYWFhZo0aZI++eQT5ebmKikpSc8//7zn/aCgIM/k2M8995wk6amnnlLDhg311Vdfadu2beratet5j1OROXPmqGHDhtq8ebM2b96s2bNna8+ePZKkLVu2aPr06dqxY4d2796tDRs26Pjx4xo5cqRWrFih9evX6+eff5Z0uojvu+8+jRkzRlu3btWf//xnSaf/B2H9+vVaunSpJkyYUG6GY8eOKSEhQVFRURoxYoT+8Y9/lLvf1q1btXDhQn311VdauHCh8vPztXXrVhUUFGj79u366quvNGzYMK9+buB8uBQJ49SrV085OTlat26dPv30U911112aOnWqkpKStH37dqWkpEg6fcYQEhKiQ4cOqbi4WJ07d5YkDRo0SCtWrKhwjE2bNmnHjh268cYbJUknTpxQx44dPe//Ntlru3bt9N5770k6Pen0ggULPPtcddVVWrp0aYXHqchHH32kbdu2ec6mDh06pF27dikwMFDJyckKDQ2VJCUkJMjtdqtevXpq2bKlWrRoIUlKT0+X0+k85/H79OmjGjVqqHXr1tq/f3+5+/z+UuTGjRs1ePBgbd++/az9unXrpoYNG0qSWrdure+//14xMTHavXu3Ro0apdtuu61aLpmESxPFBiPVrFlTXbp0UZcuXRQXF6esrCy1a9dOMTExZ52VFRcXn3NW/YCAAJ06dcrz+vjx45JOT+6akpKi+fPnl/u53y6D1qxZU6WlpZ7P/HGc8x2nIpZlacaMGerZs+cZ29esWeMZ//cZKjt73u+P4c1nO3bsqMLCQs+Z4LmO9Vueq666Sl9++aVWrVqll156SW+//bZef/31SmUEysOlSBgnLy9Pu3bt8rzeunWrrr32WrVq1Uo///yzp9hOnjypr7/+WldeeaUaNmyo9evXS5Leeustz2fDw8O1detWnTp1Svn5+Z7lQTp06KANGzbo22+/lXR6bbJvvvmmwlw9evTQzJkzPa+LioqqdJzf9OzZU7NmzdLJkyclnZ6hvqJFbqOiorR792653W5J0sKFCz3v1a9fX0eOHPFq3HPZuXOnysrKdPXVV3u1f2FhoU6dOqW+ffvqqaeeqjbLPeHSxxkbjHP06FGNGjVKxcXFCggIUEREhJxOpwIDA7Vo0SI99NBDOnTokEpLSzV69GjFxMRo7ty5ysjI0BVXXHHGGdCNN96oFi1aKC4uTrGxsZ5l7hs3bqw33nhD6enp+vXXXyVJkyZN0nXXXXfOXH//+9/1wAMPKDY2VjVr1tTjjz+u1NRUr48zcuRIjR49WpLUvHlzbdiwQW63W4mJibIsS40bN9YHH3xwzvHr1Kmjl19+WbfccouCgoKUnJzsee/2229XWlqaFi9erBkzZnj9e/3bPTbp9FldVlaWatas6dVnCwoKNGzYMM8ZcXVYBRrVA7P7A3/gdrvVq1evcu8VVXdHjx5VvXr1ZFmWHnjgAUVGRmrMmDF2xwIuKi5FApeR2bNnKyEhQTExMTp06JBGjhxpdyTgouOMDQBgFM7YAABGodgAAEah2AAARqHYAABGodgAAEah2AAARvl/zpZsSO2sKTcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skew: 0.439\n",
      "99th Percentile: 20\n",
      "1000268201.jpg\n",
      "\t a child in a pink dress is climbing up a set of stairs in an entry way\n",
      "\t a girl going into a wooden building\n",
      "\t a little girl climbing into a wooden playhouse\n",
      "\t a little girl climbing the stairs to her playhouse\n",
      "\t a little girl in a pink dress going into a wooden cabin\n",
      "1001773457.jpg\n",
      "\t a black dog and a spotted dog are fighting\n",
      "\t a black dog and a tricolored dog playing with each other on the road\n",
      "\t a black dog and a white dog with brown spots are staring at each other in the street\n",
      "\t two dogs of different breeds looking at each other on the road\n",
      "\t two dogs on pavement moving toward each other\n",
      "1002674143.jpg\n",
      "\t a little girl covered in paint sits in front of a painted rainbow with her hands in a bowl\n",
      "\t a little girl is sitting in front of a large painted rainbow\n",
      "\t a small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it\n",
      "\t there is a girl with pigtails sitting in front of a rainbow painting\n",
      "\t young girl with pigtails painting outside in the grass\n",
      "1003163366.jpg\n",
      "\t a man lays on a bench while his dog sits by him\n",
      "\t a man lays on the bench to which a white dog is also tied\n",
      "\t a man sleeping on a bench outside with a white and black dog sitting next to him\n",
      "\t a shirtless man lies on a park bench with his dog\n",
      "\t man laying on bench holding leash of dog sitting on ground\n",
      "1007129816.jpg\n",
      "\t a man in an orange hat starring at something\n",
      "\t a man wears an orange hat and glasses\n",
      "\t a man with gauges and glasses is wearing a blitz hat\n",
      "\t a man with glasses is wearing a beer can crocheted hat\n",
      "\t the man with pierced ears is wearing glasses and an orange hat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "\n",
    "#4 May: Limit caption length to 95% percentile\n",
    "seq_lengths = [len(val.split()) for values in dictCaptions.values() for val in values]\n",
    "seq_lengths.sort(reverse=True)\n",
    "print('Top 3 sequence lengths:', seq_lengths[0:3])\n",
    "\n",
    "#Histogram\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "fig=plt.figure(figsize=(7, 5))\n",
    "numBins = 40\n",
    "plt.hist(seq_lengths, bins=numBins)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Sequence Length Bins', color='black')\n",
    "ax.set_ylabel('Frequency', color='black')\n",
    "ax.spines['bottom'].set_color('lightgrey')\n",
    "ax.spines['left'].set_color('lightgrey')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.show()\n",
    "print('Skew:', round(skew(seq_lengths),3))\n",
    "\n",
    "#Get the 99'th percentile\n",
    "p99 = int(np.percentile(seq_lengths, 99))  #N'th percentile\n",
    "print('99th Percentile:', p99)\n",
    "\n",
    "#Trunc captions to p99\n",
    "for id, captions in dictCaptions.items():\n",
    "    newCaptions = []\n",
    "    for cap in captions:\n",
    "        newCaptions.append(' '.join(cap.split(' ')[0:p99]))\n",
    "    dictCaptions[id] = newCaptions\n",
    "counter = 0\n",
    "if DEBUG:\n",
    "    for id, captions in dictCaptions.items():\n",
    "        print(id)\n",
    "        for cap in captions:\n",
    "            print('\\t', cap)\n",
    "        if counter > 3:\n",
    "            break;\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54232,
     "status": "ok",
     "timestamp": 1588081870593,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "M3_M8jPqCRWX",
    "outputId": "80b91ba9-9850-479e-e53d-c2045bb846dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 : 14 4 2\n",
      "20 : 14 4 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Create train, dev & test datasets\n",
    "def split_dataset(dictCaptions):\n",
    "    #Split\n",
    "    X = list(dictCaptions.keys())\n",
    "    y = list(dictCaptions.values())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)  #Train+Dev vs. Test\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.10, random_state=42)  #Train vs. Dev\n",
    "    return X_train, X_dev, X_test, y_train, y_dev, y_test\n",
    "\n",
    "#Split\n",
    "X_train, X_dev, X_test, y_train, y_dev, y_test = split_dataset(dictCaptions)\n",
    "print(len(X_train)+len(X_test)+len(X_dev), ':', len(X_train), len(X_test), len(X_dev))\n",
    "print(len(y_train)+len(y_test)+len(y_dev), ':', len(y_train), len(y_test), len(y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence length: 22\n",
      "Vocabulary size: 210 \n",
      "\n",
      "70 \t 10 \t 20\n",
      "3 \t 3 \t 3\n",
      "(867, 2048) \t (867, 22) \t (867, 210)\n",
      "(102, 2048) \t (102, 22) \t (102, 210)\n",
      "(196, 2048) \t (196, 22) \t (196, 210)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Flatten sets and add startseq endseq bookends\n",
    "def flatten_add_bookends(X, y):\n",
    "    \"\"\"\n",
    "        X is list of photo ID's.\n",
    "        y is a list of captions (usually 5) for each photo.\n",
    "    \"\"\"\n",
    "    Xy = []\n",
    "    idx = 0\n",
    "    for captions in y:\n",
    "        Xy.extend([[X[idx], 'startseq ' + cap + ' endseq'] for cap in captions])\n",
    "        idx += 1\n",
    "    return Xy\n",
    "\n",
    "#Create inputs images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_seq_len, Xy, features, vocab_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Foreach sample...\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for key, value in Xy:\n",
    "        #Encode: text to integers\n",
    "        seq = tokenizer.texts_to_sequences([value])[0]\n",
    "        \n",
    "        #Split sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            #Split\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            \n",
    "            #Pad input sequences\n",
    "            in_seq = pad_sequences([in_seq], maxlen = max_seq_len)[0]\n",
    "            \n",
    "            #Encode output sequence: \n",
    "            out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n",
    "            \n",
    "            #Store\n",
    "            X1.append(features[key][0])\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return (array(X1), array(X2), array(y))\n",
    "\n",
    "#Convert the dataset into the right format for our custom net + LSTM\n",
    "def process_dataset(X_train, X_dev, X_test, y_train, y_dev, y_test):\n",
    "    #Flatten sets ++ startseq endseq bookends\n",
    "    Xy_train = flatten_add_bookends(X_train, y_train)\n",
    "    Xy_dev = flatten_add_bookends(X_dev, y_dev)\n",
    "    Xy_test = flatten_add_bookends(X_test, y_test)\n",
    "    \n",
    "    #Compute max length of tokens in train set\n",
    "    max_seq_len = max([len(token.split(' ')) for val in Xy_train for token in val])\n",
    "    \n",
    "    #Create tokenizer and fit on train set\n",
    "    only_captions = [val[1] for val in Xy_train]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(only_captions)\n",
    "    \n",
    "    #Load transfer learning features from pickle file\n",
    "    features_all = load(open(dictInit['FN_GEN_RESULT'], 'rb'))\n",
    "    features_train = {k: features_all[k] for k in X_train}\n",
    "    features_dev = {k: features_all[k] for k in X_dev}\n",
    "    features_test = {k: features_all[k] for k in X_test}\n",
    "    \n",
    "    #Create training sequences\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    Xy_train_seq = create_sequences(tokenizer, max_seq_len, Xy_train, features_train, vocab_size)  #X1train, X2train, ytrain\n",
    "    \n",
    "    #Create dev sequences\n",
    "    Xy_dev_seq = create_sequences(tokenizer, max_seq_len, Xy_dev, features_dev, vocab_size)  #X1dev, X2dev, ydev\n",
    "    \n",
    "    #Create test sequences\n",
    "    Xy_test_seq = create_sequences(tokenizer, max_seq_len, Xy_test, features_test, vocab_size)  #X1test, X2test, ytest\n",
    "    \n",
    "    #Add features into Xy_train, Xy_dev & Xy_test for use in testing later (only need test but doing the rest for debugging consistency)\n",
    "    Xy_train = [(features_train[Xy_train[idx][0]], Xy_train[idx][0], Xy_train[idx][1]) for idx,val in enumerate(Xy_train)]\n",
    "    Xy_dev = [(features_dev[Xy_dev[idx][0]], Xy_dev[idx][0], Xy_dev[idx][1]) for idx,val in enumerate(Xy_dev)]\n",
    "    Xy_test = [(features_test[Xy_test[idx][0]], Xy_test[idx][0], Xy_test[idx][1]) for idx,val in enumerate(Xy_test)]\n",
    "    \n",
    "    #Return\n",
    "    return tokenizer, max_seq_len, vocab_size, \\\n",
    "            Xy_train, Xy_dev, Xy_test, \\\n",
    "            Xy_train_seq, Xy_dev_seq, Xy_test_seq\n",
    "\n",
    "#Process dataset\n",
    "tokenizer, max_seq_len, vocab_size, Xy_train, Xy_dev, Xy_test, Xy_train_seq, Xy_dev_seq, Xy_test_seq = \\\n",
    "                                                    process_dataset(X_train, X_dev, X_test, y_train, y_dev, y_test)\n",
    "print('Longest sequence length:', max_seq_len)\n",
    "print('Vocabulary size:', vocab_size, '\\n')\n",
    "print(len(Xy_train), '\\t', len(Xy_dev), '\\t', len(Xy_test))\n",
    "print(len(Xy_train_seq), '\\t', len(Xy_dev_seq), '\\t', len(Xy_test_seq))\n",
    "print(Xy_train_seq[0].shape, '\\t', Xy_train_seq[1].shape, '\\t', Xy_train_seq[2].shape)\n",
    "print(Xy_dev_seq[0].shape, '\\t', Xy_dev_seq[1].shape, '\\t', Xy_dev_seq[2].shape)\n",
    "print(Xy_test_seq[0].shape, '\\t', Xy_test_seq[1].shape, '\\t', Xy_test_seq[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ug3D8Im8HQqZ"
   },
   "source": [
    "## Create & Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30820,
     "status": "ok",
     "timestamp": 1588082067473,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hRp9_ly-8urm",
    "outputId": "7ee7d535-d2f6-46cf-d321-022145a64fd2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Create a matrix of GloVe word vectors of all words in our vocabulary\n",
    "def createEmbeddingMatrix(dict_word_index, vocab_size):\n",
    "    #Load GloVe word vectors\n",
    "    start = time.time()\n",
    "    dict_glove = {}\n",
    "    lines = read_file(dictInit['FN_EMBEDDING'])\n",
    "    with open(dictInit['FN_EMBEDDING']) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            dict_glove[word] = coefs\n",
    "    print('# of vectors loaded:', len(dict_glove))\n",
    "    print('Time taken:', time.time()-start)\n",
    "\n",
    "    #Create embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, dictInit['EMBEDDING_DIM']))\n",
    "    for word, i in dict_word_index.items():\n",
    "        embedding_vector = dict_glove.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  #Words not found in GloVe will be all-zeros\n",
    "    return embedding_matrix\n",
    "\n",
    "#Create a matrix of GloVe word vectors of all words in our vocabulary\n",
    "print('Loading vectors...')\n",
    "if dictInit['TRAIN_MODEL']:\n",
    "    embedding_matrix = createEmbeddingMatrix(tokenizer.word_index, vocab_size)\n",
    "    print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63520,
     "status": "ok",
     "timestamp": 1588082100188,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "T_EtOFVe2egn",
    "outputId": "296df2c5-928d-48b3-f17b-681bf377556a"
   },
   "outputs": [],
   "source": [
    "if dictInit['TRAIN_MODEL']:\n",
    "    from keras.applications import densenet\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Embedding\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers.merge import add\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.utils import plot_model\n",
    "    \n",
    "    #Define the captioning model\n",
    "    def define_model(vocab_size, max_seq_len):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # feature extractor model\n",
    "        if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "            inputs1 = Input(shape=(2048,))\n",
    "        else:\n",
    "            inputs1 = Input(shape=(4096,))\n",
    "        fe1 = inputs1  #Dropout(0.2)(inputs1)\n",
    "        fe2 = Dense(400, activation='relu')(fe1)\n",
    "        \n",
    "        # sequence model\n",
    "        inputs2 = Input(shape=(max_seq_len,))\n",
    "        # se1 = Embedding(vocab_size, 300, mask_zero=True)(inputs2)\n",
    "        se1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_len, trainable=False)(inputs2)\n",
    "        se2 = se1  #Dropout(0.2)(se1)\n",
    "        se3 = LSTM(400)(se2)\n",
    "        \n",
    "#         # decoder model\n",
    "#         decoder1 = add([fe2, se3])\n",
    "#         decoder2 = Dense(500, activation='relu')(decoder1)\n",
    "#         outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "        \n",
    "#         # tie it together [image, seq] [word]\n",
    "#         model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "        \n",
    "        # decoder model\n",
    "        decoder1 = add([fe2, se3])\n",
    "        decoder2 = Dense(500, activation='relu')(decoder1)\n",
    "        outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "        \n",
    "        # tie it together [image, seq] [word]\n",
    "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "        \n",
    "        # summarize model\n",
    "        print(model.summary())\n",
    "        plot_model(model, to_file=dictInit['FN_TRAIN_MODEL'], show_shapes=True)\n",
    "        return model\n",
    "\n",
    "    # fit model\n",
    "\n",
    "    # define the model & checkpoint callback\n",
    "    start = time.time()\n",
    "    print('Start time:', start)\n",
    "    model = define_model(vocab_size, max_seq_len)\n",
    "    checkpoint = ModelCheckpoint(dictInit['FN_WEIGHTS'], monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit([Xy_train_seq[0], Xy_train_seq[1]], Xy_train_seq[2], epochs=3, \n",
    "              verbose=2, callbacks=[checkpoint], validation_data=([Xy_dev_seq[0], Xy_dev_seq[1]], Xy_dev_seq[2]))\n",
    "    print('Total time taken:', time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KJvdPpDD19E"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "#Init plot\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "fig=plt.figure(figsize=(22, 24))\n",
    "\n",
    "#Plot learning curves\n",
    "def plotCurve(ax, metric1, metric2, title, title_x, title_y):\n",
    "    ax.plot(history.history[metric1], '-*', label=metric1)\n",
    "    ax.plot(history.history[metric2], '--*', label=metric2)\n",
    "    ax.set_title('Training & Validation Loss', color='black')\n",
    "    ax.legend(loc='best', shadow=True)\n",
    "    ax.set_xlabel(title_x, color='black')\n",
    "    ax.set_ylabel(title_y, color='black')\n",
    "    ax.spines['bottom'].set_color('lightgrey')\n",
    "    ax.spines['left'].set_color('lightgrey')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', colors='black')\n",
    "    ax.tick_params(axis='y', colors='black')\n",
    "\n",
    "#Plot learning curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "plotCurve(ax1, 'loss', 'val_loss', 'Training & Valiation Loss', '# of Epochs', 'Loss')\n",
    "plotCurve(ax2, 'accuracy', 'val_accuracy', 'Training & Valiation Accuracy', '# of Epochs', 'Accuracy')\n",
    "plt.savefig(dictInit['FN_LEARN_CURVES'] + '_' + datetime.today().strftime('%Y-%m-%d-%H:%M:%S') + '.png',\n",
    "                                                                        bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67278,
     "status": "ok",
     "timestamp": 1588082103956,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "hP4meSU1vNQc",
    "outputId": "a0950deb-daaa-4187-c52c-d5354af6bcba"
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo_features, max_seq_len):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_seq_len):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen = max_seq_len)\n",
    "        \n",
    "        # predict next word\n",
    "        yhat = model.predict([photo_features,sequence], verbose = 0)\n",
    "        \n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        \n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        \n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        \n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "#Stores an instance of a test case\n",
    "class XyTest:\n",
    "    def __init__(self, photo_id, features):\n",
    "        self.photo_id = photo_id\n",
    "        self.features = features\n",
    "        self.descriptions = list()\n",
    "        self.prediction = []\n",
    "    \n",
    "    def addPrediction(self, prediction):\n",
    "        self.prediction = prediction.replace('startseq ','').replace(' endseq','').split(' ')\n",
    "    \n",
    "    def addDescription(self, description):\n",
    "        self.descriptions.append(description.replace('startseq ','').replace(' endseq','').split(' '))\n",
    "        \n",
    "    def toString(self):\n",
    "        res = self.photo_id + '\\n'\n",
    "        for desc in self.descriptions:\n",
    "            res += '\\t' + desc + '\\n'\n",
    "        if len(self.prediction):\n",
    "            res += '\\t' + self.prediction + '\\n'\n",
    "        return res\n",
    "\n",
    "#Evaluate model\n",
    "def evaluate_model(model, Xy_test, tokenizer, max_seq_len):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #Convert Xy_test to dict: Key = photo_id, Value = List of descriptions\n",
    "    dictXy_test = {}\n",
    "    for photo_features, photo_id, photo_description in Xy_test:\n",
    "        if photo_id not in dictXy_test:\n",
    "            dictXy_test[photo_id] = XyTest(photo_id, photo_features)\n",
    "        dictXy_test[photo_id].addDescription(photo_description)\n",
    "    \n",
    "    #Predict caption foreach test case...\n",
    "    for photo_id, xyTest in dictXy_test.items():\n",
    "        photo_features = xyTest.features\n",
    "        photo_descriptions = xyTest.descriptions  #Actual\n",
    "        yhat = generate_desc(model, tokenizer, photo_features, max_seq_len)  #Predicted description\n",
    "        xyTest.addPrediction(yhat)\n",
    "    \n",
    "    #Format input for BLEU\n",
    "    candidate, references = list(), list()\n",
    "    for photo_id, xyTest in dictXy_test.items():\n",
    "        references.append(xyTest.descriptions)\n",
    "        candidate.append(xyTest.prediction)\n",
    "    \n",
    "    #Compute BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(references, candidate, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(references, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(references, candidate, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "    #Return\n",
    "    return dictXy_test\n",
    "\n",
    "# load the model\n",
    "model_custom_from_weights = load_model(dictInit['FN_WEIGHTS'])\n",
    "# print(model_custom_from_weights.summary())\n",
    "plot_model(model_custom_from_weights, to_file = dictInit['FN_TRAIN_REMODEL'], show_shapes=True)\n",
    "\n",
    "# evaluate model\n",
    "print('Evaluating model...')\n",
    "start = time.time()\n",
    "dictXy_test = evaluate_model(model_custom_from_weights, Xy_test, tokenizer, max_seq_len)\n",
    "print('Time taken:', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 105216,
     "status": "ok",
     "timestamp": 1588082141905,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "xW19WkiyvOOs",
    "outputId": "b30bd86f-5050-4005-aba4-2a8b2abb2406"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "\n",
    "#Config model imports\n",
    "if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "    from keras.applications.inception_v3 import InceptionV3\n",
    "    from keras.applications.inception_v3 import preprocess_input\n",
    "else:\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "#Load Model\n",
    "if dictInit['TL_MODEL'] == 'Inceptionv3':\n",
    "    model_TL = InceptionV3(include_top=True, weights='imagenet')\n",
    "else:\n",
    "    model_TL = VGG16(include_top=True, weights='imagenet')\n",
    "\n",
    "#Restructure the model\n",
    "model_TL.layers.pop()\n",
    "model_TL = Model(inputs = model_TL.inputs, outputs = model_TL.layers[-1].output)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1LmTvQ8D4ONvEnj3npjPvpRvgQTOLdUGi"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21650,
     "status": "ok",
     "timestamp": 1588084779903,
     "user": {
      "displayName": "Anand Menon",
      "photoUrl": "",
      "userId": "01091567006340645327"
     },
     "user_tz": 240
    },
    "id": "DJprBlgr--wM",
    "outputId": "e6f11dd4-9cd6-4a34-9dae-b78e47b51de3"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from textwrap import wrap\n",
    "\n",
    "#Plot sample test images\n",
    "def plotSamples(dictXy_test):\n",
    "    #Get random images\n",
    "    testIDs = random.sample(dictXy_test.keys(), 16)\n",
    "    print(testIDs)\n",
    "    \n",
    "    #Init plot\n",
    "    plt.rcParams['figure.facecolor'] = 'black'\n",
    "    fig=plt.figure(figsize=(22, 24))\n",
    "    fig.patch.set_facecolor('black')\n",
    "    columns = 4\n",
    "    rows = 4\n",
    "    \n",
    "    #Plot\n",
    "    for i in range(1, columns*rows+1):\n",
    "        #Prediction\n",
    "        yhat = ' '.join(dictXy_test[testIDs[i-1]].prediction)\n",
    "        \n",
    "        #Plot image and description together\n",
    "        img = mpimg.imread(dictInit['DIR_ALL_IMAGES']+testIDs[i-1])\n",
    "        ax = fig.add_subplot(rows, columns, i)\n",
    "        ax.set_title(\"\\n\".join(wrap(yhat, 50)), color='white')\n",
    "        ax.set_facecolor('xkcd:black')\n",
    "        plt.imshow(img)\n",
    "    plt.savefig(dictInit['FN_TEST_RESULTS']+'_'+datetime.today().strftime('%Y-%m-%d-%H:%M:%S')+'.png',\n",
    "                bbox_inches='tight', facecolor='black', dpi = 50)\n",
    "    plt.show()\n",
    "\n",
    "#Plot sample test images\n",
    "plotSamples(dictXy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Papineni K, Roukos S, Ward T, Zhu W, (Jul 2002), [BLEU: a method for automatic evaluation of machine translation, Proceedings of the 40th Annual Meeting on Association for Computational Linguistics](https://dl.acm.org/doi/10.3115/1073083.1073135)\n",
    "\n",
    "Szegedy C et al., (Sep 2014), [Going Deeper with Convolutions,  arXiv:1409.4842 cs.CV](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "Simonyan K, Zisserman A, (Sep 2014), [Very Deep Convolutional Networks for Large-Scale Image Recognition, arXiv:1409.1556 cs.CV](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Tanti M, Gatt A, Camilleri K.P., (Aug 2017), [What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?, arXiv:1708.02043 cs.CL](https://arxiv.org/abs/1708.02043)\n",
    "\n",
    "Brownlee J., (Jun 2019), [How to Develop a Deep Learning Photo Caption Generator from Scratch, Machine Learning Mastery](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/)\n",
    "\n",
    "Plummer B. et al., (May 2015), [Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models, arXiv:1505.04870 cs.CV](https://arxiv.org/abs/1505.04870)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Image_Caption_v1.0.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
